{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dbc441a8",
   "metadata": {},
   "source": [
    "#### Complete LangChain Pipeline (end-to-end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba7357e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2fce8361",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['HF_TOKEN'] = os.getenv('HF_TOKEN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ce55015",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Load Data\n",
    "from langchain_community.document_loaders import DirectoryLoader, PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "df6938d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Embeddings + Vector DB\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.embeddings import HuggingFaceBgeEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1efe2c3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-05-23T01:29:23+00:00', 'author': '', 'keywords': '', 'moddate': '2022-05-23T01:29:23+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'papers/2203.14465v2.pdf', 'total_pages': 30, 'page': 0, 'page_label': '1'}, page_content='STaR: Self-Taught Reasoner\\nBootstrapping Reasoning With Reasoning\\nEric Zelikman∗1, Yuhuai Wu∗12, Jesse Mu1, Noah D. Goodman1\\n1Department of Computer Science, Stanford University\\n2 Google Research\\n{ezelikman, yuhuai, muj, ngoodman}@stanford.edu\\nAbstract\\nGenerating step-by-step \"chain-of-thought\" rationales improves language model\\nperformance on complex reasoning tasks like mathematics or commonsense\\nquestion-answering. However, inducing language model rationale generation cur-\\nrently requires either constructing massive rationale datasets or sacriﬁcing accuracy\\nby using only few-shot inference. We propose a technique to iteratively leverage a\\nsmall number of rationale examples and a large dataset without rationales, to boot-\\nstrap the ability to perform successively more complex reasoning. This technique,\\nthe \"Self-Taught Reasoner\" (STaR), relies on a simple loop: generate rationales to\\nanswer many questions, prompted with a few rationale examples; if the generated\\nanswers are wrong, try again to generate a rationale given the correct answer; ﬁne-\\ntune on all the rationales that ultimately yielded correct answers; repeat. We show\\nthat STaR signiﬁcantly improves performance on multiple datasets compared to a\\nmodel ﬁne-tuned to directly predict ﬁnal answers, and performs comparably to ﬁne-\\ntuning a 30× larger state-of-the-art language model on CommensenseQA. Thus,\\nSTaR lets a model improve itself by learning from its own generated reasoning.\\n1 Introduction\\nHuman decision-making is often the result of extended chains of thought [1, 2]. Recent work has\\nshown that explicit intermediate reasoning (“rationales”) can improve large language model (LLM)\\nperformance as well [ 3–8]. For example, [ 5] demonstrated that LLMs explicitly trained to use\\n“scratchpads” for intermediate steps can attain perfect in-distribution performance on arithmetic,\\nand strong out-of-distribution generalization, while models trained to predict answers directly fail\\nto do either. These works suggest that generating explicit rationales before giving a ﬁnal answer\\n(“rationale generation”) is valuable for LLMs across diverse tasks including mathematical reasoning,\\ncommonsense reasoning, code evaluation, social bias inference, and natural language inference.\\nHowever, the two primary methods for inducing rationale generation both have serious drawbacks.\\nOne approach to rationale generation is the construction of a ﬁne-tuning dataset of rationales, either\\nmanually by human annotators or automatically with hand-crafted templates [ 3–5, 9]. Manual\\nmethods are expensive, and it is infeasible to construct such a dataset for each interesting problem\\n[3]. Meanwhile, template-based methods rely on automatically-generated rationales but only work\\nwhen a general solution is already known [5] or reasonable hard-coded heuristics can be made [4].\\nAn alternative is to leverage in-context learning by including only a few rationale examples in the\\nlanguage model prompt. This has been shown to improve accuracy on mathematical and symbolic\\nreasoning tasks relative to prompting without rationales (“direct” prompting) [ 5, 6]. Yet, while few-\\nshot techniques with rationales tend to outperform their non-reasoning counterparts, they generally\\nsubstantially underperform models ﬁne-tuned to directly predict answers using larger datasets [5, 6].\\n*These authors contributed equally to this work\\narXiv:2203.14465v2  [cs.LG]  20 May 2022'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-05-23T01:29:23+00:00', 'author': '', 'keywords': '', 'moddate': '2022-05-23T01:29:23+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'papers/2203.14465v2.pdf', 'total_pages': 30, 'page': 1, 'page_label': '2'}, page_content='Question\\nQuestion, Rationale, Answer\\nRationale, Answer\\nFinetune\\nHintRationale, Answer\\nLanguage \\nModel\\nRationalize\\nWrong \\nAnswer\\nCorrect \\nAnswer\\nRationale \\nGeneration\\nQ: What can be used\\nto carry a small dog?\\nAnswer Choices:\\n(a) swimming pool\\n(b) basket\\n(c) dog show\\n(d) backyard\\n(e) own home\\nA: The answer must be\\nsomething that can be\\nused to carry a small\\ndog. Baskets are\\ndesigned to hold things.\\nTherefore, the answer\\nis basket (b).\\nFigure 1: An overview of STaR and a STaR-generated rationale on CommonsenseQA. We indicate\\nthe ﬁne-tuning outer loop with a dashed line. The questions and ground truth answers are expected to\\nbe present in the dataset, while the rationales are generated using STaR.\\nIn this paper, we adopt a different approach: by leveraging the LLM’s pre-existing reasoning ability,\\nwe iteratively bootstrap the ability to generate high-quality rationales. Speciﬁcally, we few-shot\\nprompt a large language model to self-generate rationales and reﬁne the model’s ability further by\\nﬁne-tuning on those rationales that lead to correct answers. We repeat this procedure, using the\\nimproved model to generate the next training set each time. This is a synergistic process, where\\nimprovements in rationale generation improve the training data, and improvements in training data\\nfurther improve rationale generation.\\nHowever, we ﬁnd this loop eventually fails to solve any new problems in the training set because it\\nreceives no direct training signal for problems it fails to solve. To overcome this issue, we propose\\nrationalization: for each problem that the model fails to answer correctly, we generate a new rationale\\nby providing the model with the correct answer. This lets the model reason backward—given the\\ncorrect answer, the model can more easily generate a useful rationale. These rationales are then\\ncollected as part of the training data, which often improves overall accuracy.\\nWe thus develop the Self-Taught Reasoner (STaR, Fig. 1) method, a scalable bootstrapping method\\nallowing models to learn to generate their own rationales, while also learning to solve increasingly\\ndifﬁcult problems. In our method, we repeat the following process: in each iteration, ﬁrst construct a\\nﬁnetuning dataset by attempting to solve the dataset using the current model’srationale generation\\nability; then, augment this dataset using rationalization, justifying ground-truth answers to problems\\nthe model failed to solve; ﬁnally, ﬁnetune the large language model on the combined dataset.\\nApplying STaR on arithmetic, math word problems, and commonsense reasoning, we observe it is\\nable to effectively translate a small number of few-shot prompts into a large rationale dataset, yielding\\ndramatic performance improvements. On CommonsenseQA [10], we ﬁnd STaR improves over both\\na few-shot baseline (+35.9%) and a baseline ﬁne-tuned to directly predict answers (+12.5%) , and\\nperforms comparably to a ﬁne-tuned model that is 30× larger (72.5% vs. 73.0%).\\nThus, we make the following contributions:\\n1. We propose a bootstrapping mechanism to iteratively generate a rationale dataset from a few\\ninitial examples with rationales—without needing to check new rationales’ correctness.\\n2. We complement rationale generation with rationalization, where a model is tasked with\\njustifying an answer and then ﬁne-tuned as if it had come up with the rationale without any\\nhint. We show rationalization accelerates and improves the bootstrapping process.\\n3. We evaluate these techniques with a variety of ablations in both mathematical and common-\\nsense reasoning domains.\\n4. We propose what is, to our knowledge, the ﬁrst technique to allow a pre-trained large\\nlanguage model to iteratively use its language modeling capacity to improve itself.\\n2'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-05-23T01:29:23+00:00', 'author': '', 'keywords': '', 'moddate': '2022-05-23T01:29:23+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'papers/2203.14465v2.pdf', 'total_pages': 30, 'page': 2, 'page_label': '3'}, page_content='2 Background and Related Work\\nIn-context Learning Recently, a collection of works has emerged exploring the capacity for large\\nlanguage models to perform in-context learning [ 11, 12]. In essence, in-context learning treats\\nfew-shot learning as a language modeling problem, by showing a few examples in the context (i.e.\\nprompt), and allowing the model to learn and identify the pattern to apply to new examples. Some\\nhave studied in-context learning based on the language modeling objective in terms of Bayesian\\ninference [13] while others have attempted to describe the process more mechanistically in terms\\nof “induction heads” [ 14]. Moreover, differences in prompt conﬁgurations have been known to have\\ndramatic effects on few-shot performance. Some have even found that replacing few-shot prompts\\nwith a “soft prompt” which can be optimized in embedding space results in noticeable gains [ 15].\\nInstead of emphasizing the representation of the question, we focus on the model output; in particular,\\nwe focus on the model’s ability to reason through a problem before coming to a conclusion.\\nRationales One of the initial works on the impact of rationales on language model performance was\\n[3], showing that training a language model on a dataset with explicit rationales preceding the answer\\ncould improve a model’s ability to generate the ﬁnal answer. However, this required many thousands\\nof training examples to be manually annotated with human reasoning. Recently, [5] demonstrated\\nthat step-by-step “scratchpads” can improve ﬁne-tuned LLM performance and generalization on\\ntasks such as arithmetic, polynomial evaluation, and program evaluation. Similarly, [6] used a single\\nfew-shot “chain-of-thought” reasoning prompt in order to improve model performance on a collection\\nof tasks, without ﬁne-tuning. Finally, [16] showed that a curriculum learning approach could help\\nsolve formal math problems, as long as 1) they were translated into Lean (a theorem-proving language\\n[17]), 2) one could directly evaluate the validity of the proofs, 3) one could sample numerous potential\\nsolutions for each problem, 4) had trained a separate value function model, and 5) started with GPT-f\\n(a model already ﬁne-tuned on a large math dataset [18]). We note that there are many domains where\\nthese conditions do not all apply. In addition, works have aimed to explain why rationales have this\\nbeneﬁcial effect: some have analyzed their impact from the perspective of latent variable models [19]\\nwhile others have provided formal proofs of the beneﬁt of intermediate task supervision [20].\\nIterated Learning A variety of iterated learning algorithms have been proposed, where solutions\\nor successful methods which are found are in turn used to ﬁnd additional solutions [21, 22, 16]. [21]\\nintroduced Expert Iteration (ExIt), a reinforcement learning technique serving as an inspiration for\\nour approach. Essentially, it consists of a loop of self-play by an “apprentice,” followed by imitation\\nlearning with feedback from a slower “expert” and then the replacement of the expert with the now-\\nimproved apprentice. [16] builds off of ExIt for formal reasoning, while [22] applies iterated learning\\nto visual question answering using modular networks which can be combined compositionally. There\\nare further similarities between STaR and expert iteration methods [ 21]. For example, ﬁltering\\ngenerated examples based on whether their ultimate answer matches the target can be seen as expert\\nfeedback. However, we have a ﬁxed “expert” and do not train a separate value function.\\nNatural Language Explanations Natural language explanations have also been discussed from the\\nperspective of explainable machine learning, focusing on justiﬁcation rather than reasoning [23, 24].\\nThe motivation for this line of work is largely grounded in explainable decision making, and similarly\\nto [3], generally does not ﬁnd that requiring post-hoc explanations improves model performance.\\n3 Method\\n3.1 Rationale Generation Bootstrapping (STaR Without Rationalization)\\nWe are given a pretrained LLM M and an initial dataset of problems x with answers y: D =\\n{(xi, yi)}D\\ni=1. Our technique starts with a small prompt set P of examples with intermediate ratio-\\nnales r: P = {(xp\\ni , rp\\ni , yp\\ni )}P\\ni=1, where P ≪ D (e.g. P = 10). Like standard few-shot prompting, we\\nconcatenate this prompt set to each example in D, i.e. xi = (xp\\n1, rp\\n1, yp\\n1, . . . , xp\\nP , rp\\nP , yp\\nP , xi), which\\nencourages the model to produce a rationale ˆri for xi followed by an answer ˆyi. We assume that\\nrationales that lead to correct answers are of better quality than those that lead to incorrect answers.\\nTherefore, we ﬁlter the generated rationales to include only the ones which result in the correct\\nanswer (ˆyi = yi). We ﬁne-tune the base modelM on this ﬁltered dataset, and then restart this process\\nby generating the new rationales with the newly ﬁne-tuned model. We keep repeating this process\\nuntil the performance plateaus. Note that during this process, once we collect a new dataset, we train\\nfrom the original pre-trained model M instead of continually training one model to avoid overﬁtting.\\nWe provide an outline of this algorithm in Algorithm 1.\\n3'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-05-23T01:29:23+00:00', 'author': '', 'keywords': '', 'moddate': '2022-05-23T01:29:23+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'papers/2203.14465v2.pdf', 'total_pages': 30, 'page': 3, 'page_label': '4'}, page_content='STaR can be seen as an approximation to an RL-style policy gradient objective. To see this, note\\nthat M can be viewed as a discrete latent variable model pM (y | x) = ∑\\nr p(r | x)p(y | x, r); in\\nother words, M ﬁrst samples a latent rationaler before predicting y. Now, given the indicator reward\\nfunction 1 (ˆy = y), the total expected reward across the dataset is\\nJ(M, X, Y ) =\\n∑\\ni\\nEˆri,ˆyi∼pM (·|xi)1 (ˆyi = yi), (1)\\n∇J(M, X, Y ) =\\n∑\\ni\\nEˆri,ˆyi∼pM (·|xi) [1 (ˆyi = yi) · ∇ log pM (ˆyi, ˆri | xi)] , (2)\\nwhere the gradient is obtained via the standard log-derivative trick for policy gradients. Note that\\nthe indicator function discards the gradient for all sampled rationales that do not lead to the correct\\nanswer yi: this is the ﬁltering process in STaR (Line 5). Thus, STaR approximatesJ by (1) greedily\\ndecoding samples of (ˆri, ˆyi) to reduce variance of this estimate (at the cost of potentially biased\\nexploration of rationales), and (2) taking multiple gradient steps on the same batch of data (similar\\nto some policy gradient algorithms [25]). These approximations make STaR a simple and broadly\\napplicable method that can be implemented with standard LLM training machinery; future work\\nshould more closely investigate the link between STaR and the RL objective above.\\n3.2 Rationalization\\nQ: Where do you put your grapes just\\nbefore checking out?\\nAnswer Choices:\\n(a) mouth\\n(b) grocery cart (CORRECT)\\n(c) super market\\n(d) fruit basket\\n(e) fruit market\\nA: The answer should be the place\\nwhere grocery items are placed before\\nchecking out. Of the above choices,\\ngrocery cart makes the most sense for\\nholding grocery items. Therefore,\\nthe answer is grocery cart (b).\\nFigure 2: A few-shot prompt hint we use for\\nrationalization (and not for rationale gener-\\nation), using the rationale from [ 6], with its\\nhint included in green, followed by the ratio-\\nnale and the answer generated by the model.\\nThe rationale generation bootstrapping algorithm car-\\nries a limitation. Since the model is only trained on\\nthe examples which it answers correctly, improve-\\nment ends when the model fails to solve new prob-\\nlems in the training set. This is fundamentally due\\nto the fact that the algorithm cannot obtain any train-\\ning signal from failed examples. Inspired by [3], we\\npropose a technique we call “rationalization”. Specif-\\nically, we provide the answer as a hint to the model\\nand ask it to generate rationales in the same style\\nas in the previous rationale generation step. Given\\nthe answer, the model is able to reason backwards,\\nand hence more easily generate a rationale that leads\\nto the correct answer. For example, in Figure 2, we\\nprovide the hint that ”(b) grocery cart” is the cor-\\nrect answer in the prompt to generate the rationale.\\nWe apply rationalization to the problems which the\\nmodel failed to solve with rationale generation. When\\nadding a rationalization-generated rationale to our dataset, we do not include the hint in its corre-\\nsponding prompt, as if the model had come up with the rationale without the hint. After ﬁltering, we\\nﬁne-tune on the previously generated dataset combined with the rationalization-generated dataset.\\nAlgorithm 1 STaR\\nInput M: a pretrained LLM; dataset D = {(xi, yi)}D\\ni=1 (w/ few-shot prompts)\\n1: M0 ← M # Copy the original model\\n2: for n in 1...N do # Outer loop\\n3: (ˆri, ˆyi) ← Mn−1(xi) ∀i ∈ [1, D] # Perform rationale generation\\n4: (ˆrrat\\ni , ˆyrat\\ni ) ← Mn−1(add_hint(xi, yi)) ∀i ∈ [1, D] # Perform rationalization\\n5: Dn ← {(xi, ˆri, yi) | i ∈ [1, D] ∧ ˆyi = yi} # Filter rationales using ground truth answers\\n6: Drat\\nn ← {(xi, ˆrrat\\ni , yi) | i ∈ [1, D] ∧ ˆyi ̸= yi ∧ ˆyrat\\ni = yi} # Filter rationalized rationales\\n7: Mn ← train(M, Dn ∪ Drat\\nn ) # Finetune the original model on correct solutions - inner loop\\n8: end for\\nAlgorithm 1 describes the full algorithm, with the parts in blue corresponding to rationalization.\\nWithout those parts, Algorithm 1 corresponds to STaR without rationalization. Figure 1 provides an\\noverview diagram. Fine-tuning on the dataset generated by rationalization has a crucial beneﬁt of\\nexposing the model to difﬁcult problems which otherwise would not have appeared in its ﬁnetuning\\ndataset. This can be understood as challenging the model to “think outside the box” about problems\\non which it was unsuccessful. A secondary beneﬁt of rationalization is an increase in dataset size.\\n4'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-05-23T01:29:23+00:00', 'author': '', 'keywords': '', 'moddate': '2022-05-23T01:29:23+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'papers/2203.14465v2.pdf', 'total_pages': 30, 'page': 4, 'page_label': '5'}, page_content='4 Experiments\\nFor our experiments, we focus on arithmetic, commonsense reasoning, and grade school math to\\ndemonstrate STaR’s breadth. In particular, for arithmetic, we follow a setup inspired by [ 5]. For\\ncommonsense question-answering we follow [13, 6] and use CommonsenseQA (CQA), a widely\\nused multiple-choice dataset for this domain [10]. For grade school math, we use GSM8K from [9].\\n4.1 Experimental Protocol\\nWe used GPT-J as our base language model, and the ﬁne-tuning script from the GPT-J repository\\n[26]. We chose GPT-J, a 6B-parameter model, because the checkpoint and ﬁne-tuning code are\\npublicly available [26], and the model is large enough to generate rationales of non-trivial quality to\\nbe bootstrapped from. More hyperparameter details about GPT-J and our ﬁne-tuning are included\\nin Appendix H. Following the default setting of [26], we perform a 100-step learning rate warmup,\\nfrom which point we use a constant learning rate. Unless stated otherwise, we start with 40 training\\nsteps at the ﬁrst outer loop, and increase the number of ﬁne-tuning training steps by20% with each\\nouter loop. In general, we found that training more slowly at the beginning ultimately beneﬁts\\nmodel performance. We expect that further improvement is possible via a thorough hyperparameter\\nsearch—we leave this to future work due to computational constraints.\\nFor arithmetic problems, we ﬁrst generate a dataset of 50,000 randomly sampled questions (uniformly\\nover the digit lengths) in the format introduced by [5]. For each outer loop iteration on arithmetic,\\nwe sample 10,000 problems from the dataset. We use 10 random few-shot rationale examples for\\neach digit for its corresponding few-shot prompt. For each of the 9, 741 questions in the training set\\nof CommonsenseQA, we add the question to the few-shot rationale prompt, and prompt the model\\nto generate the rationale and answer for that question. For few shot prompting on CQA, we start\\nwith the same 10 questions as used in [ 6], with the rationales modiﬁed slightly to ﬁx an incorrect\\nanswer and to more explicitly reference relevant knowledge. We include these modiﬁed prompts in\\nAppendix B1. These prompts serve as our complete set of explanations. We run STaR until we see\\nperformance saturate, and we report the best results.\\nWhen performing rationalization, we ﬁnd that the choice to include or omit few-shot prompts on\\nouter-loop iterations after the ﬁrst iteration does not have a substantial impact on the method’s ultimate\\nperformance. However, there are some nuances which we discuss further in Section 5, leading us to\\nuse few-shot prompts unless stated otherwise.\\n4.2 Datasets\\nInput:\\n6 2 4 + 2 5 9\\nTarget:\\n<scratch>\\n6 2 4 + 2 5 9 , C: 0\\n2 + 5 , 3 C: 1\\n6 + 2 , 8 3 C: 0\\n, 8 8 3 C: 0\\n0 8 8 3\\n</scratch>\\n8 8 3\\nFigure 3: A visualization of\\na 3-digit arithmetic problem\\nwith a scratchpad. C corre-\\nsponds to the carry from the\\nprevious digit’s summation.\\nArithmetic The arithmetic task is to calculate the sum of two n-\\ndigit integers. We generate the dataset based on the descriptions\\nin [5] and visualize an example scratchpad in Figure 3. Everything\\nup to and including “ Target:” is given as part of a prompt, and\\nthe model is asked to generate the scratchpad (start/end indicated\\nby “ <scratch>”) and the ﬁnal answer, as in [5]. Each line of the\\nscratchpad corresponds to the summation of each pair of digits from\\nthe ﬁnal digit to the ﬁrst digit, the accumulating ﬁnal digits of the\\nanswer, and a carry digit corresponding to whether the previous pair\\nsummed to at least 10. We include few-shot prompts for 1 to 5 digits.\\nWhen performing rationalization, we include the correct answer after\\n“ Target” and query the model to produce the scratchpad and then\\nreproduce the correct answer following the scratchpad.\\nCommonsenseQA The multiple-choice commonsense reasoning\\ntask, CommonsenseQA [10] (CQA), is constructed from ConceptNet, a semantic graph of concepts\\nand their relationships with over a million nodes [28]. [10] identiﬁed a set of “target” concepts in\\nConceptNet for each question, where the target concepts share a semantic relationship to one “source”\\nconcept. Then each question is crowdsourced to allow a reader to identify one target concept, while\\nmentioning the source concept. In addition, two distractor answers are added. The dataset has 12,247\\nquestions, each with ﬁve choices, with 9,741 in the train set, 1,221 in the dev set, and 1,285 in the\\n(withheld) test set.\\n1Based on [27], this is unlikely to meaningfully affect [6]’s few-shot performance.\\n5'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-05-23T01:29:23+00:00', 'author': '', 'keywords': '', 'moddate': '2022-05-23T01:29:23+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'papers/2203.14465v2.pdf', 'total_pages': 30, 'page': 5, 'page_label': '6'}, page_content='Iterations\\nAccuracy (%)\\n0\\n20\\n40\\n60\\n80\\n100\\n0 4 8 12 16 20 24 28\\n1\\n2\\n3\\n4\\n5\\n(a) Without rationalization\\nIterations\\n0\\n20\\n40\\n60\\n80\\n100\\n0 4 8 12 16\\n1\\n2\\n3\\n4\\n5 (b) With rationalization\\nFigure 4: A visualization of the accuracy of n-digit summation with each iteration of STaR with\\nand without rationalization for arithmetic. Each series corresponds to the accuracy of summing two\\nn-digit numbers.\\nCorresponding to the broad variety of ConceptNet, CQA contains a diverse set of questions which\\nrequire commonsense reasoning ability building off of standard world knowledge, where human\\nperformance is 89% [ 10]. Many have pointed out that CQA contains a number of biases, along\\nseveral dimensions including gender [3]. We discuss how this may impact our method in Appendix G.\\nThere are also many typos and questions which are fundamentally ambiguous 2. We use it despite\\nthese issues as it is a general question-answering dataset relying on both common world knowledge\\nand simple reasoning, which serves as a good test-bed for our method.\\nGrade School Math (GSM8K) We also evaluate on the Grade School Math (GSM8K) dataset,\\nwhich contains 7,473 train and 1,319 test examples of grade-school-level word problems [9]. These\\nmath problems are posed in natural language and require two to eight calculation steps to arrive at a\\nﬁnal answer. This dataset combines the skills needed for arithmetic and commonsense reasoning.\\n4.3 Symbolic Reasoning: Results on Arithmetic\\nThe accuracies of the model across digits 1-5 over each iteration of the outer loop are plotted in\\nFigure 4. After running STaR for 16 iterations, the overall accuracy is 89.5%. For reference, a\\nbaseline trained on 10,000 examples without rationales for 5,000 steps attains 76.3% accuracy.\\nNotably, few-shot accuracy on arithmetic problems is very low, even with rationales: accuracy on\\n2-digit addition is less than 1%, and accuracy on more digits close to zero.\\nIterations\\nAccuracy (%)\\n0\\n20\\n40\\n60\\n80\\n100\\n0 4 8 12 16 20 24 28 32 36\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\n10\\nFigure 5: We introduce additional digits to\\nSTaR with rationalization at the 20th iteration.\\nWith rationalization, the accuracy is able to improve\\nespecially quickly. After one ﬁne-tuning iteration on\\nthe model’s generated scratchpads, 2-digit addition\\nimproves to 32% from less than 1%. Without ratio-\\nnalization, the performance improvement is stage-\\nwise: the model generally has poor performance\\non the n-digit sum until it has good performance\\non the (n − 1)-digit sum. With rationalization, the\\nmodel can learn many lengths at once, though not\\nwith equal accuracy. Rationalization allows many\\nproblems to be solved few-shot, so we start STaR\\ntraining with 300 steps (note, doing so without ra-\\ntionalization causes overﬁtting on1-digit addition),\\nand increase training by 20 steps per iteration.\\nWe also perform an experiment where we continue pre-training STaR with rationalization with\\nadditional digits, starting before the 20th iteration, while keeping the total number of training examples\\nﬁxed at each iteration. We ﬁnd that not only does this appear to quickly improve performance on the\\ninitial set of digits, but when evaluated on 9 and 10 digit examples, never seen during training, the\\nmodel successfully solves many of these out-of-distribution problems. As visualized in Figure 5, the\\nintroduction of these digits appears to make the training less stable, but the exact cause is unclear.\\n2For example, “Billy bought coffee and waited for his wife to arrive from France. Where might he have\\nbeen?” includes airport and train station as options. The correct answer, perhaps surprisingly, is train station.\\n6'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-05-23T01:29:23+00:00', 'author': '', 'keywords': '', 'moddate': '2022-05-23T01:29:23+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'papers/2203.14465v2.pdf', 'total_pages': 30, 'page': 6, 'page_label': '7'}, page_content='Table 1: We evaluate several baselines, including a few-shot GPT-J evaluation both with and without\\nscratchpads, a GPT-J baseline ﬁnetuned to directly predict the answer, and STaR with and without\\nrationalization applied to GPT-J. We use CoT to denote non-STaR models outputting rationales, and\\nDirect to indicate those directly predicting the ﬁnal answer. Note the ﬁnal STaR model is trained on\\n78.2% of the training dataset with rationale generation, and an additional 8.5% from rationalization.\\nCQA Dev Set Accuracy (%) Train Data Used (%)\\nGPT-3 Direct Finetuned [29] 73.0 100\\nFew-shot Direct GPT-J 20.9 ∼0\\nFew-shot CoT GPT-J3 36.6 ∼0\\nFew-shot CoT LaMDA 137B [6] 55.6 ∼0\\nGPT-J Direct Finetuned 60.0 100\\nSTaR without rationalization 68.8 69.7\\nSTaR with rationalization 72.5 86.7\\n4.4 Natural Language Reasoning: Commonsense Question Answering\\nThe CommonsesenseQA (CQA) setting introduces several new challenges. In the arithmetic task,\\nan incorrect scratchpad in the reasoning step, and to a lesser degree in the rationalization step, was\\nextremely likely to result in an incorrect answer. On the other hand, CQA problems are 5-way\\nmultiple choice questions. Thus, one will get the right answer at random approximately 20% of\\nthe time, regardless of the quality of reasoning. Moreover, some simple heuristics (e.g. semantic\\nsimilarity) can meaningfully improve this to ≈30% without any reasoning, as shown by [10].\\nWe evaluate this dataset as described in the experimental protocol and compare to several baselines.\\nThe ﬁrst baseline is to ﬁnetune GPT-J to directly output the ﬁnal answer, which we call “GPT-J\\nFinetuned”. We also compare to GPT-3 ﬁnetuned to directly predict the ﬁnal answer from [29], and a\\n137B parameter Lambda model few-shot prompted with chain-of-thought (CoT) rationales from [6].\\nWe found that, as shown in Table 1, STaR without rationalization outperformed GPT-J ﬁne-tuned\\ndirectly on the ﬁnal answer for the entire dataset, despite training on less of the data. The inclusion of\\nrationalization improved this performance to 72.5%, far closer to the 73% of the 30× larger GPT-3.\\nAs expected, we also see STaR surpassed the few-shot baselines, including the much-larger 137B\\nLaMDA model [30, 6]. We expect accuracy would be further improved if we applied STaR to a\\nmodel with higher few-shot performance.\\nCase Study Note that it is harder to judge the rationale quality: for arithmetic, one can compare\\nthem to the ground truth rationales, but for CQA the evaluation is necessarily qualitative. For this\\nreason, we include a case study in Figure 7. We observe that the rationales provided are generally\\ncoherent and of a similar structure to the few-shot rationales. We make the following two observations:\\n1. After training with STaR, we see the model was able to generate reasonable rationales that\\nsolve new problems, which explains part of the observed performance gain.\\n2. We also see that there were many instances in which STaR improved the quality of rationales\\nover those generated in a few-shot manner.\\nHuman Evaluation Based on the observation that STaR may improve reasoning quality for prob-\\nlems even when they were initially answered correctly via few-shot prompting, we performed a\\npreliminary qualitative analysis. We randomly selected 50 rationales generated from few-shot CoT\\nand STaR-generated rationales on questions which they both answered correctly, as well as human-\\ngenerated rationales for these problems from [3]. We then presented a random subset of 10 questions\\nand rationales to each of 20 crowdworkers on Proliﬁc [31] with the rationales in a randomized order,\\nasking them to rank the rationales based on which they felt best justiﬁed the answer. The participants\\nwere 30% more likely to rank the STaR-generated rationales higher than the few-shot rationales\\n(p = .039). This indicates that, as mentioned in the case study, STaR can improve the quality of\\nrationale generation.\\nWe also found that the participants were 74% more likely to prefer the STaR-generated rationales\\nover the human-generated rationales (p < .001). To be clear, we do not believe that this indicates\\nhuman-level rationale-generation performance. Instead, we feel that it speaks to the difﬁculty of\\neliciting high-quality rationales. We reproduce the test prompts in Appendix C and elaborate on the\\nlimitations of the crowdsourced explanations dataset.\\n3We use the same few-shot rationales as described in Section 4.1 - namely ﬁxing typos and improving clarity.\\n7'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-05-23T01:29:23+00:00', 'author': '', 'keywords': '', 'moddate': '2022-05-23T01:29:23+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'papers/2203.14465v2.pdf', 'total_pages': 30, 'page': 7, 'page_label': '8'}, page_content='Table 2: We ﬁnd that STaR substantially improves GSM8K performance over the baselines, despite\\ntraining on only 25.0% of the data for the model without rationalization, and 28.7% of the dataset\\n(with 0.5% from rationalization) for the model with rationalization.\\nGSM8K Test Accuracy (%) Train Data Used (%)\\nFew-shot Direct GPT-J 3.0 ∼0\\nFew-shot CoT GPT-J 3.1 ∼0\\nGPT-J Direct Finetuned 5.8 100\\nSTaR without rationalization 10.1 25.0\\nSTaR with rationalization 10.7 28.7\\nFailure Cases Finally, we found a variety of interesting failure cases, many of which corresponded\\nto standard logical fallacies. For example, the model often made statements related to the topic of the\\nquestion but which were not actually arguments for why the answer should be true. Sometimes, the\\nmodel claimed the question implied the answer as an argument, without explaining why. Other times,\\nespecially early in training, the model answered as if it has knowledge about a particular individual,\\ninstead of making a general statement - e.g. “the king’s castle is a place where he feels safe” instead\\nof “castles are places where kings feel safe.” We provide examples and analyze errors in Appendix A.\\nFew-shot Prompt Training Including few-shot prompts during ﬁne-tuning [12] appears to have\\na meaningful performance beneﬁt (60.9% to 68.8% without rationalization, 69.9% to 72.5% with\\nrationalization). Thus, we generally suggest its use for at least some portion of the training, though\\nwe discuss some caveats in Section 5.\\n4.5 Mathematical Reasoning in Language: Grade School Math\\n0 1 2 3 4 5 6 7 8\\nHuman-produced calculator steps\\n0\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8Generated calculator steps\\nGenerated vs human-produced # of calculator steps\\n0\\n1\\n2\\n3\\n4\\n5\\n6\\nLog frequency\\nFigure 6: A comparison of the number of calcu-\\nlator steps generated by the model in order to\\nsolve examples in the training set relative to the\\nnumber of steps used in the ground truth.\\nWe again ﬁnd on GSM8K that STaR substantially\\nimproves performance beyond few-shot with ra-\\ntionales or training to directly predict the answers\\n(without rationales), shown in Table 2 and include\\nthe few-shot prompt in Appendix I. We observe\\nthat on this task, the use of rationalization does not\\nsubstantially improve performance. Note that, in\\ntraining, it was necessary to cap the number of train-\\ning steps at the 30th iterations (after 7912 steps),\\nto prevent the training process from becoming pro-\\nhibitively long. The results were reached after 36\\niterations for STaR without rationalization and an\\nadditional 10 iterations with rationalization.\\nMost often, the number of calculation steps gen-\\nerated by the model matches the number of steps\\ntaken by humans (generally between 53% and 57%\\nagreement across all iterations). We visualize this\\nexplicitly in Figure 6. We see that when the ground\\ntruth and model disagree on the number of calcu-\\nlation steps, the model typically uses fewer. Sometimes this is because the model skips steps, but\\noccasionally it ﬁnds different solutions. We show an example in Appendix J, where the model\\ndisregards redundant information and solves a 7-step problem in a single step.\\n5 Discussion and Challenges\\nThe Impact of Rationalization An essential question is exactly what role rationalization plays.\\nIntuitively, rationalization allows a model to reverse-engineer a solution, or provides a heuristic for\\nidentifying whether each step makes the conclusion more likely. This parallels real-world problems\\nwhere the ﬁnal result is known, but challenging to derive a good justiﬁcation. From a mathematical\\nperspective, while rationale generation samples rationales from the distribution p(r | x) provided by\\nour model M, rationalization conditions on the answer, letting us access an alternative distribution\\np(r | x, y) which may be a better search space for rationales. Then rationalization could be framed as\\nan off-policy estimate of the objective in Equation 1, sampling from the hint-augmented model as a\\nproposal distribution. Future work should establish more connections between rationalization and\\nthese RL objectives, and examine more generally when and why rationalization improves learning.\\n8'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-05-23T01:29:23+00:00', 'author': '', 'keywords': '', 'moddate': '2022-05-23T01:29:23+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'papers/2203.14465v2.pdf', 'total_pages': 30, 'page': 8, 'page_label': '9'}, page_content='In addition, due to the low sampling temperature, the outputs without rationalization correspond\\nto the examples where the model is most conﬁdent in its answer. This results in these examples\\nproviding a weaker gradient signal than the rationalization examples, at least in the ﬁrst iteration.\\nSince we retrain from the initial pre-trained model every time we run a ﬁne-tuning iteration, the\\ndegree of this effect is also difﬁcult to measure directly. Finally, we must point out that the method\\nto add the “hint” does not follow immediately from the question and answer and in some contexts\\nproviding it may be nontrivial. An exploration of the various impacts of different hinting techniques\\nand their generality is an avenue for future work.\\nTemperature One intuitive alternative to rationalization, if one seeks to expand the training dataset,\\nis more and higher-temperature sampling. However, in practice, we found that this is counterpro-\\nductive. In general, it substantially increases the likelihood of a correct answer despite incorrect\\nreasoning, and training on bad or irrelevant reasoning prevents generalization. This is particularly\\nclear in more structured tasks, like arithmetic, where the scratchpads that the model learns to produce\\nwith a higher-temperature sampling approach diverge into meaninglessness and cause the model to\\nstagnate. Overall, we found that higher temperatures as an alternative to rationalization (e.g. 0.5\\nor 0.7) consistently led to models worse than models with reasoning alone. In addition, as text\\ngeneration by large language models is sequential (i.e. one cannot produce a token without producing\\nthe preceding token), generating text is a bottleneck and this is computationally far less efﬁcient\\nthan rationalization. For example, generating 10 sample outputs is approximately 10 times slower\\nthan generating one sample output. However, one potentially valuable way to leverage multiple\\nsamples would be to use the method proposed in [32], using the majority-vote result of multiple high-\\ntemperature scratchpads as a ground truth against which we compare a low-temperature scratchpad.\\nThis may allow one to apply STaR to a dataset of only questions, without answers.\\nFew-shot Prompting A noteworthy phenomenon is that the inclusion of few-shot prompting during\\nsampling seems to dramatically reduce “drift” where later rationales become increasingly dissimilar\\nfrom the initial few-shot set of rationales. One beneﬁt of this is that the model may be less constrained\\nby the quality and difﬁculty of the initial rationales, theoretically allowing it to generalize more. One\\npotentially negative consequence is that the style of the rationales may less-closely match the original\\nprompting style. Another beneﬁt is in terms of computational resources - a shorter prompt length\\nallows for a shorter sequence length when sampling. Technically, the point in training at which we\\n“disable” few-shot prompts is another hyperparameter which we could tune, but we leave this to future\\nwork. In addition, by leaving prompts out after the initial outer-loop iteration, the model tends to\\nperform gradually worse at rationalization as it trains for longer periods of time. As a result, it may\\nbe necessary to include some hints during training for long periods of time with this approach.\\nUltimately, the choice to include few-shot prompts in later iterations of training appears to depend on\\nthe use-case: when the goal is consistent adherence to a particular prompt style, which may beneﬁt\\nexplainability, include few-shot prompts in sampling; when the goal is a faster training loop, one may\\nremove them. Moreover, it is possible that with other datasets or larger models there is an impact on\\nperformance, so we encourage this to be generally treated as a hyperparameter.\\n6 Conclusion\\nWe present the Self-Taught Reasoner (STaR), which iteratively improves a model’s ability to generate\\nrationales to solve problems. We few-shot prompt a model to solve many problems in a step-by-step\\nmanner by generating rationales, and then prompt it to rationalize the correct answer for problems\\nit gets wrong. We ﬁnetune on both the initially correct solutions and rationalized correct solutions,\\nand repeat the process. We ﬁnd that this technique signiﬁcantly improves the model’s generalization\\nperformance on both symbolic reasoning and natural language reasoning.\\nThere are several important limitations on STaR as presented. In order for the ﬁrst iteration of STaR\\nto succeed, few-shot performance must be above chance, implying that the initial model must be\\nbig enough to have some reasoning capabilities. For instance we found that GPT-2 was not able\\nto bootstrap from few-shot reasoning in even the arithmetic domain. A further limitation is that\\nsettings with a high level of chance performance (e.g. binary decisions) yield many poor rationales,\\nconfounding the STaR approach. An open problem is how to ﬁlter bad reasoning in these settings.\\nNonetheless, we believe using examples without reasoning to bootstrap reasoning is a very general\\napproach, and that STaR can serve as the basis of more sophisticated techniques across many domains.\\n9'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-05-23T01:29:23+00:00', 'author': '', 'keywords': '', 'moddate': '2022-05-23T01:29:23+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'papers/2203.14465v2.pdf', 'total_pages': 30, 'page': 9, 'page_label': '10'}, page_content='Acknowledgements\\nWe thank Imanol Schlag for his detailed feedback about this work, as well as Rose E Wang, Markus\\nRabe, Aitor Lewkowycz, Rishi Bommasani, Allen Nie, Alex Tamkin, and Qian Huang. We thank\\nCem Anil for his very helpful insight that rationale ﬁnetuning performance can be improved if the\\ntraining includes the few-shot rationales. We also thank Ben Prystawski for his suggestions on survey\\ncreation. We thank Google TPU Research Cloud for TPU access.\\nReferences\\n[1] William James, Frederick Burkhardt, Fredson Bowers, and Ignas K Skrupskelis. The principles\\nof psychology, volume 1. Macmillan London, 1890.\\n[2] K Anders Ericsson and Herbert A Simon. Protocol analysis: V erbal reports as data. the MIT\\nPress, 1984.\\n[3] Nazneen Fatema Rajani, Bryan McCann, Caiming Xiong, and Richard Socher. Explain yourself!\\nleveraging language models for commonsense reasoning. ACL, 2019.\\n[4] Vered Shwartz, Peter West, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Unsupervised\\ncommonsense question answering with self-talk. EMNLP 2020, 2020.\\n[5] Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin,\\nDavid Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, et al. Show\\nyour work: Scratchpads for intermediate computation with language models. arXiv preprint\\narXiv:2112.00114, 2021.\\n[6] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny\\nZhou. Chain of thought prompting elicits reasoning in large language models. arXiv preprint\\narXiv:2201.11903, 2022.\\n[7] Ana Marasovi´c, Iz Beltagy, Doug Downey, and Matthew E Peters. Few-shot self-rationalization\\nwith natural language prompts. arXiv preprint arXiv:2111.08284, 2021.\\n[8] Andrew K Lampinen, Ishita Dasgupta, Stephanie CY Chan, Kory Matthewson, Michael Henry\\nTessler, Antonia Creswell, James L McClelland, Jane X Wang, and Felix Hill. Can language\\nmodels learn from explanations in context? arXiv preprint arXiv:2204.02329, 2022.\\n[9] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Jacob Hilton, Reiichiro Nakano, Christo-\\npher Hesse, and John Schulman. Training veriﬁers to solve math word problems.arXiv preprint\\narXiv:2110.14168, 2021.\\n[10] Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. Commonsenseqa: A\\nquestion answering challenge targeting commonsense knowledge. In Proceedings of the 2019\\nConference of the North American Chapter of the Association for Computational Linguistics:\\nHuman Language Technologies, V olume 1 (Long and Short Papers), pages 4149–4158, 2019.\\n[11] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,\\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are\\nfew-shot learners. Advances in neural information processing systems , 33:1877–1901, 2020.\\n[12] Jason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan\\nDu, Andrew M Dai, and Quoc V Le. Finetuned language models are zero-shot learners. ICLR\\n2022, 2021.\\n[13] Sang Michael Xie, Aditi Raghunathan, Percy Liang, and Tengyu Ma. An explanation of\\nin-context learning as implicit bayesian inference. arXiv preprint arXiv:2111.02080, 2021.\\n[14] Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova DasSarma, Tom\\nHenighan, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, and et al. In-context learning\\nand induction heads. Transformer Circuits, Mar 2022.\\n10'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-05-23T01:29:23+00:00', 'author': '', 'keywords': '', 'moddate': '2022-05-23T01:29:23+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'papers/2203.14465v2.pdf', 'total_pages': 30, 'page': 10, 'page_label': '11'}, page_content='[15] Brian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-efﬁcient\\nprompt tuning. EMNLP 2021, 2021.\\n[16] Stanislas Polu, Jesse Michael Han, Kunhao Zheng, Mantas Baksys, Igor Babuschkin, and Ilya\\nSutskever. Formal mathematics statement curriculum learning.arXiv preprint arXiv:2202.01344,\\n2022.\\n[17] Leonardo de Moura, Soonho Kong, Jeremy Avigad, Floris van Doorn, and Jakob von Raumer.\\nThe lean theorem prover (system description). In International Conference on Automated\\nDeduction, pages 378–388. Springer, 2015.\\n[18] Stanislas Polu and Ilya Sutskever. Generative language modeling for automated theorem proving.\\narXiv preprint arXiv:2009.03393, 2020.\\n[19] Wangchunshu Zhou, Jinyi Hu, Hanlin Zhang, Xiaodan Liang, Maosong Sun, Chenyan Xiong,\\nand Jian Tang. Towards interpretable natural language understanding with explanations as latent\\nvariables. Advances in Neural Information Processing Systems , 33:6803–6814, 2020.\\n[20] Noam Wies, Yoav Levine, and Amnon Shashua. Sub-task decomposition enables learning in\\nsequence to sequence tasks. arXiv preprint arXiv:2204.02892, 2022.\\n[21] Thomas Anthony, Zheng Tian, and David Barber. Thinking fast and slow with deep learning\\nand tree search. Advances in Neural Information Processing Systems , 30, 2017.\\n[22] Ankit Vani, Max Schwarzer, Yuchen Lu, Eeshan Dhekane, and Aaron Courville. Iterated\\nlearning for emergent systematicity in vqa. ICLR 2021, 2021.\\n[23] Oana-Maria Camburu, Tim Rocktäschel, Thomas Lukasiewicz, and Phil Blunsom. e-snli:\\nNatural language inference with natural language explanations. Advances in Neural Information\\nProcessing Systems, 31, 2018.\\n[24] Hanxiong Chen, Xu Chen, Shaoyun Shi, and Yongfeng Zhang. Generate natural language\\nexplanations for recommendation. arXiv preprint arXiv:2101.03392, 2021.\\n[25] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal\\npolicy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.\\n[26] Ben Wang. Mesh-Transformer-JAX: Model-Parallel Implementation of Transformer Lan-\\nguage Model with JAX. https://github.com/kingoflolz/mesh-transformer-jax,\\nMay 2021.\\n[27] Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi, and\\nLuke Zettlemoyer. Rethinking the role of demonstrations: What makes in-context learning\\nwork? arXiv preprint arXiv:2202.12837, 2022.\\n[28] Robyn Speer, Joshua Chin, and Catherine Havasi. Conceptnet 5.5: An open multilingual graph\\nof general knowledge. singh 2002 (2016). arXiv preprint arxiv:1612.03975, 2016.\\n[29] Yichong Xu, Chenguang Zhu, Shuohang Wang, Siqi Sun, Hao Cheng, Xiaodong Liu, Jianfeng\\nGao, Pengcheng He, Michael Zeng, and Xuedong Huang. Human parity on commonsenseqa:\\nAugmenting self-attention with external attention. arXiv:2112.03254, December 2021. human\\nparity result on CommonsenseQA.\\n[30] Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-\\nTze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, et al. Lamda: Language models for\\ndialog applications. arXiv preprint arXiv:2201.08239, 2022.\\n[31] Stefan Palan and Christian Schitter. Proliﬁc. ac—a subject pool for online experiments. Journal\\nof Behavioral and Experimental Finance , 17:22–27, 2018.\\n[32] Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, and Denny Zhou. Self-\\nconsistency improves chain of thought reasoning in language models, 2022.\\n[33] Bernease Herman. The promise and peril of human evaluation for model interpretability. arXiv\\npreprint arXiv:1711.07414, 2017.\\n11'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-05-23T01:29:23+00:00', 'author': '', 'keywords': '', 'moddate': '2022-05-23T01:29:23+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'papers/2203.14465v2.pdf', 'total_pages': 30, 'page': 11, 'page_label': '12'}, page_content='[34] Alon Jacovi and Yoav Goldberg. Towards faithfully interpretable nlp systems: How should we\\ndeﬁne and evaluate faithfulness? ACL 2020, 2020.\\n[35] Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason\\nPhang, Horace He, Anish Thite, Noa Nabeshima, et al. The pile: An 800gb dataset of diverse\\ntext for language modeling. arXiv preprint arXiv:2101.00027, 2020.\\n[36] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint\\narXiv:1412.6980, 2014.\\n12'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-05-23T01:29:23+00:00', 'author': '', 'keywords': '', 'moddate': '2022-05-23T01:29:23+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'papers/2203.14465v2.pdf', 'total_pages': 30, 'page': 12, 'page_label': '13'}, page_content='Appendix\\nA CommonsenseQA Error Patterns\\nThroughout our experiments, we came across a variety of interesting failure cases for commonsense\\nreasoning. Note that all the ﬁnal answers are correct – however, we take issue with the reasoning\\nused in order to arrive at those answers.\\nA.1 Question Implies Answer\\nOne key failure case was answers in the form of “the answer must be something that is <question\\nproperty>. <answer> is <question property>. Therefore, the correct answer is <answer>.” In these\\ncases, the model fails to explain why the answer that it has chosen satisﬁes the question property.\\nQ: What might someone get from learning about science?\\nAnswer Choices:\\n(a) headache\\n(b) see things differently\\n(c) increased knowledge\\n(d) accidents\\n(e) appreciation of nature\\nA: The answer must be something that someone would get from learning about\\nscience. Learning about science would increase knowledge. Therefore, the\\nanswer is increased knowledge (c).\\nThese rationales, while perhaps useful to the model, read to us as opaque and unexplanatory.\\nA.2 Begging the Question\\nA related but stronger version of the previous failure case, while less common, is particularly\\nuninsightful. Sometimes the model will imply the answer that it has chosen in its question.\\nQ: The christian rock band was on a large stage, the singer’s foot got caught\\non a wire and he had a small what?\\nAnswer Choices:\\n(a) commit sin\\n(b) fail\\n(c) disobey\\n(d) stumble\\n(e) boo boo\\nA: The answer must be something that would cause the singer to stumble. The\\nsinger’s foot got caught on a wire, so he would stumble. Therefore, the\\nanswer is stumble (d).\\nA.3 Exercise to the Reader\\nA rare failure case is when the model ﬁnds it unnecessary to justify its answer. For example:\\nQ: Other than school or from text where can one gain knowledge?\\nAnswer Choices:\\n(a) book\\n(b) class\\n(c) thinking\\n(d) head\\n(e) meeting\\nA: The answer must be a place where someone can gain knowledge other than\\nschool or from text. The answer is meeting (e).\\n13'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-05-23T01:29:23+00:00', 'author': '', 'keywords': '', 'moddate': '2022-05-23T01:29:23+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'papers/2203.14465v2.pdf', 'total_pages': 30, 'page': 13, 'page_label': '14'}, page_content='A.4 World State Assertions\\nSometimes, the model will assume that it knows something about a subject or a person whose name\\nwas used as a variable. This leads to somewhat comical examples of reasoning. Part of the reason\\nfor this is that generally, there is an expectation that good rationales will leverage understanding of\\nmore general classes of objects and appeal to the relationship between those general classes and the\\nparticular instance. For example, the argument that “a person would typically feel exhilaration from\\nheights” is generally more compelling than the argument that “James would feel exhilaration from\\nheights.”\\nQ: A person’s hand hurts after hitting the bottle. What might happen to\\nthe bottle?\\nAnswer Choices:\\n(a) alcoholism\\n(b) breaking\\n(c) fly\\n(d) nausea\\n(e) ticket\\nA: The answer must be something that happens to the bottle. The bottle is\\nbroken. Therefore, the answer is breaking (b).\\nQ: James loved climbing. What emotion might he feel from heights?\\nAnswer Choices:\\n(a) exhaustion\\n(b) exhilaration\\n(c) falling down\\n(d) nervousness\\n(e) vertigo\\nA: The answer must be a feeling that would cause someone to climb a mountain.\\nJames would feel exhilaration from heights. Therefore, the answer is\\nexhilaration (b).\\nA.5 Red Herrings\\nSome errors in reasoning corresponded to the model making a statement which, while technically\\ntrue, is not useful in demonstrating the claim.\\nQ: A squirrel ran up to it’s home, where is the small dog likely to be\\nbarking up?\\nAnswer Choices:\\n(a) manhattan apartment\\n(b) wood\\n(c) neighbor’s yard\\n(d) tree\\n(e) basket\\nA: The answer must be a place where a small dog would be likely to be\\nbarking. Squirrels are not small dogs. Therefore, the answer is tree (d).\\nQ: John ran very fast, his feet touched the ground in what way?\\nAnswer Choices:\\n(a) behind\\n(b) loose\\n(c) feverishly\\n(d) slowly\\n(e) lightly\\nA: The answer must be a way of running that is different from the way John\\nran. Running is a fast, light-footed activity. Therefore, the answer is\\nlightly (e).\\n14'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-05-23T01:29:23+00:00', 'author': '', 'keywords': '', 'moddate': '2022-05-23T01:29:23+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'papers/2203.14465v2.pdf', 'total_pages': 30, 'page': 14, 'page_label': '15'}, page_content='A.6 Hint Short-cutting\\nIn the experiments where the model was shown some examples of “hints” during training, in order to\\nprevent it from losing the ability to perform rationalization over time, the model appeared to pick up\\non the fact that the ﬁnal answer would always correspond to the hinted answer. This led to answers\\nsuch as\\nQ: Billy bought coffee and waited for his wife to arrive from France.\\nWhere might he have been?\\nAnswer Choices:\\n(a) airport\\n(b) grocery store\\n(c) internet cafe\\n(d) supermarket\\n(e) train station\\nA: The answer must be a place where Billy could have been waiting for his\\nwife to arrive from France. The airport is a place where people can wait\\nfor flights. Therefore, the answer is train station (e).\\n15'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-05-23T01:29:23+00:00', 'author': '', 'keywords': '', 'moddate': '2022-05-23T01:29:23+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'papers/2203.14465v2.pdf', 'total_pages': 30, 'page': 15, 'page_label': '16'}, page_content='B Modiﬁed CQA Prompts\\nFor reference, we include our modiﬁed prompts based closely on those in [6].\\nQ: What do people use to absorb extra ink from a fountain pen?\\nAnswer Choices:\\n(a) shirt pocket\\n(b) calligrapher’s hand\\n(c) inkwell\\n(d) desk drawer\\n(e) blotter\\nA: The answer must be used to absorb extra ink. Blotters are designed to absorb\\nliquids. Therefore, the answer is blotter (e).\\nQ: What home entertainment equipment requires cable?\\nAnswer Choices:\\n(a) radio shack\\n(b) substation\\n(c) television\\n(d) cabinet\\n(e) desk\\nA: The answer must require cable. Cable is used to provide satellite channels to\\ntelevisions. Therefore, the answer is television (c).\\nQ: The fox walked from the city into the forest, what was it looking for?\\nAnswer Choices:\\n(a) pretty flowers\\n(b) hen house\\n(c) natural habitat\\n(d) storybook\\n(e) dense forest\\nA: The answer must be a reason for a fox to go into the forest. The forest is a fox’\\ns natural habitat. Therefore, the answer is natural habitat (c).\\nQ: Sammy wanted to go to where the people were. Where might he go?\\nAnswer Choices:\\n(a) populated areas\\n(b) race track\\n(c) desert\\n(d) apartment\\n(e) roadblock\\nA: The answer must be a place with many people. Populated areas, by definition, have\\na lot of people. Therefore, the answer is populated areas (a).\\nQ: Where do you put your grapes just before checking out?\\nAnswer Choices:\\n(a) mouth\\n(b) grocery cart\\n(c) super market\\n(d) fruit basket\\n(e) fruit market\\nA: The answer should be the place where grocery items are placed before checking out\\n. Of the above choices, grocery cart makes the most sense for holding grocery items.\\nTherefore, the answer is grocery cart (b).\\nQ: Google Maps and other highway and street GPS services have replaced what?\\nAnswer Choices:\\n(a) united states\\n(b) mexico\\n(c) countryside\\n(d) atlas\\n(e) oceans\\nA: The answer must be something that used to do what Google Maps and GPS services do\\n, which is give directions. Atlases were also used to give directions. Therefore,\\nthe answer is atlas (d).\\n16'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-05-23T01:29:23+00:00', 'author': '', 'keywords': '', 'moddate': '2022-05-23T01:29:23+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'papers/2203.14465v2.pdf', 'total_pages': 30, 'page': 16, 'page_label': '17'}, page_content='Q: Before getting a divorce, what did the wife feel who was doing all the work?\\nAnswer Choices:\\n(a) harder\\n(b) anguish\\n(c) bitterness\\n(d) tears\\n(e) sadness\\nA: The answer should be a feeling which would cause someone who was doing all the\\nwork to get divorced. If someone feels bitter towards their spouse, they are likely\\nto want a divorce. Therefore, the answer is bitterness (c).\\n17'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-05-23T01:29:23+00:00', 'author': '', 'keywords': '', 'moddate': '2022-05-23T01:29:23+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'papers/2203.14465v2.pdf', 'total_pages': 30, 'page': 17, 'page_label': '18'}, page_content='C Human-evaluated Test Prompts\\nWe also selected a random sampling of 50 questions which were correctly answered both few-shot\\nand by a STaR-trained model (without rationalization), as discussed in 4.4. Presented in a random\\norder, twenty crowdworkers preferred the STaR-generated answers. We reproduce the examples here\\nwith the few-shot rationale ﬁrst, the STaR-trained rationale second, and the human rationale third,\\nthough these were shufﬂed when presented to participants. We selected human answers from [3]’s\\noriginal split rationales where possible, ﬁnding that duplicate rationales were much more common in\\nthe new split rationales. For example, the explanation “Rivers ﬂow trough valleys,” appeared over\\n400 times verbatim in the new split dataset, and “This word was most relevant” appeared over 150\\ntimes. [ 3]’s dataset also includes explanations like “The only answer that makes sense” or “BOB\\nWILL NOT POKEMON CARDS WERE COMMON AND WORTHLESS BUT WRONG ABOUT\\nTHEM SO FEEL REALLY RARE TO DELAY” or restatements of the answer. We append the phrase\\n“Therefore, the answer is ANSWERTEXT (ANSWERLETTER)” with ANSWERTEXT replaced\\nby the correct answer’s text and ANSWERLETTER replaced by the correct answer letter. This is\\ndone 1) to make it less obvious that one of the answers is generated by a different source and 2) to\\nprioritize differences in rationales, not the answer format.\\nBefore the questions and after the consent form, we presented the following directions:\\nEach question will have three explanations for one answer, which is a pre-determined correct (\"best\") answer.\\nPlease rank the explanations for each question from best to worst (with 1 as the best and 3 as the worst) based on how well\\nthey justify the correct answer.\\nNote there are some typos and grammatical errors in the examples, in the questions, answers, and the explanation. Please\\ntreat the answer letter (a / b / c / d / e) at the end of the explanations as the correct answer letter, even if another\\nanswer is valid (and perhaps more reasonable), and even if it does not match with the answer text preceding it.\\nThe examples were subsampled and presented to the crowdworkers:\\nQ: Who is likely yo have a caring heart?\\nAnswer Choices:\\n(a) person\\n(b) species\\n(c) artichoke\\n(d) all mammals\\n(e) live animal\\nA: The answer must be a person. All mammals have a caring\\nheart. Therefore, the answer is person (a).\\nA: The answer must be someone who is likely to have a\\ncaring heart. Humans are the most likely to have a caring\\nheart. Therefore, the answer is person (a).\\nA: A person is the only human being capable of having a\\ncaring heart. Therefore, the answer is person (a).\\nQ: If a person wanted to become a good writer what should\\nthey do in college?\\nAnswer Choices:\\n(a) word sentence\\n(b) own animal\\n(c) read the newspaper\\n(d) catch cold\\n(e) study literature\\nA: The answer must be something that would help a person\\nbecome a good writer. The answer is study literature (e).\\nA: The answer must be something that would help a person\\nbecome a good writer. Literature is a form of study.\\nTherefore, the answer is study literature (e).\\nA: studying literature will give one skills of writing.\\nTherefore, the answer is study literature (e).\\nQ: The human always would have fun making up questions for\\nthe A.I. overlords, he found the task quite what?\\nAnswer Choices:\\n(a) do enjoy\\n(b) eat cake\\n(c) enjoy living\\n(d) get laid\\n(e) enjoyable\\nA: The answer must be something that the human would enjoy\\ndoing. The human enjoys making up questions for the A.I.\\noverlords. Therefore, the answer is enjoyable (e).\\nA: The answer must be something that the human found\\nenjoyable. The human found the task of making up questions\\nfor the A.I. overlords enjoyable. Therefore, the answer is\\nenjoyable (e).\\nA: Having fun is enjoyable. Therefore, the answer is\\nenjoyable (e).\\nQ: What happens when I make a pig of myself eating breakfast\\n?\\nAnswer Choices:\\n(a) full stomach\\n(b) gain energy\\n(c) dieting\\n(d) have energy\\n(e) feeling satisfied\\nA: The answer must be a feeling which would cause someone\\nto feel full after eating breakfast. The answer is full\\nstomach (a).\\nA: The answer must be a feeling which would cause someone\\nto make a pig of themselves eating breakfast. If someone\\nfeels full, they are likely to stop eating. Therefore, the\\nanswer is full stomach (a).\\nA: A colloquial phrase for over indulging in food.\\nTherefore, the answer is full stomach (a).\\nQ: In order to see a story on the big screen what must you\\ndo?\\nAnswer Choices:\\n(a) go to movies\\n(b) visualize\\n(c) reading\\n(d) open book\\n(e) sketching a picture\\nA: The answer must be something that you do to see a story\\non the big screen. Movies are a way to see a story on the\\nbig screen. Therefore, the answer is movies (a).\\nA: The answer must be something that is required to see a\\nstory on the big screen. Movies are a form of storytelling.\\nTherefore, the answer is go to movies (a).\\nA: The big screen is a colloquial term meaning a movie\\ntheater. Therefore, the answer is go to movies (a).\\nQ: What is the opposite of remembering?\\nAnswer Choices:\\n(a) answering question\\n(b) stopping to think\\n(c) knowing\\n(d) closing eyes\\n18'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-05-23T01:29:23+00:00', 'author': '', 'keywords': '', 'moddate': '2022-05-23T01:29:23+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'papers/2203.14465v2.pdf', 'total_pages': 30, 'page': 18, 'page_label': '19'}, page_content='(e) forgetting\\nA: The answer must be something that is the opposite of\\nremembering. The answer is forgetting (e).\\nA: The answer must be a feeling which would cause someone\\nto not remember something. If someone forgets something,\\nthey are likely to not remember it. Therefore, the answer\\nis forgetting (e).\\nA: Health complications. Therefore, the answer is\\nforgetting (e).\\nQ: If you aren’t experiencing curiosity and want to ensure\\nviewing enjoyment, what may you watch?\\nAnswer Choices:\\n(a) examine thing\\n(b) stare into a mirror\\n(c) go somewhere\\n(d) hear news\\n(e) see favorite show\\nA: The answer must be something that will make you want to\\nwatch. If you are not experiencing curiosity, you may want\\nto watch something that will make you want to watch.\\nTherefore, the answer is watch (e).\\nA: The answer must be something that will not cause you to\\nwant to watch. If you are not experiencing curiosity, you\\nare likely to want to watch something that is entertaining.\\nTherefore, the answer is favorite show (e).\\nA: see favorite show can you do in front of a tv if you\\nhave curiosity about what is being displayed on the tv.\\nTherefore, the answer is favorite show (e).\\nQ: The carotid is a main artery susceptible to damage where?\\nAnswer Choices:\\n(a) neck\\n(b) face\\n(c) busy city\\n(d) torso\\n(e) human body\\nA: The answer must be a place where the carotid artery is\\nsusceptible to damage. The carotid artery is located in the\\nneck. Therefore, the answer is neck (a).\\nA: The answer must be a place where the carotid is\\nsusceptible to damage. The carotid is susceptible to damage\\nin the neck. Therefore, the answer is neck (a).\\nA: The carotid artery is located in the neck. Therefore,\\nthe answer is neck (a).\\nQ: The king needed to feel safe, where did he go?\\nAnswer Choices:\\n(a) castle\\n(b) throne room\\n(c) deck of cards\\n(d) fort\\n(e) court\\nA: The answer must be a place where the king feels safe.\\nThe king’s castle is a place where he feels safe. Therefore,\\nthe answer is castle (a).\\nA: The answer must be a place where the king can feel safe.\\nCastles are places where kings feel safe. Therefore, the\\nanswer is castle (a).\\nA: In general, King live in castle. Therefore, the answer\\nis castle (a).\\nQ: What is the result of instituting civil action?\\nAnswer Choices:\\n(a) bad feelings\\n(b) confusion\\n(c) going to court\\n(d) hurt feelings\\n(e) sandwiches\\nA: The answer must be a result of instituting civil action.\\nThe result of instituting civil action is a lawsuit.\\nTherefore, the answer is going to court (c).\\nA: The answer must be a result of instituting civil action.\\nIf someone goes to court, they are likely to feel hurt\\nfeelings. Therefore, the answer is going to court (c).\\nA: Anger and Trial will not institute a court case.\\nTherefore, the answer is going to court (c).\\nQ: The person wanted to fix his car, what did he order?\\nAnswer Choices:\\n(a) selling things\\n(b) gas\\n(c) feel resentful\\n(d) machine part\\n(e) cross street\\nA: The answer must be something that would help the person\\nfix his car. The person would not want to sell things,\\nbecause he would not want to give up his car. Therefore,\\nthe answer is machine part (d).\\nA: The answer must be something that would be used to fix a\\ncar. Machine parts are used to fix cars. Therefore, the\\nanswer is machine part (d).\\nA: A machine part is the only thing listed that would fix\\nthe car. Therefore, the answer is machine part (d).\\nQ: He always traveled up the mountain when winter arrived,\\nhe had a passion and desire to do what?\\nAnswer Choices:\\n(a) play volleyball\\n(b) play hockey\\n(c) ski\\n(d) knit\\n(e) skate\\nA: The answer must be something that the person does when\\nwinter arrives. The answer is skiing (c).\\nA: The answer must be something that the person did when\\nwinter arrived. The answer is ski (c).\\nA: Skiing is the only option that is done on a mountain.\\nTherefore, the answer is ski (c).\\nQ: Bob thought that his Pokemon cards were common and\\nworthless, but he was wrong about them. They were really\\nwhat?\\nAnswer Choices:\\n(a) personal\\n(b) special\\n(c) shiny\\n(d) rare\\n(e) irregular\\nA: The answer must be something that is not common and\\nworthless. Pokemon cards are not common and worthless.\\nTherefore, the answer is rare (d).\\nA: The answer must be something that are rare. Pokemon\\ncards are rare. Therefore, the answer is rare (d).\\nA: BOB WILL NOT POKEMON CARDS WERE COMMON AND WORTHLESS BUT\\nWRONG ABOUT THEM SO FEEL REALLY RARE TO DELAY. Therefore,\\nthe answer is rare (d).\\nQ: If you’re lighting a candle to see what are you in?\\nAnswer Choices:\\n(a) dimly lit room\\n(b) synagogue\\n(c) sunlight\\n(d) birthday cake\\n(e) bedroom\\nA: The answer must be a place where you can see what you\\nare in. The answer is a room with dim lighting. Therefore,\\nthe answer is dimly lit room (a).\\nA: The answer must be a place where you can see a candle.\\nCandles are used to see things in the dark. Therefore, the\\nanswer is dimly lit room (a).\\nA: Lighting or illumination is the deliberate use of light\\nto achieve a practical or aesthetic effect. Lighting\\nincludes the use of both artificial light sources like\\nlamps ... Therefore, the answer is dimly lit room (a).\\nQ: The person was thirsty, what should he have done?\\nAnswer Choices:\\n(a) eat\\n(b) sunny days\\n(c) compliments\\n(d) drink water\\n(e) all kinds of things\\nA: The answer must be something that would make the person\\nfeel better. If the person is thirsty, he should drink\\nwater. Therefore, the answer is drink water (d).\\nA: The answer must be something that would make someone\\nfeel better if they were thirsty. If someone is thirsty,\\nthey are likely to want to drink water. Therefore, the\\nanswer is drink water (d).\\nA: The person was thirsty, so drink water. Therefore, the\\nanswer is drink water (d).\\nQ: Where would you find many products for sale?\\nAnswer Choices:\\n(a) market\\n(b) cost one dollar\\n(c) shelf\\n(d) salon\\n(e) demonstration\\nA: The answer must be a place where many products are sold.\\nA market is a place where many products are sold.\\nTherefore, the answer is market (a).\\nA: The answer must be a place with a lot of products for\\nsale. Markets are the most common place to buy products.\\nTherefore, the answer is market (a).\\nA: The only place products are for sale are in markets and\\nstores. Therefore, the answer is market (a).\\nQ: If man and mammoth wanted to meet, they would probably\\nhave to do it during when?\\nAnswer Choices:\\n(a) a baby shower\\n(b) ancient history\\n19'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-05-23T01:29:23+00:00', 'author': '', 'keywords': '', 'moddate': '2022-05-23T01:29:23+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'papers/2203.14465v2.pdf', 'total_pages': 30, 'page': 19, 'page_label': '20'}, page_content='(c) storybook\\n(d) stone age\\n(e) forest\\nA: The answer must be a time when man and mammoth would\\nmeet. Stone age is the time when man and mammoth lived.\\nTherefore, the answer is stone age (d).\\nA: The answer must be a time when man and mammoth would be\\nable to meet. The stone age was a time when man and mammoth\\nwould be able to meet. Therefore, the answer is stone age\\n(d).\\nA: During the stone age is the only time mammoths lived.\\nTherefore, the answer is stone age (d).\\nQ: They prided themselves on being a wealth of knowledge,\\nand that’s why many chose to attend their what?\\nAnswer Choices:\\n(a) book store\\n(b) university\\n(c) meeting\\n(d) class\\n(e) encyclopedia\\nA: The answer must be a place where people go to learn\\nabout things. The answer is university (b).\\nA: The answer must be a place where people go to learn\\nabout things. Universities are places where people go to\\nlearn about things. Therefore, the answer is university (b).\\nA: because in the university you get more knowledge.\\nTherefore, the answer is university (b).\\nQ: what happens when visiting museum that have narrow halls?\\nAnswer Choices:\\n(a) sore feet\\n(b) amazement\\n(c) retrospection\\n(d) boredom\\n(e) crowding\\nA: The answer must be something that happens when visiting\\na museum that has narrow halls. Crowding is a common\\nproblem in museums. Therefore, the answer is crowding (e).\\nA: The answer must be something that would cause someone to\\nhave sore feet if they are walking in narrow halls. If\\nsomeone feels cramped, they are likely to want to go\\nsomewhere else. Therefore, the answer is crowding (e).\\nA: museum is one of the crowdest place. Therefore, the\\nanswer is crowding (e).\\nQ: On their hike they brought a filtering straw, they were\\nworried about germs in the what?\\nAnswer Choices:\\n(a) make sick\\n(b) doctor\\n(c) water\\n(d) stream\\n(e) mouth\\nA: The answer must be something that can filter out germs.\\nFiltering straws are used to filter out germs. Therefore,\\nthe answer is filtering straw (c).\\nA: The answer must be something that would cause someone to\\nbring a filtering straw on a hike. Filtering straws are\\nused to filter water. Therefore, the answer is water (c).\\nA: straw it something used to drink water. Therefore, the\\nanswer is water (c).\\nQ: If I am not good at playing poker, I will begin what if I\\nplay?\\nAnswer Choices:\\n(a) discord\\n(b) social event\\n(c) losing money\\n(d) competition\\n(e) heightened awareness\\nA: The answer must be something that will happen if you are\\nnot good at playing poker. If you are not good at playing\\npoker, you will begin to lose money. Therefore, the answer\\nis losing money (c).\\nA: The answer must be something that will help you if you\\nare not good at playing poker. If you are losing money, you\\nare likely to want to stop playing. Therefore, the answer\\nis losing money (c).\\nA: Poker is a game in which people lose their money.\\nTherefore, the answer is losing money (c).\\nQ: People are meeting people, what is the likely physical\\ngesture?\\nAnswer Choices:\\n(a) introducing yourself\\n(b) smiling\\n(c) shake hands\\n(d) share a hug\\n(e) talking\\nA: The answer must be a physical gesture that is used to\\ngreet someone. The answer is handshake (c).\\nA: The answer must be a gesture used to meet people. If\\nsomeone wants to meet people, they are likely to want to\\nshake hands. Therefore, the answer is shaking hands (c).\\nA: A handshake is a globally widespread. Therefore, the\\nanswer is shaking hands (c).\\nQ: What do dogs get that make them itch?\\nAnswer Choices:\\n(a) bones\\n(b) fleas\\n(c) bad breath\\n(d) wagging tails\\n(e) four legs\\nA: The answer must be something that makes dogs itch. Dogs\\nget fleas, which make them itch. Therefore, the answer is\\nfleas (b).\\nA: The answer must be something that dogs get that make\\nthem itch. Fleas are a common cause of dog itch. Therefore,\\nthe answer is fleas (b).\\nA: fleas makes you itch. Therefore, the answer is fleas (b).\\nQ: What is ignorance often equated with?\\nAnswer Choices:\\n(a) bliss\\n(b) hostility\\n(c) peace\\n(d) accidents\\n(e) find truth\\nA: The answer must be something that is often equated with\\nignorance. Bliss is often equated with ignorance. Therefore,\\nthe answer is bliss (a).\\nA: The answer must be a feeling which would cause someone\\nto be ignorant. If someone feels bliss, they are likely to\\nbe ignorant. Therefore, the answer is bliss (a).\\nA: bliss comes from ignorance. Therefore, the answer is\\nbliss (a).\\nQ: The two teams were competing all the time, what likely\\ndeveloped between them?\\nAnswer Choices:\\n(a) accomplishment\\n(b) enemies\\n(c) rivalry\\n(d) friendship\\n(e) bad blood\\nA: The answer must be something that developed between the\\ntwo teams. The teams were competing all the time. Therefore,\\nthe answer is rivalry (c).\\nA: The answer must be a feeling which would cause two teams\\nto compete all the time. Rivalry is a common feeling which\\nwould cause two teams to compete all the time. Therefore,\\nthe answer is rivalry (c).\\nA: likely developed between them of rivalry. Therefore, the\\nanswer is rivalry (c).\\nQ: James doesn’t want a relationship. He thinks that loving\\nanother will only create what?\\nAnswer Choices:\\n(a) satisfaction\\n(b) jealousy\\n(c) happiness\\n(d) wonderful\\n(e) quiet\\nA: The answer must be something that would make James feel\\nbad about not having a relationship. If someone feels\\njealous, they are likely to want a relationship. Therefore,\\nthe answer is jealousy (b).\\nA: The answer must be a feeling which would cause someone\\nwho doesn’t want a relationship to not want one. If someone\\nfeels jealous of another, they are likely to not want a\\nrelationship. Therefore, the answer is jealousy (b).\\nA: Jealousy is a negative emotion. Therefore, the answer is\\njealousy (b).\\nQ: The had a reputation for celebrating in a debaucherous\\nmanner, what often happened as a result?\\nAnswer Choices:\\n(a) drunkenness\\n(b) enjoyment\\n(c) have fun\\n(d) hang over\\n(e) fighting\\nA: The answer must be a result of a celebration. The answer\\nis drunkenness (a).\\nA: The answer must be something that would cause someone to\\nhave a reputation for celebrating in a debaucherous manner.\\nAlcohol is a depressant. Therefore, the answer is\\ndrunkenness (a).\\n20'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-05-23T01:29:23+00:00', 'author': '', 'keywords': '', 'moddate': '2022-05-23T01:29:23+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'papers/2203.14465v2.pdf', 'total_pages': 30, 'page': 20, 'page_label': '21'}, page_content='A: it is normal not a good idea to get drunk. Therefore,\\nthe answer is drunkenness (a).\\nQ: The paper towels are usually in plain sight in the\\nkitchen, where did the dad go next to find them?\\nAnswer Choices:\\n(a) pantry\\n(b) cabinet\\n(c) grocery store\\n(d) waste bin\\n(e) market\\nA: The answer must be a place where the dad would go to\\nfind the paper towels. The pantry is a place where food is\\nstored. Therefore, the answer is pantry (a).\\nA: The answer must be a place where paper towels are stored.\\nPantries are the most likely place to store paper towels.\\nTherefore, the answer is pantry (a).\\nA: The pantry is in the kitchen and stores things.\\nTherefore, the answer is pantry (a).\\nQ: My house is very dry, with little water in the air. What\\ntool should I use to fix this?\\nAnswer Choices:\\n(a) humidifier\\n(b) surface of earth\\n(c) skating rink\\n(d) ice cube\\n(e) burn it to the ground\\nA: The answer must be a tool that will make the air in your\\nhouse moist. A humidifier is a tool that will make the air\\nin your house moist. Therefore, the answer is humidifier (\\na).\\nA: The answer must be something that would make the air in\\nmy house more humid. A humidifier is used to make the air\\nin a room more humid. Therefore, the answer is humidifier (\\na).\\nA: This word was most relevant. Therefore, the answer is\\nhumidifier (a).\\nQ: A motorcycle is quicker because it isn’t as what as a car\\n?\\nAnswer Choices:\\n(a) heavy\\n(b) shiny\\n(c) red\\n(d) obesity\\n(e) expensive\\nA: The answer must be something that is not as heavy as a\\ncar. A motorcycle is lighter than a car. Therefore, the\\nanswer is not heavy (a).\\nA: The answer must be something that is not as quick as a\\nmotorcycle. Motorcycles are not as heavy as cars. Therefore,\\nthe answer is heavy (a).\\nA: Because it is much smaller. Therefore, the answer is\\nheavy (a).\\nQ: The baseball team was playing ball. It was time for the\\npitcher to act. How did the pitcher get the ball to the\\nbatter?\\nAnswer Choices:\\n(a) lose\\n(b) injury\\n(c) competition\\n(d) throwing\\n(e) rolled\\nA: The answer must be a way to get the ball to the batter.\\nThe pitcher is the one who throws the ball. Therefore, the\\nanswer is throwing (d).\\nA: The answer must be a way for the pitcher to get the ball\\nto the batter. The pitcher is the person who is\\nresponsible for getting the ball to the batter. Therefore,\\nthe answer is throwing (d).\\nA: The pitcher threw the ball to the batter so that he\\ncould hit it. Therefore, the answer is throwing (d).\\nQ: In what sort of container should you keep your ficus?\\nAnswer Choices:\\n(a) refigerator\\n(b) screened porch\\n(c) clay pot\\n(d) shady places\\n(e) arboretum\\nA: The answer must be a container that keeps the ficus in a\\nshady place. Clay pots are used to keep plants in shady\\nplaces. Therefore, the answer is clay pot (c).\\nA: The answer must be a container in which ficus can be\\nkept. Ficus are tropical plants. Therefore, the answer is\\nclay pot (c).\\nA: Clay pot is the only type of container listed. Therefore,\\nthe answer is clay pot (c).\\nQ: James was cooking s stew in his apartment kitchen. Where\\nmight he look for a potato.\\nAnswer Choices:\\n(a) restaurants\\n(b) casserole\\n(c) root cellar\\n(d) pockets\\n(e) pantry\\nA: The answer must be a place where potatoes are stored.\\nThe answer is pantry (e).\\nA: The answer must be a place where potatoes are stored.\\nPantries are the most likely place to store potatoes.\\nTherefore, the answer is pantry (e).\\nA: apartments do not often have root cellars, and are not\\nrestaurants. a pantry is closest to James inside his\\napartment. Therefore, the answer is pantry (e).\\nQ: If ships are in a storm and the sun comes out, what has\\nhappened?\\nAnswer Choices:\\n(a) sinks\\n(b) cross river\\n(c) collide\\n(d) bottom out\\n(e) carry people\\nA: The answer must be a thing that happens when ships are\\nin a storm and the sun comes out. Ships are in a storm when\\nthey are in the middle of the ocean. Therefore, the answer\\nis bottom out (d).\\nA: The answer must be something that happens when the sun\\ncomes out. The sun is a source of energy. Therefore, the\\nanswer is bottom out (d).\\nA: the sun comes out so its not a storm any more. Therefore,\\nthe answer is bottom out (d).\\nQ: Where do all marmot live before they are captured?\\nAnswer Choices:\\n(a) switzerland\\n(b) spain\\n(c) vancouver island\\n(d) wilderness\\n(e) australia\\nA: The answer must be a place where marmots live. Marmots\\nlive in the wilderness. Therefore, the answer is wilderness\\n(d).\\nA: The answer must be a place where marmots are found. The\\nwilderness is the most likely place to find marmots.\\nTherefore, the answer is wilderness (d).\\nA: Marmots don’t live in Australia or Switzerland.\\nTherefore, the answer is wilderness (d).\\nQ: Where does one wait on a bench for something on tracks?\\nAnswer Choices:\\n(a) train station\\n(b) state park\\n(c) crosswalk\\n(d) bus stop\\n(e) bus depot\\nA: The answer must be a place where one waits for a train,\\nbus, or other vehicle. The answer is train station (a).\\nA: The answer must be a place where one can wait for a\\ntrain. Train stations are places where trains stop.\\nTherefore, the answer is train station (a).\\nA: Only trains are on tracks. Therefore, the answer is\\ntrain station (a).\\nQ: What can planning vacation lead to between a couple when\\nthey disagree?\\nAnswer Choices:\\n(a) relaxation\\n(b) enjoying\\n(c) arguments\\n(d) going abroad\\n(e) spending money\\nA: The answer must be something that can happen when a\\ncouple disagrees. Arguments are a common result of\\ndisagreements. Therefore, the answer is arguments (c).\\nA: The answer must be a result of planning vacation.\\nArguments are a common result of planning vacation.\\nTherefore, the answer is arguments (c).\\nA: Arguments are the direct result of disagreements.\\nTherefore, the answer is arguments (c).\\nQ: A sloth climbs slow, but it can eventually reach all the\\nway up where?\\nAnswer Choices:\\n(a) forest canopy\\n(b) universe\\n(c) tropical rainforest\\n(d) large trees\\n(e) nature preserve\\n21'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-05-23T01:29:23+00:00', 'author': '', 'keywords': '', 'moddate': '2022-05-23T01:29:23+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'papers/2203.14465v2.pdf', 'total_pages': 30, 'page': 21, 'page_label': '22'}, page_content='A: The answer must be a place where a sloth can climb. The\\nforest canopy is the highest point in the forest. Therefore,\\nthe answer is forest canopy (a).\\nA: The answer must be a place where a sloth can climb. The\\ncanopy of a forest is the highest point in a forest.\\nTherefore, the answer is canopy of a forest (a).\\nA: The sloth is a mammal that lives in the trees of South\\nAmerica. They have a slow and peculiar way of moving about.\\nIn fact, they are the slowest land mammals and travel very\\nsluggishly on the ground (0.025 m/s-0.050 m/s). Therefore,\\nthe answer is canopy of a forest (a).\\nQ: Where in America is known for theaters?\\nAnswer Choices:\\n(a) new york city\\n(b) shopping mall\\n(c) downtown area\\n(d) skating rink\\n(e) military base\\nA: The answer must be a place where people go to see movies.\\nTheaters are places where people go to see movies.\\nTherefore, the answer is theater (a).\\nA: The answer must be a place with a lot of theaters. New\\nYork City is known for its many theaters. Therefore, the\\nanswer is New York City (a).\\nA: New York City is the main region in US known for\\ntheaters. Therefore, the answer is New York City (a).\\nQ: What does hearing someone play violin beautifully make\\nyou?\\nAnswer Choices:\\n(a) buy earplugs\\n(b) inspiring\\n(c) guitar\\n(d) make music\\n(e) like music\\nA: The answer must be something that makes you feel\\ninspired. Hearing someone play violin beautifully makes you\\nfeel inspired. Therefore, the answer is inspiring (b).\\nA: The answer must be a feeling which would cause someone\\nto want to hear someone play violin beautifully. Music is a\\nform of art. Therefore, the answer is inspiring (b).\\nA: Hearing someone play a violin beautifully makes me\\ninspired. Therefore, the answer is inspiring (b).\\nQ: What emotion does committing murder induce?\\nAnswer Choices:\\n(a) fear\\n(b) go to jail\\n(c) problems\\n(d) cruel\\n(e) dead\\nA: The answer must be a feeling that would cause someone to\\ncommit murder. Murder is a crime. Therefore, the answer is\\nfear (a).\\nA: The answer must be a feeling which would cause someone\\nto commit murder. If someone feels fear, they are likely to\\nwant to avoid committing murder. Therefore, the answer is\\nfear (a).\\nA: The only feeling. Therefore, the answer is fear (a).\\nQ: Sitting to close while watching TV can cause what sort of\\npain?\\nAnswer Choices:\\n(a) brain problems\\n(b) laziness\\n(c) get fat\\n(d) headache\\n(e) laughter\\nA: The answer must be a pain that comes from sitting too\\nclose to the TV. Sitting too close to the TV can cause\\nheadaches. Therefore, the answer is headache (d).\\nA: The answer must be a type of pain which can occur if you\\nare sitting too close to the TV. Headaches are a common\\nside effect of sitting too close to the TV. Therefore, the\\nanswer is headache (d).\\nA: the process of aging in humans. Therefore, the answer is\\nheadache (d).\\nQ: What might people do to protect their legs from getting\\ndirty on the farm?\\nAnswer Choices:\\n(a) wear jeans\\n(b) milk cow\\n(c) believe in god\\n(d) avoid mud\\n(e) plant flowers\\nA: The answer must be something that people do to protect\\ntheir legs from getting dirty. Of the above choices,\\nwearing jeans is the most logical. Therefore, the answer is\\njeans (a).\\nA: The answer must be something that would protect legs\\nfrom getting dirty on the farm. Jeans are used to protect\\nlegs from getting dirty. Therefore, the answer is wear\\njeans (a).\\nA: Jeans cover over legs. Therefore, the answer is wear\\njeans (a).\\nQ: What types of stoves are there?\\nAnswer Choices:\\n(a) these twos are irrelevent to the questions\\n(b) warm pot of beans\\n(c) brown meat\\n(d) warm room\\n(e) gas or electric\\nA: The answer must be a type of stove. The stove is used to\\nheat food. Therefore, the answer is gas or electric (e).\\nA: The answer must be a type of stove. Stoves are used to\\nwarm rooms. Therefore, the answer is electric (e).\\nA: stoves are gas or electric. Therefore, the answer is gas\\nor electric (e).\\nQ: In what geological feature will you find fungus growing?\\nAnswer Choices:\\n(a) shower stall\\n(b) toenails\\n(c) basement\\n(d) forrest\\n(e) cave\\nA: The answer must be a place where fungus grows. The\\nanswer is cave (e).\\nA: The answer must be a place where fungus grows. Fungus\\ngrows in caves. Therefore, the answer is cave (e).\\nA: Rivers flow trough valleys. Therefore, the answer is\\ncave (e).\\nQ: What is our sun considered to be the center of?\\nAnswer Choices:\\n(a) universe\\n(b) solar system\\n(c) outer space\\n(d) star\\n(e) beans\\nA: The answer must be something that is considered to be\\nthe center of the universe. The sun is the center of the\\nsolar system. Therefore, the answer is solar system (b).\\nA: The answer must be a place where the sun is the center\\nof. The sun is the center of the solar system. Therefore,\\nthe answer is solar system (b).\\nA: It is the best option because it is the closest.\\nTherefore, the answer is solar system (b).\\nQ: What are people in a library likely doing?\\nAnswer Choices:\\n(a) talk to each other\\n(b) board ships\\n(c) study books\\n(d) suffer hunger\\n(e) playing games\\nA: The answer must be something that people in a library\\nare likely to be doing. People in a library are likely to\\nbe studying books. Therefore, the answer is study books (c).\\nA: The answer must be something that is likely to happen in\\na library. Libraries are places where people study books.\\nTherefore, the answer is studying books (c).\\nA: Books shelved as study-skills: How to Become a Straight-\\nA Student by Cal Newport, The Study Skills Handbook by\\nStella Cottrell, Make It Stick: The Scienc... Therefore,\\nthe answer is study books (c).\\nQ: A shark isn’t something you have to worry about at the\\nlake in the woods, but there are plenty of dangers in the\\nwhat?\\nAnswer Choices:\\n(a) great oceans\\n(b) great outdoors\\n(c) tropical waters\\n(d) pacific ocean\\n(e) gulf of mexico\\nA: The answer must be something that is dangerous in the\\ngreat outdoors. The great outdoors is a place where there\\nare many dangers. Therefore, the answer is great outdoors (\\nb).\\nA: The answer must be a place where sharks are a danger.\\nLakes in the woods are not a place where sharks are a\\ndanger. Therefore, the answer is great outdoors (b).\\nA: The great outdoors is the only choice where you would\\nfind a lake in the woods. Therefore, the answer is great\\noutdoors (b).\\nQ: You can share files with someone if you have a connection\\nto a what?\\n22'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-05-23T01:29:23+00:00', 'author': '', 'keywords': '', 'moddate': '2022-05-23T01:29:23+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'papers/2203.14465v2.pdf', 'total_pages': 30, 'page': 22, 'page_label': '23'}, page_content='Answer Choices:\\n(a) freeway\\n(b) radio\\n(c) wires\\n(d) computer network\\n(e) electrical circuit\\nA: The answer must be a way to share files. The answer is\\ncomputer network (d).\\nA: The answer must be a connection which allows you to\\nshare files. A computer network is a collection of\\ncomputers which are connected to each other. Therefore, the\\nanswer is computer network (d).\\nA: Files can be transmitted over a computer network, and\\nnot a freeway. \"wires\" doesnt make any sense. Therefore,\\nthe answer is computer network (d).\\nQ: What can an injury happen to?\\nAnswer Choices:\\n(a) emergency room\\n(b) doctor’s office\\n(c) living thing\\n(d) bones\\n(e) accident\\nA: The answer must be something that can happen to a living\\nthing. The answer is living thing (c).\\nA: The answer must be something that can happen to a living\\nthing. If someone is injured, they are likely to want to\\ngo to the emergency room. Therefore, the answer is living\\nthing (c).\\nA: Living things feel pain, and therefore can be injured.\\nTherefore, the answer is living thing (c).\\n23'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-05-23T01:29:23+00:00', 'author': '', 'keywords': '', 'moddate': '2022-05-23T01:29:23+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'papers/2203.14465v2.pdf', 'total_pages': 30, 'page': 23, 'page_label': '24'}, page_content='D Example Rationalizations on CQA\\nWe include a randomly sampled set of rationalizations which the model is able to produce before\\nﬁne-tuning. We observe that sometimes, the model constructs an argument roughly of the form “the\\nanswer must have a set of properties. correct answer has those properties. therefore, the answer is\\ncorrect answer.” This structure of argument is fairly standard, but given that the model originally\\nanswered those questions incorrectly, it resembles template-matching more than reasoning. The\\ntechnique of rephrasing the question in a simpler way before answering it directly is also often\\neffective. In general, the rationales from the pretrained model’s rationalization appears to be of\\nsimilar quality to the pretrained model’s rationale generation. We reference the pretrained model here\\nrather than the STaR-trained model, as the ﬁnal iteration’s rationales are the ones which the model\\nstruggled to learn from – that is, if the rationales were good, then we might expect that the model\\nwould have already incorporated them into its rationale generation.\\nQ: Why do I eat just enough when eating breakfast?\\nAnswer Choices:\\n(a) gas\\n(b) full stomach\\n(c) feeling satisfied (CORRECT)\\n(d) have energy\\n(e) bloated\\nA: The answer must be a reason for eating just enough when eating breakfast. Eating\\njust enough is a way to feel satisfied. Therefore, the answer is feeling satisfied (\\nc).\\nQ: How is a dog likely to communicate with another dog?\\nAnswer Choices:\\n(a) reproduce\\n(b) bark (CORRECT)\\n(c) do many things\\n(d) clone\\n(e) jump up\\nA: The answer must be a way for a dog to communicate with another dog. Dogs bark to\\ncommunicate with each other. Therefore, the answer is bark (b).\\nQ: If a person is trying to weasel out of punishment for a crime, where are they\\nlikely to be?\\nAnswer Choices:\\n(a) court room (CORRECT)\\n(b) cherry tree\\n(c) chicken coop\\n(d) natural history museum\\n(e) jail\\nA: The answer must be a place where a person would be trying to get out of\\npunishment for a crime. The answer is court room (a).\\nQ: What would easily hide a snake?\\nAnswer Choices:\\n(a) living room of a house\\n(b) kew gardens\\n(c) terrarium\\n(d) thick forest (CORRECT)\\n(e) tropical forest\\nA: The answer must be something that would hide a snake. The answer is thick forest\\n(d).\\nQ: The person didn’t like the varying size of the cake slices handed out, she demand\\nmore what?\\nAnswer Choices:\\n(a) fairness (CORRECT)\\n(b) candies\\n(c) compliments\\n(d) well fed\\n(e) arguements\\n24'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-05-23T01:29:23+00:00', 'author': '', 'keywords': '', 'moddate': '2022-05-23T01:29:23+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'papers/2203.14465v2.pdf', 'total_pages': 30, 'page': 24, 'page_label': '25'}, page_content='A: The answer must be a reason for the person to demand more cake. The person is not\\nhappy with the size of the cake slices. Therefore, the answer is fairness (a).\\nQ: In America, the courthouse of any given town has a likely location, where is it?\\nAnswer Choices:\\n(a) electrical circuit\\n(b) getting married\\n(c) capital city\\n(d) center of town (CORRECT)\\n(e) michigan\\nA: The answer must be a location where a courthouse is located. The capital city is\\nthe location of the capital of a state. Therefore, the answer is center of town (d).\\nQ: Where might I find a bowl nestled among other bowls in the kitchen?\\nAnswer Choices:\\n(a) refrigerator\\n(b) kitchen top\\n(c) cupboard (CORRECT)\\n(d) dishwasher\\n(e) apartment\\nA: The answer must be a place where a bowl is found. The kitchen is a place where\\nbowls are found. Therefore, the answer is cupboard (c).\\nQ: Which state has blue ridge mountains that are called Blue Mountain?\\nAnswer Choices:\\n(a) appalachians\\n(b) virginia\\n(c) pennsylvania (CORRECT)\\n(d) north carolina\\n(e) montana\\nA: The answer must be a state with mountains. The answer is Pennsylvania (c).\\nQ: Where could you find many radio studio?\\nAnswer Choices:\\n(a) radio station\\n(b) country\\n(c) clear area\\n(d) office building\\n(e) large city (CORRECT)\\nA: The answer must be a place where many radio studios are located. Radio studios\\nare used to broadcast radio programs. Therefore, the answer is large city (e).\\nQ: Where would someone bring you a cup?\\nAnswer Choices:\\n(a) apartment\\n(b) closet\\n(c) restaurant (CORRECT)\\n(d) table\\n(e) party\\nA: The answer must be a place where someone would bring you a cup. A restaurant is a\\nplace where people bring cups. Therefore, the answer is restaurant (c).\\n25'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-05-23T01:29:23+00:00', 'author': '', 'keywords': '', 'moddate': '2022-05-23T01:29:23+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'papers/2203.14465v2.pdf', 'total_pages': 30, 'page': 25, 'page_label': '26'}, page_content='E STaR Without Rationalization (Rationale Generation Bootstrapping)\\nFor convenience, we include the rationale generation bootstrapping algorithm alone here. However, it\\nis simply Algorithm 1, with the blue components removed.\\nAlgorithm 2 Rationale Generation Bootstrapping (STaR without rationalization)\\nInput M: a pretrained LLM; dataset D = {(xi, yi)}D\\ni=1 (w/ few-shot prompts)\\n1: M0 ← M # Copy the original model\\n2: for n in 1...N do # Outer loop\\n3: (ˆri, ˆyi) ← Mn−1(xi) ∀i ∈ [1, D] # Perform rationale generation\\n4: Dn ← {(xi, ˆri, yi) | i ∈ [1, D] ∧ ˆyi = yi} # Filter rationales using ground truth answers\\n5: Mn ← train(M, Dn) # Finetune the original model on the correct solutions - inner loop\\n6: end for\\nF Case Study\\nQ: John wants to eat at an Indian\\nrestaurant in on top of the space\\nneedle, where would he have to go?\\nAnswer Choices:\\n(a) reservation (b) curry\\n(c) britsh town (d) seattle\\n(e) large city\\nA: The answer must be a place with\\na space needle. The space needle is\\na landmark in Seattle. Therefore,\\nthe answer is Seattle (d).\\nQ: I put a towel in a cardboard box for\\nthe kitten, I wanted it to have what?\\nAnswer Choices:\\n(a) farmhouse (b) zoos (c) cold place\\n(d) floor (e) warm place\\nA: The answer must be a place where the\\nkitten would be comfortable. The kitten\\nwould be comfortable in a warm place.\\nTherefore, the answer is warm place (e).\\nQ: Needlepoint requires many\\naccessories, what holds the work taut?\\nAnswer Choices:\\n(a) sewing (b) collection of folk art\\n(c) livingroom (d) frame (e) bathroom\\nA: The answer must be something that\\nholds the work taut. The work is taut\\nbecause it is held in place. Therefore,\\nthe answer is frame (d).\\n(a) Problems where the model was able to solve ques-\\ntions which it had previously answered incorrectly.\\nQ: They prided themselves on being a\\nwealth of knowledge, and that’s why many\\nchose to attend their what?\\nAnswer Choices:\\n(a) book store (b) university (c) meeting\\n(d) class (e) encyclopedia\\nA: The answer must be a place where people\\ngo to learn about things. The answer is\\nuniversity (b).\\nA: The answer must be a place where people\\ngo to learn about things. Universities are\\nplaces where people go to learn about\\nthings. Therefore, the answer is\\nuniversity (b).\\nQ: You want a real estate agent that is\\nstraight forward with you, the last\\nthing you want is for them to what a\\nhouse’s quality?\\nAnswer Choices:\\n(a) cover up (b) offer price\\n(c) misrepresent (d) lie (e) falsify\\nA: The answer must be a way to determine\\nthe quality of a house. The answer is\\nmisrepresent (c).\\nA: The answer must be something that\\nwould be a bad thing for the real estate\\nagent to do. The real estate agent would\\nbe misrepresenting the house’s quality\\nif they were to lie about it. Therefore,\\nthe answer is misrepresent (c).\\n(b) Problems where the model was already able to an-\\nswer them with few-shot reasoning, but was able to\\nanswer better after more iterations.\\nFigure 7: Case study on STaR. We highlight some problems which the model is not initially able\\nto solve, as well as rationales which are initially correct but improve. We note that for CQA, one is\\ntasked with selecting the “best” answer, where multiple answers may be technically correct.\\n26'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-05-23T01:29:23+00:00', 'author': '', 'keywords': '', 'moddate': '2022-05-23T01:29:23+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'papers/2203.14465v2.pdf', 'total_pages': 30, 'page': 26, 'page_label': '27'}, page_content='G Bias and Faithfulness\\nIt is important to note that STaR is designed to amplify the reasoning that leads to correct solutions\\non a given dataset. The implication of this with a dataset like CommonsenseQA is that if biases are\\n“useful” in solving the dataset then they will be ampliﬁed. This is made worse by rationalization, as\\nbiased answers that the model may not naturally arrive at are, in a sense, pulled out of the model.\\nThe exact interaction between the bias in a dataset and the pre-existing bias learned by the model\\nis unclear, and something to consider before real-world deployment of all large language models in\\ngeneral, but STaR-trained models in particular.\\nWe ﬁnd some encouraging initial results on this however: for questions where gender is not relevant,\\nthe model appears to disregard it in its explanation. For example: “Q: Where is a good place for\\na woman to store her sunglasses? → A: The answer must be a place where sunglasses are stored.\\nSunglasses are stored in purses. Therefore, the answer is purse (e).” or “Q: The only baggage the\\nwoman checked was a drawstring bag, where was she heading with it? → A: The answer must be\\na place where a drawstring bag is checked. The answer is airport (e).” We believe this question\\nwarrants a much more comprehensive study.\\nOne important challenge with models which seek to be interpretable or provide explanations for\\ntheir reasoning is that of faithfulness. While our primary emphasis is not on the explainability\\nbeneﬁts that STaR may bring, there is a fundamental challenge around evaluating explanations\\nand rationales: namely, faithfulness [ 33, 34]. [ 34] describe faithful explanations as those which\\n“accurately [represent] the reasoning process behind the model’s prediction.” While STaR encourages\\nthe use of reasoning in rationales which leads the model to correct answers, it is difﬁcult, if not\\nimpossible, to ensure that the rationales reﬂect the model’s internal processing. For example, it is\\nstraightforward to imagine the model implicitly selecting a particular answer immediately and then\\ngenerating a rationale to justify that selected answer. This would allow a model to generate unbiased\\nrationales while selecting answers in a biased way.\\nThe fact that our model outperforms one ﬁne-tuned to directly predict the answers, and ablation\\nstudies from papers such as [6] make it clear that the generation of a rationale before producing an\\nanswer non-trivially improves the model’s answer quality. However, it is difﬁcult to evaluate the\\ndegree to which any particular answer’s rationale is faithful. However, we note that there problems\\nare not unique to STaR, but are symptomatic of the difﬁculty of understanding large language models\\nand in particular the rationales generated by large language models.\\nH Hyperparameters\\nGPT-J is a 28-layer decoder-only transformer, with an embedding size of 1024, 16 attention heads of\\ndimension 256, and an FFN hidden layer of size 16384. It was pre-trained on the Pile [35], with a\\nvocabulary size of 50.4K\\nIn general, unless otherwise stated, we use a batch size of 8 sequences, each of length 1024. We also\\nuse packing, namely, packing the shorter examples to form longer sequences (up to length 1024) to\\nimprove TPU utilization. We do not use weight decay, and we train and sample on a single TPU-v3\\nnode. We performed a hyperparameter search over learning rates from 10−7 to 10−4 using the Adam\\noptimizer [36]. We found that 10−6 was consistently the best-performing learning rate.\\n27'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-05-23T01:29:23+00:00', 'author': '', 'keywords': '', 'moddate': '2022-05-23T01:29:23+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'papers/2203.14465v2.pdf', 'total_pages': 30, 'page': 27, 'page_label': '28'}, page_content='I GSM8K Few-shot Prompt\\nWe include the following few-shot prompts for GSM8K, based on the examples in [9].\\nQ: Natalia sold clips to 48 of her friends in April, and then she sold half as many\\nclips in May. How many clips did Natalia sell altogether in April and May?\\nA: Natalia sold 48/2 = <<48/2=24>>24 clips in May.\\nNatalia sold 48+24 = <<48+24=72>>72 clips altogether in April and May.\\n#### 72\\nQ: Betty is saving money for a new wallet which costs $100. Betty has only half of\\nthe money she needs. Her parents decided to give her $15 for that purpose, and her\\ngrandparents twice as much as her parents. How much more money does Betty need to\\nbuy the wallet?\\nA: In the beginning, Betty has only 100 / 2 = $<<100/2=50>>50.\\nBetty’s grandparents gave her 15 * 2 = $<<15*2=30>>30.\\nThis means, Betty needs 100 - 50 - 30 - 15 = $<<100-50-30-15=5>>5 more.\\n#### 5\\nQ: Julie is reading a 120-page book. Yesterday, she was able to read 12 pages and\\ntoday, she read twice as many pages as yesterday. If she wants to read half of the\\nremaining pages tomorrow, how many pages should she read?\\nA: Maila read 12 x 2 = <<12*2=24>>24 pages today.\\nSo she was able to read a total of 12 + 24 = <<12+24=36>>36 pages since yesterday.\\nThere are 120 - 36 = <<120-36=84>>84 pages left to be read.\\nSince she wants to read half of the remaining pages tomorrow, then she should read\\n84/2 = <<84/2=42>>42 pages.\\n#### 42\\nQ: Mark has a garden with flowers. He planted plants of three different colors in it\\n. Ten of them are yellow, and there are 80% more of those in purple. There are only\\n25% as many green flowers as there are yellow and purple flowers. How many flowers\\ndoes Mark have in his garden?\\nA: There are 80/100 * 10 = <<80/100*10=8>>8 more purple flowers than yellow flowers.\\nSo in Mark’s garden, there are 10 + 8 = <<10+8=18>>18 purple flowers.\\nPurple and yellow flowers sum up to 10 + 18 = <<10+18=28>>28 flowers.\\nThat means in Mark’s garden there are 25/100 * 28 = <<25/100*28=7>>7 green flowers.\\nSo in total Mark has 28 + 7 = <<28+7=35>>35 plants in his garden.\\n#### 35\\nQ: Alexis is applying for a new job and bought a new set of business clothes to wear\\nto the interview. She went to a department store with a budget of $200 and spent\\n$30 on a button-up shirt, $46 on suit pants, $38 on a suit coat, $11 on socks, and\\n$18 on a belt. She also purchased a pair of shoes, but lost the receipt for them.\\nShe has $16 left from her budget. How much did Alexis pay for the shoes?\\nA: Let S be the amount Alexis paid for the shoes.\\nShe spent S + 30 + 46 + 38 + 11 + 18 = S + <<+30+46+38+11+18=143>>143.\\nShe used all but $16 of her budget, so S + 143 = 200 - 16 = 184.\\nThus, Alexis paid S = 184 - 143 = $<<184-143=41>>41 for the shoes.\\n#### 41\\nQ: Tina makes $18.00 an hour. If she works more than 8 hours per shift, she is\\neligible for overtime, which is paid by your hourly wage + 1/2 your hourly wage. If\\nshe works 10 hours every day for 5 days, how much money does she make?\\nA: She works 8 hours a day for $18 per hour so she makes 8*18 = $\\n<<8*18=144.00>>144.00 per 8-hour shift\\nShe works 10 hours a day and anything over 8 hours is eligible for overtime, so she\\ngets 10-8 = <<10-8=2>>2 hours of overtime\\nOvertime is calculated as time and a half so and she makes $18/hour so her overtime\\npay is 18*.5 = $<<18*.5=9.00>>9.00\\nHer overtime pay is 18+9 = $<<18+9=27.00>>27.00\\nHer base pay is $144.00 per 8-hour shift and she works 5 days and makes 5 * $144 = $\\n<<144*5=720.00>>720.00\\nHer overtime pay is $27.00 per hour and she works 2 hours of overtime per day and\\nmakes 27*2 = $<<27*2=54.00>>54.00 in overtime pay\\n28'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-05-23T01:29:23+00:00', 'author': '', 'keywords': '', 'moddate': '2022-05-23T01:29:23+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'papers/2203.14465v2.pdf', 'total_pages': 30, 'page': 28, 'page_label': '29'}, page_content='2 hours of overtime pay for 5 days means she makes 54*5 = $270.00\\nIn 5 days her base pay is $720.00 and she makes $270.00 in overtime pay so she makes\\n$720 + $270 = $<<720+270=990.00>>990.00\\n#### 990\\n29'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-05-23T01:29:23+00:00', 'author': '', 'keywords': '', 'moddate': '2022-05-23T01:29:23+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'papers/2203.14465v2.pdf', 'total_pages': 30, 'page': 29, 'page_label': '30'}, page_content='J STaR GSM8K Solutions\\nWe observe some interesting patterns with the GSM8K solutions proposed by the STaR-trained\\nmodel. Typically, when the solution takes substantially fewer calculation steps than the ground truth,\\nit corresponds to an instance where the model accidentally answered the question correctly despite\\nmistakes in its reasoning. In some cases, however, the model produces simpler solutions than those in\\nthe ground truth. One example is shown in Figure 8.\\nQ: A van is delivering  180 bottles of drinks to a neighborhood, and each bottle contains \\neither cider or beer or a mixture of two. Out of the  180 bottles, 40 contain only cider, 80 \\ncontain only beer, and the rest are a mixture of the two drinks. If the delivery man gives \\nhalf the number of each bottle of drink to the first house, how many bottles does the first \\nhouse get?\\nSTaR (on GPT-J):\\nThe delivery man gives half the number of \\neach bottle of drink to the first house.\\nSo he gives 180/2 = <<180/2=90>>90 bottles \\nof drink to the first house.\\n#### 90\\nHuman (Ground truth from GSM8K dataset):\\nThe total number of bottles containing \\ncider or beer is 40+80 = <<40+80=120>>120\\nIf the number of bottles in the van was \\n180, then 180-120 =<<180-120=60>>60 \\ncontains a mixture of the two drinks.\\nAt the first house, the delivery man \\ndropped 1/2*40 = <<1/2*40=20>>20 bottles.\\nThere were also 1/2*80 = <<1/2*80=40>>40 \\nbottles of beer that the delivery man gave \\nto the first house.\\nTogether, the first house received 40+20 = \\n<<40+20=60>>60 bottles of cider and beer\\nThe total number of bottles that had a \\nmixture of the two drinks that were dropped \\nat the first house is 1/2*60 = \\n<<1/2*60=30>>30\\nThe first house got 60+30 = <<60+30=90>>90 \\nbottles of all types of drinks.\\n#### 90\\nFigure 8: An example problem in the training set where STaR derives a signiﬁcantly simpler solution\\nthan the ground truth.\\n30'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-23T01:45:31+00:00', 'author': '', 'keywords': '', 'moddate': '2025-01-23T01:45:31+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'papers/2501.12948v1.pdf', 'total_pages': 22, 'page': 0, 'page_label': '1'}, page_content='DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via\\nReinforcement Learning\\nDeepSeek-AI\\nresearch@deepseek.com\\nAbstract\\nWe introduce our first-generation reasoning models, DeepSeek-R1-Zero and DeepSeek-R1.\\nDeepSeek-R1-Zero, a model trained via large-scale reinforcement learning (RL) without super-\\nvised fine-tuning (SFT) as a preliminary step, demonstrates remarkable reasoning capabilities.\\nThrough RL, DeepSeek-R1-Zero naturally emerges with numerous powerful and intriguing\\nreasoning behaviors. However, it encounters challenges such as poor readability, and language\\nmixing. To address these issues and further enhance reasoning performance, we introduce\\nDeepSeek-R1, which incorporates multi-stage training and cold-start data before RL. DeepSeek-\\nR1 achieves performance comparable to OpenAI-o1-1217 on reasoning tasks. To support the\\nresearch community, we open-source DeepSeek-R1-Zero, DeepSeek-R1, and six dense models\\n(1.5B, 7B, 8B, 14B, 32B, 70B) distilled from DeepSeek-R1 based on Qwen and Llama.\\nAIME 2024\\n(Pass@1)\\nCodeforces\\n(Percentile)\\nGPQA Diamond\\n(Pass@1)\\nMATH-500\\n(Pass@1)\\nMMLU\\n(Pass@1)\\nSWE-bench Verified\\n(Resolved)\\n0\\n20\\n40\\n60\\n80\\n100Accuracy / Percentile (%)\\n79.8\\n96.3\\n71.5\\n97.3\\n90.8\\n49.2\\n79.2\\n96.6\\n75.7\\n96.4\\n91.8\\n48.9\\n72.6\\n90.6\\n62.1\\n94.3\\n87.4\\n36.8\\n63.6\\n93.4\\n60.0\\n90.0\\n85.2\\n41.6\\n39.2\\n58.7 59.1\\n90.2\\n88.5\\n42.0\\nDeepSeek-R1 OpenAI-o1-1217 DeepSeek-R1-32B OpenAI-o1-mini DeepSeek-V3\\nFigure 1 | Benchmark performance of DeepSeek-R1.\\narXiv:2501.12948v1  [cs.CL]  22 Jan 2025'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-23T01:45:31+00:00', 'author': '', 'keywords': '', 'moddate': '2025-01-23T01:45:31+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'papers/2501.12948v1.pdf', 'total_pages': 22, 'page': 1, 'page_label': '2'}, page_content='Contents\\n1 Introduction 3\\n1.1 Contributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4\\n1.2 Summary of Evaluation Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4\\n2 Approach 5\\n2.1 Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5\\n2.2 DeepSeek-R1-Zero: Reinforcement Learning on the Base Model . . . . . . . . . . 5\\n2.2.1 Reinforcement Learning Algorithm . . . . . . . . . . . . . . . . . . . . . . 5\\n2.2.2 Reward Modeling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6\\n2.2.3 Training Template . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6\\n2.2.4 Performance, Self-evolution Process and Aha Moment of DeepSeek-R1-Zero 6\\n2.3 DeepSeek-R1: Reinforcement Learning with Cold Start . . . . . . . . . . . . . . . 9\\n2.3.1 Cold Start . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9\\n2.3.2 Reasoning-oriented Reinforcement Learning . . . . . . . . . . . . . . . . . 10\\n2.3.3 Rejection Sampling and Supervised Fine-Tuning . . . . . . . . . . . . . . . 10\\n2.3.4 Reinforcement Learning for all Scenarios . . . . . . . . . . . . . . . . . . . 11\\n2.4 Distillation: Empower Small Models with Reasoning Capability . . . . . . . . . . 11\\n3 Experiment 11\\n3.1 DeepSeek-R1 Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13\\n3.2 Distilled Model Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14\\n4 Discussion 14\\n4.1 Distillation v.s. Reinforcement Learning . . . . . . . . . . . . . . . . . . . . . . . . 14\\n4.2 Unsuccessful Attempts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15\\n5 Conclusion, Limitations, and Future Work 16\\nA Contributions and Acknowledgments 20\\n2'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-23T01:45:31+00:00', 'author': '', 'keywords': '', 'moddate': '2025-01-23T01:45:31+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'papers/2501.12948v1.pdf', 'total_pages': 22, 'page': 2, 'page_label': '3'}, page_content='1. Introduction\\nIn recent years, Large Language Models (LLMs) have been undergoing rapid iteration and\\nevolution (Anthropic, 2024; Google, 2024; OpenAI, 2024a), progressively diminishing the gap\\ntowards Artificial General Intelligence (AGI).\\nRecently, post-training has emerged as an important component of the full training pipeline.\\nIt has been shown to enhance accuracy on reasoning tasks, align with social values, and adapt\\nto user preferences, all while requiring relatively minimal computational resources against\\npre-training. In the context of reasoning capabilities, OpenAI’s o1 (OpenAI, 2024b) series models\\nwere the first to introduce inference-time scaling by increasing the length of the Chain-of-\\nThought reasoning process. This approach has achieved significant improvements in various\\nreasoning tasks, such as mathematics, coding, and scientific reasoning. However, the challenge\\nof effective test-time scaling remains an open question for the research community. Several prior\\nworks have explored various approaches, including process-based reward models (Lightman\\net al., 2023; Uesato et al., 2022; Wang et al., 2023), reinforcement learning (Kumar et al., 2024),\\nand search algorithms such as Monte Carlo Tree Search and Beam Search (Feng et al., 2024; Trinh\\net al., 2024; Xin et al., 2024). However, none of these methods has achieved general reasoning\\nperformance comparable to OpenAI’s o1 series models.\\nIn this paper, we take the first step toward improving language model reasoning capabilities\\nusing pure reinforcement learning (RL). Our goal is to explore the potential of LLMs to develop\\nreasoning capabilities without any supervised data, focusing on their self-evolution through\\na pure RL process. Specifically, we use DeepSeek-V3-Base as the base model and employ\\nGRPO (Shao et al., 2024) as the RL framework to improve model performance in reasoning.\\nDuring training, DeepSeek-R1-Zero naturally emerged with numerous powerful and interesting\\nreasoning behaviors. After thousands of RL steps, DeepSeek-R1-Zero exhibits super performance\\non reasoning benchmarks. For instance, the pass@1 score on AIME 2024 increases from 15.6% to\\n71.0%, and with majority voting, the score further improves to 86.7%, matching the performance\\nof OpenAI-o1-0912.\\nHowever, DeepSeek-R1-Zero encounters challenges such as poor readability, and language\\nmixing. To address these issues and further enhance reasoning performance, we introduce\\nDeepSeek-R1, which incorporates a small amount of cold-start data and a multi-stage training\\npipeline. Specifically, we begin by collecting thousands of cold-start data to fine-tune the\\nDeepSeek-V3-Base model. Following this, we perform reasoning-oriented RL like DeepSeek-R1-\\nZero. Upon nearing convergence in the RL process, we create new SFT data through rejection\\nsampling on the RL checkpoint, combined with supervised data from DeepSeek-V3 in domains\\nsuch as writing, factual QA, and self-cognition, and then retrain the DeepSeek-V3-Base model.\\nAfter fine-tuning with the new data, the checkpoint undergoes an additional RL process, taking\\ninto account prompts from all scenarios. After these steps, we obtained a checkpoint referred to\\nas DeepSeek-R1, which achieves performance on par with OpenAI-o1-1217.\\nWe further explore distillation from DeepSeek-R1 to smaller dense models. Using Qwen2.5-\\n32B (Qwen, 2024b) as the base model, direct distillation from DeepSeek-R1 outperforms applying\\nRL on it. This demonstrates that the reasoning patterns discovered by larger base models are cru-\\ncial for improving reasoning capabilities. We open-source the distilled Qwen and Llama (Dubey\\net al., 2024) series. Notably, our distilled 14B model outperforms state-of-the-art open-source\\nQwQ-32B-Preview (Qwen, 2024a) by a large margin, and the distilled 32B and 70B models set a\\nnew record on the reasoning benchmarks among dense models.\\n3'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-23T01:45:31+00:00', 'author': '', 'keywords': '', 'moddate': '2025-01-23T01:45:31+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'papers/2501.12948v1.pdf', 'total_pages': 22, 'page': 3, 'page_label': '4'}, page_content='1.1. Contributions\\nPost-T raining: Large-Scale Reinforcement Learning on the Base Model\\n• We directly apply RL to the base model without relying on supervised fine-tuning (SFT) as\\na preliminary step. This approach allows the model to explore chain-of-thought (CoT) for\\nsolving complex problems, resulting in the development of DeepSeek-R1-Zero. DeepSeek-\\nR1-Zero demonstrates capabilities such as self-verification, reflection, and generating\\nlong CoTs, marking a significant milestone for the research community. Notably, it is the\\nfirst open research to validate that reasoning capabilities of LLMs can be incentivized\\npurely through RL, without the need for SFT. This breakthrough paves the way for future\\nadvancements in this area.\\n• We introduce our pipeline to develop DeepSeek-R1. The pipeline incorporates two RL\\nstages aimed at discovering improved reasoning patterns and aligning with human pref-\\nerences, as well as two SFT stages that serve as the seed for the model’s reasoning and\\nnon-reasoning capabilities. We believe the pipeline will benefit the industry by creating\\nbetter models.\\nDistillation: Smaller Models Can Be Powerful T oo\\n• We demonstrate that the reasoning patterns of larger models can be distilled into smaller\\nmodels, resulting in better performance compared to the reasoning patterns discovered\\nthrough RL on small models. The open source DeepSeek-R1, as well as its API, will benefit\\nthe research community to distill better smaller models in the future.\\n• Using the reasoning data generated by DeepSeek-R1, we fine-tuned several dense models\\nthat are widely used in the research community. The evaluation results demonstrate that\\nthe distilled smaller dense models perform exceptionally well on benchmarks. DeepSeek-\\nR1-Distill-Qwen-7B achieves 55.5% on AIME 2024, surpassing QwQ-32B-Preview. Addi-\\ntionally, DeepSeek-R1-Distill-Qwen-32B scores 72.6% on AIME 2024, 94.3% on MATH-500,\\nand 57.2% on LiveCodeBench. These results significantly outperform previous open-\\nsource models and are comparable to o1-mini. We open-source distilled 1.5B, 7B, 8B, 14B,\\n32B, and 70B checkpoints based on Qwen2.5 and Llama3 series to the community.\\n1.2. Summary of Evaluation Results\\n• Reasoning tasks: (1) DeepSeek-R1 achieves a score of 79.8% Pass@1 on AIME 2024, slightly\\nsurpassing OpenAI-o1-1217. On MATH-500, it attains an impressive score of 97.3%,\\nperforming on par with OpenAI-o1-1217 and significantly outperforming other models. (2)\\nOn coding-related tasks, DeepSeek-R1 demonstrates expert level in code competition tasks,\\nas it achieves 2,029 Elo rating on Codeforces outperforming 96.3% human participants in\\nthe competition. For engineering-related tasks, DeepSeek-R1 performs slightly better than\\nDeepSeek-V3, which could help developers in real world tasks.\\n• Knowledge: On benchmarks such as MMLU, MMLU-Pro, and GPQA Diamond, DeepSeek-\\nR1 achieves outstanding results, significantly outperforming DeepSeek-V3 with scores\\nof 90.8% on MMLU, 84.0% on MMLU-Pro, and 71.5% on GPQA Diamond. While its\\nperformance is slightly below that of OpenAI-o1-1217 on these benchmarks, DeepSeek-R1\\nsurpasses other closed-source models, demonstrating its competitive edge in educational\\ntasks. On the factual benchmark SimpleQA, DeepSeek-R1 outperforms DeepSeek-V3,\\ndemonstrating its capability in handling fact-based queries. A similar trend is observed\\nwhere OpenAI-o1 surpasses 4o on this benchmark.\\n4'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-23T01:45:31+00:00', 'author': '', 'keywords': '', 'moddate': '2025-01-23T01:45:31+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'papers/2501.12948v1.pdf', 'total_pages': 22, 'page': 4, 'page_label': '5'}, page_content='• Others: DeepSeek-R1 also excels in a wide range of tasks, including creative writing,\\ngeneral question answering, editing, summarization, and more. It achieves an impressive\\nlength-controlled win-rate of 87.6% on AlpacaEval 2.0 and a win-rate of 92.3% on Are-\\nnaHard, showcasing its strong ability to intelligently handle non-exam-oriented queries.\\nAdditionally, DeepSeek-R1 demonstrates outstanding performance on tasks requiring\\nlong-context understanding, substantially outperforming DeepSeek-V3 on long-context\\nbenchmarks.\\n2. Approach\\n2.1. Overview\\nPrevious work has heavily relied on large amounts of supervised data to enhance model\\nperformance. In this study, we demonstrate that reasoning capabilities can be significantly\\nimproved through large-scale reinforcement learning (RL), even without using supervised\\nfine-tuning (SFT) as a cold start. Furthermore, performance can be further enhanced with\\nthe inclusion of a small amount of cold-start data. In the following sections, we present: (1)\\nDeepSeek-R1-Zero, which applies RL directly to the base model without any SFT data, and\\n(2) DeepSeek-R1, which applies RL starting from a checkpoint fine-tuned with thousands of\\nlong Chain-of-Thought (CoT) examples. 3) Distill the reasoning capability from DeepSeek-R1 to\\nsmall dense models.\\n2.2. DeepSeek-R1-Zero: Reinforcement Learning on the Base Model\\nReinforcement learning has demonstrated significant effectiveness in reasoning tasks, as ev-\\nidenced by our previous works (Shao et al., 2024; Wang et al., 2023). However, these works\\nheavily depended on supervised data, which are time-intensive to gather. In this section, we\\nexplore the potential of LLMs to develop reasoning capabilities without any supervised data ,\\nfocusing on their self-evolution through a pure reinforcement learning process. We start with a\\nbrief overview of our RL algorithm, followed by the presentation of some exciting results, and\\nhope this provides the community with valuable insights.\\n2.2.1. Reinforcement Learning Algorithm\\nGroup Relative Policy Optimization In order to save the training costs of RL, we adopt Group\\nRelative Policy Optimization (GRPO) (Shao et al., 2024), which foregoes the critic model that is\\ntypically the same size as the policy model, and estimates the baseline from group scores instead.\\nSpecifically, for each question 𝑞, GRPO samples a group of outputs {𝑜1, 𝑜2, · · ·, 𝑜𝐺} from the old\\npolicy 𝜋𝜃𝑜𝑙𝑑 and then optimizes the policy model 𝜋𝜃 by maximizing the following objective:\\nJ𝐺𝑅𝑃𝑂 (𝜃) = E[𝑞 ∼ 𝑃(𝑄), {𝑜𝑖}𝐺\\n𝑖=1 ∼ 𝜋𝜃𝑜𝑙𝑑 (𝑂|𝑞)]\\n1\\n𝐺\\n𝐺∑︁\\n𝑖=1\\n\\x12\\nmin\\n\\x12 𝜋𝜃(𝑜𝑖|𝑞)\\n𝜋𝜃𝑜𝑙𝑑 (𝑜𝑖|𝑞) 𝐴𝑖, clip\\n\\x12 𝜋𝜃(𝑜𝑖|𝑞)\\n𝜋𝜃𝑜𝑙𝑑 (𝑜𝑖|𝑞) , 1 − 𝜀, 1 + 𝜀\\n\\x13\\n𝐴𝑖\\n\\x13\\n− 𝛽D𝐾 𝐿\\n\\x00\\n𝜋𝜃|| 𝜋𝑟𝑒 𝑓\\n\\x01\\x13\\n, (1)\\nD𝐾 𝐿\\n\\x00\\n𝜋𝜃|| 𝜋𝑟𝑒 𝑓\\n\\x01 =\\n𝜋𝑟𝑒 𝑓 (𝑜𝑖|𝑞)\\n𝜋𝜃(𝑜𝑖|𝑞) − log\\n𝜋𝑟𝑒 𝑓 (𝑜𝑖|𝑞)\\n𝜋𝜃(𝑜𝑖|𝑞) − 1, (2)\\nwhere 𝜀 and 𝛽 are hyper-parameters, and 𝐴𝑖 is the advantage, computed using a group of\\nrewards {𝑟1, 𝑟2, . . . , 𝑟𝐺} corresponding to the outputs within each group:\\n𝐴𝑖 = 𝑟𝑖 − m𝑒𝑎𝑛({𝑟1, 𝑟2, · · ·, 𝑟𝐺})\\ns𝑡𝑑 ({𝑟1, 𝑟2, · · ·, 𝑟𝐺}) . (3)\\n5'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-23T01:45:31+00:00', 'author': '', 'keywords': '', 'moddate': '2025-01-23T01:45:31+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'papers/2501.12948v1.pdf', 'total_pages': 22, 'page': 5, 'page_label': '6'}, page_content='A conversation between User and Assistant. The user asks a question, and the Assistant solves it.\\nThe assistant first thinks about the reasoning process in the mind and then provides the user\\nwith the answer. The reasoning process and answer are enclosed within <think> </think> and\\n<answer> </answer> tags, respectively, i.e., <think> reasoning process here </think>\\n<answer> answer here </answer>. User: prompt. Assistant:\\nTable 1 | Template for DeepSeek-R1-Zero. prompt will be replaced with the specific reasoning\\nquestion during training.\\n2.2.2. Reward Modeling\\nThe reward is the source of the training signal, which decides the optimization direction of RL.\\nTo train DeepSeek-R1-Zero, we adopt a rule-based reward system that mainly consists of two\\ntypes of rewards:\\n• Accuracy rewards: The accuracy reward model evaluates whether the response is correct.\\nFor example, in the case of math problems with deterministic results, the model is required\\nto provide the final answer in a specified format (e.g., within a box), enabling reliable\\nrule-based verification of correctness. Similarly, for LeetCode problems, a compiler can be\\nused to generate feedback based on predefined test cases.\\n• Format rewards: In addition to the accuracy reward model, we employ a format reward\\nmodel that enforces the model to put its thinking process between ‘<think>’ and ‘</think>’\\ntags.\\nWe do not apply the outcome or process neural reward model in developing DeepSeek-R1-Zero,\\nbecause we find that the neural reward model may suffer from reward hacking in the large-scale\\nreinforcement learning process, and retraining the reward model needs additional training\\nresources and it complicates the whole training pipeline.\\n2.2.3. Training T emplate\\nTo train DeepSeek-R1-Zero, we begin by designing a straightforward template that guides\\nthe base model to adhere to our specified instructions. As depicted in Table 1, this template\\nrequires DeepSeek-R1-Zero to first produce a reasoning process, followed by the final answer.\\nWe intentionally limit our constraints to this structural format, avoiding any content-specific\\nbiases—such as mandating reflective reasoning or promoting particular problem-solving strate-\\ngies—to ensure that we can accurately observe the model’s natural progression during the RL\\nprocess.\\n2.2.4. Performance, Self-evolution Process and Aha Moment of DeepSeek-R1-Zero\\nPerformance of DeepSeek-R1-Zero Figure 2 depicts the performance trajectory of DeepSeek-\\nR1-Zero on the AIME 2024 benchmark throughout the RL training process. As illustrated,\\nDeepSeek-R1-Zero demonstrates a steady and consistent enhancement in performance as the\\nRL training advances. Notably, the average pass@1 score on AIME 2024 shows a significant\\nincrease, jumping from an initial 15.6% to an impressive 71.0%, reaching performance levels\\ncomparable to OpenAI-o1-0912. This significant improvement highlights the efficacy of our RL\\nalgorithm in optimizing the model’s performance over time.\\nTable 2 provides a comparative analysis between DeepSeek-R1-Zero and OpenAI’s o1-0912\\nmodels across a variety of reasoning-related benchmarks. The findings reveal that RL empowers\\n6'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-23T01:45:31+00:00', 'author': '', 'keywords': '', 'moddate': '2025-01-23T01:45:31+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'papers/2501.12948v1.pdf', 'total_pages': 22, 'page': 6, 'page_label': '7'}, page_content='Model AIME 2024 MATH-500 GPQA LiveCode CodeForcesDiamond Bench\\npass@1 cons@64 pass@1 pass@1 pass@1 rating\\nOpenAI-o1-mini 63.6 80.0 90.0 60.0 53.8 1820\\nOpenAI-o1-0912 74.4 83.3 94.8 77.3 63.4 1843\\nDeepSeek-R1-Zero 71.0 86.7 95.9 73.3 50.0 1444\\nTable 2 | Comparison of DeepSeek-R1-Zero and OpenAI o1 models on reasoning-related\\nbenchmarks.\\nFigure 2 | AIME accuracy of DeepSeek-R1-Zero during training. For each question, we sample\\n16 responses and calculate the overall average accuracy to ensure a stable evaluation.\\nDeepSeek-R1-Zero to attain robust reasoning capabilities without the need for any supervised\\nfine-tuning data. This is a noteworthy achievement, as it underscores the model’s ability to\\nlearn and generalize effectively through RL alone. Additionally, the performance of DeepSeek-\\nR1-Zero can be further augmented through the application of majority voting. For example,\\nwhen majority voting is employed on the AIME benchmark, DeepSeek-R1-Zero’s performance\\nescalates from 71.0% to 86.7%, thereby exceeding the performance of OpenAI-o1-0912. The\\nability of DeepSeek-R1-Zero to achieve such competitive performance, both with and without\\nmajority voting, highlights its strong foundational capabilities and its potential for further\\nadvancements in reasoning tasks.\\nSelf-evolution Process of DeepSeek-R1-Zero The self-evolution process of DeepSeek-R1-Zero\\nis a fascinating demonstration of how RL can drive a model to improve its reasoning capabilities\\nautonomously. By initiating RL directly from the base model, we can closely monitor the model’s\\nprogression without the influence of the supervised fine-tuning stage. This approach provides\\na clear view of how the model evolves over time, particularly in terms of its ability to handle\\ncomplex reasoning tasks.\\nAs depicted in Figure 3, the thinking time of DeepSeek-R1-Zero shows consistent improve-\\n7'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-23T01:45:31+00:00', 'author': '', 'keywords': '', 'moddate': '2025-01-23T01:45:31+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'papers/2501.12948v1.pdf', 'total_pages': 22, 'page': 7, 'page_label': '8'}, page_content='Figure 3 | The average response length of DeepSeek-R1-Zero on the training set during the RL\\nprocess. DeepSeek-R1-Zero naturally learns to solve reasoning tasks with more thinking time.\\nment throughout the training process. This improvement is not the result of external adjustments\\nbut rather an intrinsic development within the model. DeepSeek-R1-Zero naturally acquires the\\nability to solve increasingly complex reasoning tasks by leveraging extended test-time compu-\\ntation. This computation ranges from generating hundreds to thousands of reasoning tokens,\\nallowing the model to explore and refine its thought processes in greater depth.\\nOne of the most remarkable aspects of this self-evolution is the emergence of sophisticated\\nbehaviors as the test-time computation increases. Behaviors such as reflection—where the model\\nrevisits and reevaluates its previous steps—and the exploration of alternative approaches to\\nproblem-solving arise spontaneously. These behaviors are not explicitly programmed but instead\\nemerge as a result of the model’s interaction with the reinforcement learning environment. This\\nspontaneous development significantly enhances DeepSeek-R1-Zero’s reasoning capabilities,\\nenabling it to tackle more challenging tasks with greater efficiency and accuracy.\\nAha Moment of DeepSeek-R1-Zero A particularly intriguing phenomenon observed during\\nthe training of DeepSeek-R1-Zero is the occurrence of an “aha moment”. This moment, as\\nillustrated in Table 3, occurs in an intermediate version of the model. During this phase,\\nDeepSeek-R1-Zero learns to allocate more thinking time to a problem by reevaluating its initial\\napproach. This behavior is not only a testament to the model’s growing reasoning abilities\\nbut also a captivating example of how reinforcement learning can lead to unexpected and\\nsophisticated outcomes.\\nThis moment is not only an “aha moment” for the model but also for the researchers\\nobserving its behavior. It underscores the power and beauty of reinforcement learning: rather\\nthan explicitly teaching the model on how to solve a problem, we simply provide it with the\\nright incentives, and it autonomously develops advanced problem-solving strategies. The\\n“aha moment” serves as a powerful reminder of the potential of RL to unlock new levels of\\nintelligence in artificial systems, paving the way for more autonomous and adaptive models in\\nthe future.\\n8'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-23T01:45:31+00:00', 'author': '', 'keywords': '', 'moddate': '2025-01-23T01:45:31+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'papers/2501.12948v1.pdf', 'total_pages': 22, 'page': 8, 'page_label': '9'}, page_content='Question: If 𝑎 > 1, then the sum of the real solutions of\\n√︁\\n𝑎 − √\\n𝑎 + 𝑥 = 𝑥 is equal to\\nResponse: <think>\\nTo solve the equation\\n√︁\\n𝑎 − √\\n𝑎 + 𝑥 = 𝑥, let’s start by squaring both· · ·\\x10√︁\\n𝑎 − √\\n𝑎 + 𝑥\\n\\x112\\n= 𝑥2 =⇒ 𝑎 − √\\n𝑎 + 𝑥 = 𝑥2.\\nRearrange to isolate the inner square root term:\\n(𝑎 − 𝑥2)2 = 𝑎 + 𝑥 =⇒ 𝑎2 − 2𝑎𝑥2 + ( 𝑥2)2 = 𝑎 + 𝑥 =⇒ 𝑥4 − 2𝑎𝑥2 − 𝑥 + ( 𝑎2 − 𝑎) = 0\\n. . .\\nWait, wait. Wait. That’s an aha moment I can flag here.\\nLet’s reevaluate this step-by-step to identify if the correct sum can be· · ·\\nWe started with the equation:√︁\\n𝑎 − √\\n𝑎 + 𝑥 = 𝑥\\nFirst, let’s square both sides:\\n𝑎 − √\\n𝑎 + 𝑥 = 𝑥2 =⇒ √\\n𝑎 + 𝑥 = 𝑎 − 𝑥2\\nNext, I could square both sides again, treating the equation: · · ·\\n. . .\\nTable 3 | An interesting “aha moment” of an intermediate version of DeepSeek-R1-Zero. The\\nmodel learns to rethink using an anthropomorphic tone. This is also an aha moment for us,\\nallowing us to witness the power and beauty of reinforcement learning.\\nDrawback of DeepSeek-R1-Zero Although DeepSeek-R1-Zero exhibits strong reasoning\\ncapabilities and autonomously develops unexpected and powerful reasoning behaviors, it faces\\nseveral issues. For instance, DeepSeek-R1-Zero struggles with challenges like poor readability,\\nand language mixing. To make reasoning processes more readable and share them with the\\nopen community, we explore DeepSeek-R1, a method that utilizes RL with human-friendly\\ncold-start data.\\n2.3. DeepSeek-R1: Reinforcement Learning with Cold Start\\nInspired by the promising results of DeepSeek-R1-Zero, two natural questions arise: 1) Can\\nreasoning performance be further improved or convergence accelerated by incorporating a small\\namount of high-quality data as a cold start? 2) How can we train a user-friendly model that\\nnot only produces clear and coherent Chains of Thought (CoT) but also demonstrates strong\\ngeneral capabilities? To address these questions, we design a pipeline to train DeepSeek-R1. The\\npipeline consists of four stages, outlined as follows.\\n2.3.1. Cold Start\\nUnlike DeepSeek-R1-Zero, to prevent the early unstable cold start phase of RL training from\\nthe base model, for DeepSeek-R1 we construct and collect a small amount of long CoT data\\nto fine-tune the model as the initial RL actor. To collect such data, we have explored several\\napproaches: using few-shot prompting with a long CoT as an example, directly prompting\\nmodels to generate detailed answers with reflection and verification, gathering DeepSeek-R1-\\nZero outputs in a readable format, and refining the results through post-processing by human\\nannotators.\\nIn this work, we collect thousands of cold-start data to fine-tune the DeepSeek-V3-Base as\\nthe starting point for RL. Compared to DeepSeek-R1-Zero, the advantages of cold start data\\n9'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-23T01:45:31+00:00', 'author': '', 'keywords': '', 'moddate': '2025-01-23T01:45:31+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'papers/2501.12948v1.pdf', 'total_pages': 22, 'page': 9, 'page_label': '10'}, page_content='include:\\n• Readability: A key limitation of DeepSeek-R1-Zero is that its content is often not suitable\\nfor reading. Responses may mix multiple languages or lack markdown formatting to\\nhighlight answers for users. In contrast, when creating cold-start data for DeepSeek-R1,\\nwe design a readable pattern that includes a summary at the end of each response and\\nfilters out responses that are not reader-friendly. Here, we define the output format as\\n|special_token|<reasoning_process>|special_token|<summary>, where the reasoning\\nprocess is the CoT for the query, and the summary is used to summarize the reasoning\\nresults.\\n• Potential: By carefully designing the pattern for cold-start data with human priors, we\\nobserve better performance against DeepSeek-R1-Zero. We believe the iterative training is\\na better way for reasoning models.\\n2.3.2. Reasoning-oriented Reinforcement Learning\\nAfter fine-tuning DeepSeek-V3-Base on the cold start data, we apply the same large-scale\\nreinforcement learning training process as employed in DeepSeek-R1-Zero. This phase focuses\\non enhancing the model’s reasoning capabilities, particularly in reasoning-intensive tasks such\\nas coding, mathematics, science, and logic reasoning, which involve well-defined problems with\\nclear solutions. During the training process, we observe that CoT often exhibits language mixing,\\nparticularly when RL prompts involve multiple languages. To mitigate the issue of language\\nmixing, we introduce a language consistency reward during RL training, which is calculated\\nas the proportion of target language words in the CoT. Although ablation experiments show\\nthat such alignment results in a slight degradation in the model’s performance, this reward\\naligns with human preferences, making it more readable. Finally, we combine the accuracy of\\nreasoning tasks and the reward for language consistency by directly summing them to form the\\nfinal reward. We then apply RL training on the fine-tuned model until it achieves convergence\\non reasoning tasks.\\n2.3.3. Rejection Sampling and Supervised Fine-Tuning\\nWhen reasoning-oriented RL converges, we utilize the resulting checkpoint to collect SFT\\n(Supervised Fine-Tuning) data for the subsequent round. Unlike the initial cold-start data, which\\nprimarily focuses on reasoning, this stage incorporates data from other domains to enhance the\\nmodel’s capabilities in writing, role-playing, and other general-purpose tasks. Specifically, we\\ngenerate the data and fine-tune the model as described below.\\nReasoning data We curate reasoning prompts and generate reasoning trajectories by perform-\\ning rejection sampling from the checkpoint from the above RL training. In the previous stage,\\nwe only included data that could be evaluated using rule-based rewards. However, in this stage,\\nwe expand the dataset by incorporating additional data, some of which use a generative reward\\nmodel by feeding the ground-truth and model predictions into DeepSeek-V3 for judgment.\\nAdditionally, because the model output is sometimes chaotic and difficult to read, we have\\nfiltered out chain-of-thought with mixed languages, long parapraphs, and code blocks. For\\neach prompt, we sample multiple responses and retain only the correct ones. In total, we collect\\nabout 600k reasoning related training samples.\\n10'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-23T01:45:31+00:00', 'author': '', 'keywords': '', 'moddate': '2025-01-23T01:45:31+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'papers/2501.12948v1.pdf', 'total_pages': 22, 'page': 10, 'page_label': '11'}, page_content='Non-Reasoning data For non-reasoning data, such as writing, factual QA, self-cognition,\\nand translation, we adopt the DeepSeek-V3 pipeline and reuse portions of the SFT dataset of\\nDeepSeek-V3. For certain non-reasoning tasks, we call DeepSeek-V3 to generate a potential\\nchain-of-thought before answering the question by prompting. However, for simpler queries,\\nsuch as “hello” we do not provide a CoT in response. In the end, we collected a total of\\napproximately 200k training samples that are unrelated to reasoning.\\nWe fine-tune DeepSeek-V3-Base for two epochs using the above curated dataset of about\\n800k samples.\\n2.3.4. Reinforcement Learning for all Scenarios\\nTo further align the model with human preferences, we implement a secondary reinforcement\\nlearning stage aimed at improving the model’s helpfulness and harmlessness while simultane-\\nously refining its reasoning capabilities. Specifically, we train the model using a combination\\nof reward signals and diverse prompt distributions. For reasoning data, we adhere to the\\nmethodology outlined in DeepSeek-R1-Zero, which utilizes rule-based rewards to guide the\\nlearning process in math, code, and logical reasoning domains. For general data, we resort to\\nreward models to capture human preferences in complex and nuanced scenarios. We build\\nupon the DeepSeek-V3 pipeline and adopt a similar distribution of preference pairs and train-\\ning prompts. For helpfulness, we focus exclusively on the final summary, ensuring that the\\nassessment emphasizes the utility and relevance of the response to the user while minimizing\\ninterference with the underlying reasoning process. For harmlessness, we evaluate the entire\\nresponse of the model, including both the reasoning process and the summary, to identify and\\nmitigate any potential risks, biases, or harmful content that may arise during the generation\\nprocess. Ultimately, the integration of reward signals and diverse data distributions enables us\\nto train a model that excels in reasoning while prioritizing helpfulness and harmlessness.\\n2.4. Distillation: Empower Small Models with Reasoning Capability\\nTo equip more efficient smaller models with reasoning capabilities like DeepSeek-R1, we directly\\nfine-tuned open-source models like Qwen (Qwen, 2024b) and Llama (AI@Meta, 2024) using\\nthe 800k samples curated with DeepSeek-R1, as detailed in §2.3.3. Our findings indicate that\\nthis straightforward distillation method significantly enhances the reasoning abilities of smaller\\nmodels. The base models we use here are Qwen2.5-Math-1.5B, Qwen2.5-Math-7B, Qwen2.5-\\n14B, Qwen2.5-32B, Llama-3.1-8B, and Llama-3.3-70B-Instruct. We select Llama-3.3 because its\\nreasoning capability is slightly better than that of Llama-3.1.\\nFor distilled models, we apply only SFT and do not include an RL stage, even though\\nincorporating RL could substantially boost model performance. Our primary goal here is to\\ndemonstrate the effectiveness of the distillation technique, leaving the exploration of the RL\\nstage to the broader research community.\\n3. Experiment\\nBenchmarks We evaluate models on MMLU (Hendrycks et al., 2020), MMLU-Redux (Gema\\net al., 2024), MMLU-Pro (Wang et al., 2024), C-Eval (Huang et al., 2023), and CMMLU (Li et al.,\\n2023), IFEval (Zhou et al., 2023), FRAMES (Krishna et al., 2024), GPQA Diamond (Rein et al.,\\n2023), SimpleQA (OpenAI, 2024c), C-SimpleQA (He et al., 2024), SWE-Bench Verified (OpenAI,\\n11'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-23T01:45:31+00:00', 'author': '', 'keywords': '', 'moddate': '2025-01-23T01:45:31+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'papers/2501.12948v1.pdf', 'total_pages': 22, 'page': 11, 'page_label': '12'}, page_content='2024d), Aider 1, LiveCodeBench (Jain et al., 2024) (2024-08 – 2025-01), Codeforces 2, Chinese\\nNational High School Mathematics Olympiad (CNMO 2024)3, and American Invitational Math-\\nematics Examination 2024 (AIME 2024) (MAA, 2024). In addition to standard benchmarks, we\\nalso evaluate our models on open-ended generation tasks using LLMs as judges. Specifically, we\\nadhere to the original configurations of AlpacaEval 2.0 (Dubois et al., 2024) and Arena-Hard (Li\\net al., 2024), which leverage GPT-4-Turbo-1106 as judges for pairwise comparisons. Here, we\\nonly feed the final summary to evaluation to avoid the length bias. For distilled models, we\\nreport representative results on AIME 2024, MATH-500, GPQA Diamond, Codeforces, and\\nLiveCodeBench.\\nEvaluation Prompts Following the setup in DeepSeek-V3, standard benchmarks such as\\nMMLU, DROP , GPQA Diamond, and SimpleQA are evaluated using prompts from the simple-\\nevals framework. For MMLU-Redux, we adopt the Zero-Eval prompt format (Lin, 2024) in a\\nzero-shot setting. In terms of MMLU-Pro, C-Eval and CLUE-WSC, since the original prompts\\nare few-shot, we slightly modify the prompt to the zero-shot setting. The CoT in few-shot\\nmay hurt the performance of DeepSeek-R1. Other datasets follow their original evaluation\\nprotocols with default prompts provided by their creators. For code and math benchmarks, the\\nHumanEval-Mul dataset covers eight mainstream programming languages (Python, Java, C++,\\nC#, JavaScript, TypeScript, PHP , and Bash). Model performance on LiveCodeBench is evaluated\\nusing CoT format, with data collected between August 2024 and January 2025. The Codeforces\\ndataset is evaluated using problems from 10 Div.2 contests along with expert-crafted test cases,\\nafter which the expected ratings and percentages of competitors are calculated. SWE-Bench\\nverified results are obtained via the agentless framework (Xia et al., 2024). AIDER-related\\nbenchmarks are measured using a \"diff\" format. DeepSeek-R1 outputs are capped at a maximum\\nof 32,768 tokens for each benchmark.\\nBaselines We conduct comprehensive evaluations against several strong baselines, including\\nDeepSeek-V3, Claude-Sonnet-3.5-1022, GPT-4o-0513, OpenAI-o1-mini, and OpenAI-o1-1217.\\nSince accessing the OpenAI-o1-1217 API is challenging in mainland China, we report its perfor-\\nmance based on official reports. For distilled models, we also compare the open-source model\\nQwQ-32B-Preview (Qwen, 2024a).\\nEvaluation Setup We set the maximum generation length to 32,768 tokens for the models.\\nWe found that using greedy decoding to evaluate long-output reasoning models results in\\nhigher repetition rates and significant variability across different checkpoints. Therefore, we\\ndefault to pass@𝑘 evaluation (Chen et al., 2021) and report pass@1 using a non-zero temperature.\\nSpecifically, we use a sampling temperature of 0.6 and a top- 𝑝 value of 0.95 to generate 𝑘\\nresponses (typically between 4 and 64, depending on the test set size) for each question. Pass@1\\nis then calculated as\\npass@1 = 1\\n𝑘\\n𝑘∑︁\\n𝑖=1\\n𝑝𝑖,\\nwhere 𝑝𝑖 denotes the correctness of the 𝑖-th response. This method provides more reliable\\nperformance estimates. For AIME 2024, we also report consensus (majority vote) results (Wang\\net al., 2022) using 64 samples, denoted as cons@64.\\n1https://aider.chat\\n2https://codeforces.com\\n3https://www.cms.org.cn/Home/comp/comp/cid/12.html\\n12'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-23T01:45:31+00:00', 'author': '', 'keywords': '', 'moddate': '2025-01-23T01:45:31+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'papers/2501.12948v1.pdf', 'total_pages': 22, 'page': 12, 'page_label': '13'}, page_content='3.1. DeepSeek-R1 Evaluation\\nBenchmark (Metric)\\nClaude-3.5- GPT-4o DeepSeek OpenAI OpenAI DeepSeek\\nSonnet-1022 0513 V3 o1-mini o1-1217 R1\\nArchitecture - - MoE - - MoE\\n# Activated Params - - 37B - - 37B\\n# Total Params - - 671B - - 671B\\nEnglish\\nMMLU (Pass@1) 88.3 87.2 88.5 85.2 91.8 90.8\\nMMLU-Redux (EM) 88.9 88.0 89.1 86.7 - 92.9\\nMMLU-Pro (EM) 78.0 72.6 75.9 80.3 - 84.0\\nDROP (3-shot F1) 88.3 83.7 91.6 83.9 90.2 92.2\\nIF-Eval (Prompt Strict) 86.5 84.3 86.1 84.8 - 83.3\\nGPQA Diamond (Pass@1) 65.0 49.9 59.1 60.0 75.7 71.5\\nSimpleQA (Correct) 28.4 38.2 24.9 7.0 47.0 30.1\\nFRAMES (Acc.) 72.5 80.5 73.3 76.9 - 82.5\\nAlpacaEval2.0 (LC-winrate) 52.0 51.1 70.0 57.8 - 87.6\\nArenaHard (GPT-4-1106) 85.2 80.4 85.5 92.0 - 92.3\\nCode\\nLiveCodeBench (Pass@1-COT) 38.9 32.9 36.2 53.8 63.4 65.9\\nCodeforces (Percentile) 20.3 23.6 58.7 93.4 96.6 96.3\\nCodeforces (Rating) 717 759 1134 1820 2061 2029\\nSWE Verified (Resolved) 50.8 38.8 42.0 41.6 48.9 49.2\\nAider-Polyglot (Acc.) 45.3 16.0 49.6 32.9 61.7 53.3\\nMath\\nAIME 2024 (Pass@1) 16.0 9.3 39.2 63.6 79.2 79.8\\nMATH-500 (Pass@1) 78.3 74.6 90.2 90.0 96.4 97.3\\nCNMO 2024 (Pass@1) 13.1 10.8 43.2 67.6 - 78.8\\nChinese\\nCLUEWSC (EM) 85.4 87.9 90.9 89.9 - 92.8\\nC-Eval (EM) 76.7 76.0 86.5 68.9 - 91.8\\nC-SimpleQA (Correct) 55.4 58.7 68.0 40.3 - 63.7\\nTable 4 | Comparison between DeepSeek-R1 and other representative models.\\nFor education-oriented knowledge benchmarks such as MMLU, MMLU-Pro, and GPQA\\nDiamond, DeepSeek-R1 demonstrates superior performance compared to DeepSeek-V3. This im-\\nprovement is primarily attributed to enhanced accuracy in STEM-related questions, where signif-\\nicant gains are achieved through large-scale reinforcement learning. Additionally, DeepSeek-R1\\nexcels on FRAMES, a long-context-dependent QA task, showcasing its strong document analysis\\ncapabilities. This highlights the potential of reasoning models in AI-driven search and data\\nanalysis tasks. On the factual benchmark SimpleQA, DeepSeek-R1 outperforms DeepSeek-V3,\\ndemonstrating its capability in handling fact-based queries. A similar trend is observed where\\nOpenAI-o1 surpasses GPT-4o on this benchmark. However, DeepSeek-R1 performs worse than\\nDeepSeek-V3 on the Chinese SimpleQA benchmark, primarily due to its tendency to refuse\\nanswering certain queries after safety RL. Without safety RL, DeepSeek-R1 could achieve an\\naccuracy of over 70%.\\nDeepSeek-R1 also delivers impressive results on IF-Eval, a benchmark designed to assess a\\nmodel’s ability to follow format instructions. These improvements can be linked to the inclusion\\nof instruction-following data during the final stages of supervised fine-tuning (SFT) and RL\\ntraining. Furthermore, remarkable performance is observed on AlpacaEval2.0 and ArenaHard,\\nindicating DeepSeek-R1’s strengths in writing tasks and open-domain question answering. Its\\nsignificant outperformance of DeepSeek-V3 underscores the generalization benefits of large-scale\\nRL, which not only boosts reasoning capabilities but also improves performance across diverse\\ndomains. Moreover, the summary lengths generated by DeepSeek-R1 are concise, with an\\naverage of 689 tokens on ArenaHard and 2,218 characters on AlpacaEval 2.0. This indicates that\\n13'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-23T01:45:31+00:00', 'author': '', 'keywords': '', 'moddate': '2025-01-23T01:45:31+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'papers/2501.12948v1.pdf', 'total_pages': 22, 'page': 13, 'page_label': '14'}, page_content='DeepSeek-R1 avoids introducing length bias during GPT-based evaluations, further solidifying\\nits robustness across multiple tasks.\\nOn math tasks, DeepSeek-R1 demonstrates performance on par with OpenAI-o1-1217,\\nsurpassing other models by a large margin. A similar trend is observed on coding algorithm\\ntasks, such as LiveCodeBench and Codeforces, where reasoning-focused models dominate these\\nbenchmarks. On engineering-oriented coding tasks, OpenAI-o1-1217 outperforms DeepSeek-R1\\non Aider but achieves comparable performance on SWE Verified. We believe the engineering\\nperformance of DeepSeek-R1 will improve in the next version, as the amount of related RL\\ntraining data currently remains very limited.\\n3.2. Distilled Model Evaluation\\nModel AIME 2024 MATH-500 GPQA LiveCode CodeForcesDiamond Bench\\npass@1 cons@64 pass@1 pass@1 pass@1 rating\\nGPT-4o-0513 9.3 13.4 74.6 49.9 32.9 759\\nClaude-3.5-Sonnet-1022 16.0 26.7 78.3 65.0 38.9 717\\nOpenAI-o1-mini 63.6 80.0 90.0 60.0 53.8 1820\\nQwQ-32B-Preview 50.0 60.0 90.6 54.5 41.9 1316\\nDeepSeek-R1-Distill-Qwen-1.5B28.9 52.7 83.9 33.8 16.9 954\\nDeepSeek-R1-Distill-Qwen-7B 55.5 83.3 92.8 49.1 37.6 1189\\nDeepSeek-R1-Distill-Qwen-14B69.7 80.0 93.9 59.1 53.1 1481\\nDeepSeek-R1-Distill-Qwen-32B 72.6 83.3 94.3 62.1 57.2 1691\\nDeepSeek-R1-Distill-Llama-8B 50.4 80.0 89.1 49.0 39.6 1205\\nDeepSeek-R1-Distill-Llama-70B70.0 86.7 94.5 65.2 57.5 1633\\nTable 5 | Comparison of DeepSeek-R1 distilled models and other comparable models on\\nreasoning-related benchmarks.\\nAs shown in Table 5, simply distilling DeepSeek-R1’s outputs enables the efficient DeepSeek-\\nR1-7B (i.e., DeepSeek-R1-Distill-Qwen-7B, abbreviated similarly below) to outperform non-\\nreasoning models like GPT-4o-0513 across the board. DeepSeek-R1-14B surpasses QwQ-32B-\\nPreview on all evaluation metrics, while DeepSeek-R1-32B and DeepSeek-R1-70B significantly\\nexceed o1-mini on most benchmarks. These results demonstrate the strong potential of distilla-\\ntion. Additionally, we found that applying RL to these distilled models yields significant further\\ngains. We believe this warrants further exploration and therefore present only the results of the\\nsimple SFT-distilled models here.\\n4. Discussion\\n4.1. Distillation v .s. Reinforcement Learning\\nIn Section 3.2, we can see that by distilling DeepSeek-R1, the small model can achieve impressive\\nresults. However, there is still one question left: can the model achieve comparable performance\\nthrough the large-scale RL training discussed in the paper without distillation?\\nTo answer this question, we conduct large-scale RL training on Qwen-32B-Base using math,\\ncode, and STEM data, training for over 10K steps, resulting in DeepSeek-R1-Zero-Qwen-32B. The\\nexperimental results, shown in Table 6, demonstrate that the 32B base model, after large-scale\\n14'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-23T01:45:31+00:00', 'author': '', 'keywords': '', 'moddate': '2025-01-23T01:45:31+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'papers/2501.12948v1.pdf', 'total_pages': 22, 'page': 14, 'page_label': '15'}, page_content='Model\\nAIME 2024 MATH-500 GPQA Diamond LiveCodeBench\\npass@1 cons@64 pass@1 pass@1 pass@1\\nQwQ-32B-Preview 50.0 60.0 90.6 54.5 41.9\\nDeepSeek-R1-Zero-Qwen-32B 47.0 60.0 91.6 55.0 40.2\\nDeepSeek-R1-Distill-Qwen-32B 72.6 83.3 94.3 62.1 57.2\\nTable 6 | Comparison of distilled and RL Models on Reasoning-Related Benchmarks.\\nRL training, achieves performance on par with QwQ-32B-Preview. However, DeepSeek-R1-\\nDistill-Qwen-32B, which is distilled from DeepSeek-R1, performs significantly better than\\nDeepSeek-R1-Zero-Qwen-32B across all benchmarks.\\nTherefore, we can draw two conclusions: First, distilling more powerful models into smaller\\nones yields excellent results, whereas smaller models relying on the large-scale RL mentioned in\\nthis paper require enormous computational power and may not even achieve the performance\\nof distillation. Second, while distillation strategies are both economical and effective, advancing\\nbeyond the boundaries of intelligence may still require more powerful base models and larger-\\nscale reinforcement learning.\\n4.2. Unsuccessful Attempts\\nIn the early stages of developing DeepSeek-R1, we also encountered failures and setbacks along\\nthe way. We share our failure experiences here to provide insights, but this does not imply that\\nthese approaches are incapable of developing effective reasoning models.\\nProcess Reward Model (PRM) PRM is a reasonable method to guide the model toward better\\napproaches for solving reasoning tasks (Lightman et al., 2023; Uesato et al., 2022; Wang et al.,\\n2023). However, in practice, PRM has three main limitations that may hinder its ultimate suc-\\ncess. First, it is challenging to explicitly define a fine-grain step in general reasoning. Second,\\ndetermining whether the current intermediate step is correct is a challenging task. Automated\\nannotation using models may not yield satisfactory results, while manual annotation is not con-\\nducive to scaling up. Third, once a model-based PRM is introduced, it inevitably leads to reward\\nhacking (Gao et al., 2022), and retraining the reward model needs additional training resources\\nand it complicates the whole training pipeline. In conclusion, while PRM demonstrates a good\\nability to rerank the top-N responses generated by the model or assist in guided search (Snell\\net al., 2024), its advantages are limited compared to the additional computational overhead it\\nintroduces during the large-scale reinforcement learning process in our experiments.\\nMonte Carlo T ree Search (MCTS) Inspired by AlphaGo (Silver et al., 2017b) and AlphaZero (Sil-\\nver et al., 2017a), we explored using Monte Carlo Tree Search (MCTS) to enhance test-time\\ncompute scalability. This approach involves breaking answers into smaller parts to allow the\\nmodel to explore the solution space systematically. To facilitate this, we prompt the model to\\ngenerate multiple tags that correspond to specific reasoning steps necessary for the search. For\\ntraining, we first use collected prompts to find answers via MCTS guided by a pre-trained value\\nmodel. Subsequently, we use the resulting question-answer pairs to train both the actor model\\nand the value model, iteratively refining the process.\\nHowever, this approach encounters several challenges when scaling up the training. First,\\nunlike chess, where the search space is relatively well-defined, token generation presents an\\n15'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-23T01:45:31+00:00', 'author': '', 'keywords': '', 'moddate': '2025-01-23T01:45:31+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'papers/2501.12948v1.pdf', 'total_pages': 22, 'page': 15, 'page_label': '16'}, page_content='exponentially larger search space. To address this, we set a maximum extension limit for each\\nnode, but this can lead to the model getting stuck in local optima. Second, the value model\\ndirectly influences the quality of generation since it guides each step of the search process.\\nTraining a fine-grained value model is inherently difficult, which makes it challenging for the\\nmodel to iteratively improve. While AlphaGo’s core success relied on training a value model to\\nprogressively enhance its performance, this principle proves difficult to replicate in our setup\\ndue to the complexities of token generation.\\nIn conclusion, while MCTS can improve performance during inference when paired with a\\npre-trained value model, iteratively boosting model performance through self-search remains a\\nsignificant challenge.\\n5. Conclusion, Limitations, and Future Work\\nIn this work, we share our journey in enhancing model reasoning abilities through reinforcement\\nlearning. DeepSeek-R1-Zero represents a pure RL approach without relying on cold-start\\ndata, achieving strong performance across various tasks. DeepSeek-R1 is more powerful,\\nleveraging cold-start data alongside iterative RL fine-tuning. Ultimately, DeepSeek-R1 achieves\\nperformance comparable to OpenAI-o1-1217 on a range of tasks.\\nWe further explore distillation the reasoning capability to small dense models. We use\\nDeepSeek-R1 as the teacher model to generate 800K training samples, and fine-tune several small\\ndense models. The results are promising: DeepSeek-R1-Distill-Qwen-1.5B outperforms GPT-4o\\nand Claude-3.5-Sonnet on math benchmarks with 28.9% on AIME and 83.9% on MATH. Other\\ndense models also achieve impressive results, significantly outperforming other instruction-\\ntuned models based on the same underlying checkpoints.\\nIn the future, we plan to invest in research across the following directions for DeepSeek-R1.\\n• General Capability: Currently, the capabilities of DeepSeek-R1 fall short of DeepSeek-V3\\nin tasks such as function calling, multi-turn, complex role-playing, and JSON output.\\nMoving forward, we plan to explore how long CoT can be leveraged to enhance tasks in\\nthese fields.\\n• Language Mixing: DeepSeek-R1 is currently optimized for Chinese and English, which\\nmay result in language mixing issues when handling queries in other languages. For\\ninstance, DeepSeek-R1 might use English for reasoning and responses, even if the query is\\nin a language other than English or Chinese. We aim to address this limitation in future\\nupdates.\\n• Prompting Engineering: When evaluating DeepSeek-R1, we observe that it is sensitive\\nto prompts. Few-shot prompting consistently degrades its performance. Therefore, we\\nrecommend users directly describe the problem and specify the output format using a\\nzero-shot setting for optimal results.\\n• Software Engineering T asks: Due to the long evaluation times, which impact the effi-\\nciency of the RL process, large-scale RL has not been applied extensively in software\\nengineering tasks. As a result, DeepSeek-R1 has not demonstrated a huge improvement\\nover DeepSeek-V3 on software engineering benchmarks. Future versions will address\\nthis by implementing rejection sampling on software engineering data or incorporating\\nasynchronous evaluations during the RL process to improve efficiency.\\n16'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-23T01:45:31+00:00', 'author': '', 'keywords': '', 'moddate': '2025-01-23T01:45:31+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'papers/2501.12948v1.pdf', 'total_pages': 22, 'page': 16, 'page_label': '17'}, page_content='References\\nAI@Meta. Llama 3.1 model card, 2024. URL https://github.com/meta-llama/llama-m\\nodels/blob/main/models/llama3_1/MODEL_CARD.md.\\nAnthropic. Claude 3.5 sonnet, 2024. URL https://www.anthropic.com/news/claude-3\\n-5-sonnet.\\nM. Chen, J. Tworek, H. Jun, Q. Yuan, H. P . de Oliveira Pinto, J. Kaplan, H. Edwards, Y. Burda,\\nN. Joseph, G. Brockman, A. Ray, R. Puri, G. Krueger, M. Petrov, H. Khlaaf, G. Sastry, P . Mishkin,\\nB. Chan, S. Gray, N. Ryder, M. Pavlov, A. Power, L. Kaiser, M. Bavarian, C. Winter, P . Tillet,\\nF. P . Such, D. Cummings, M. Plappert, F. Chantzis, E. Barnes, A. Herbert-Voss, W. H. Guss,\\nA. Nichol, A. Paino, N. Tezak, J. Tang, I. Babuschkin, S. Balaji, S. Jain, W. Saunders, C. Hesse,\\nA. N. Carr, J. Leike, J. Achiam, V . Misra, E. Morikawa, A. Radford, M. Knight, M. Brundage,\\nM. Murati, K. Mayer, P . Welinder, B. McGrew, D. Amodei, S. McCandlish, I. Sutskever, and\\nW. Zaremba. Evaluating large language models trained on code. CoRR, abs/2107.03374, 2021.\\nURL https://arxiv.org/abs/2107.03374.\\nA. Dubey, A. Jauhri, A. Pandey, A. Kadian, A. Al-Dahle, A. Letman, A. Mathur, A. Schelten,\\nA. Yang, A. Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024.\\nY. Dubois, B. Galambosi, P . Liang, and T. B. Hashimoto. Length-controlled alpacaeval: A simple\\nway to debias automatic evaluators. arXiv preprint arXiv:2404.04475, 2024.\\nX. Feng, Z. Wan, M. Wen, S. M. McAleer, Y. Wen, W. Zhang, and J. Wang. Alphazero-like\\ntree-search can guide large language model decoding and training, 2024. URL https:\\n//arxiv.org/abs/2309.17179.\\nL. Gao, J. Schulman, and J. Hilton. Scaling laws for reward model overoptimization, 2022. URL\\nhttps://arxiv.org/abs/2210.10760.\\nA. P . Gema, J. O. J. Leang, G. Hong, A. Devoto, A. C. M. Mancino, R. Saxena, X. He, Y. Zhao,\\nX. Du, M. R. G. Madani, C. Barale, R. McHardy, J. Harris, J. Kaddour, E. van Krieken, and\\nP . Minervini. Are we done with mmlu? CoRR, abs/2406.04127, 2024. URL https://doi.or\\ng/10.48550/arXiv.2406.04127.\\nGoogle. Our next-generation model: Gemini 1.5, 2024. URL https://blog.google/techno\\nlogy/ai/google-gemini-next-generation-model-february-2024 .\\nY. He, S. Li, J. Liu, Y. Tan, W. Wang, H. Huang, X. Bu, H. Guo, C. Hu, B. Zheng, et al. Chi-\\nnese simpleqa: A chinese factuality evaluation for large language models. arXiv preprint\\narXiv:2411.07140, 2024.\\nD. Hendrycks, C. Burns, S. Basart, A. Zou, M. Mazeika, D. Song, and J. Steinhardt. Measuring\\nmassive multitask language understanding. arXiv preprint arXiv:2009.03300, 2020.\\nY. Huang, Y. Bai, Z. Zhu, J. Zhang, J. Zhang, T. Su, J. Liu, C. Lv, Y. Zhang, J. Lei, et al. C-Eval: A\\nmulti-level multi-discipline chinese evaluation suite for foundation models. arXiv preprint\\narXiv:2305.08322, 2023.\\nN. Jain, K. Han, A. Gu, W. Li, F. Yan, T. Zhang, S. Wang, A. Solar-Lezama, K. Sen, and I. Stoica.\\nLivecodebench: Holistic and contamination free evaluation of large language models for code.\\nCoRR, abs/2403.07974, 2024. URL https://doi.org/10.48550/arXiv.2403.07974.\\n17'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-23T01:45:31+00:00', 'author': '', 'keywords': '', 'moddate': '2025-01-23T01:45:31+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'papers/2501.12948v1.pdf', 'total_pages': 22, 'page': 17, 'page_label': '18'}, page_content='S. Krishna, K. Krishna, A. Mohananey, S. Schwarcz, A. Stambler, S. Upadhyay, and M. Faruqui.\\nFact, fetch, and reason: A unified evaluation of retrieval-augmented generation. CoRR,\\nabs/2409.12941, 2024. doi: 10.48550/ARXIV.2409.12941. URL https://doi.org/10.485\\n50/arXiv.2409.12941.\\nA. Kumar, V . Zhuang, R. Agarwal, Y. Su, J. D. Co-Reyes, A. Singh, K. Baumli, S. Iqbal, C. Bishop,\\nR. Roelofs, et al. Training language models to self-correct via reinforcement learning. arXiv\\npreprint arXiv:2409.12917, 2024.\\nH. Li, Y. Zhang, F. Koto, Y. Yang, H. Zhao, Y. Gong, N. Duan, and T. Baldwin. CMMLU: Measur-\\ning massive multitask language understanding in Chinese. arXiv preprint arXiv:2306.09212,\\n2023.\\nT. Li, W.-L. Chiang, E. Frick, L. Dunlap, T. Wu, B. Zhu, J. E. Gonzalez, and I. Stoica. From\\ncrowdsourced data to high-quality benchmarks: Arena-hard and benchbuilder pipeline. arXiv\\npreprint arXiv:2406.11939, 2024.\\nH. Lightman, V . Kosaraju, Y. Burda, H. Edwards, B. Baker, T. Lee, J. Leike, J. Schulman,\\nI. Sutskever, and K. Cobbe. Let’s verify step by step. arXiv preprint arXiv:2305.20050, 2023.\\nB. Y. Lin. ZeroEval: A Unified Framework for Evaluating Language Models, July 2024. URL\\nhttps://github.com/WildEval/ZeroEval.\\nMAA. American invitational mathematics examination - aime. In American Invitational\\nMathematics Examination - AIME 2024, February 2024. URL https://maa.org/math\\n-competitions/american-invitational-mathematics-examination-aime .\\nOpenAI. Hello GPT-4o, 2024a. URL https://openai.com/index/hello-gpt-4o/.\\nOpenAI. Learning to reason with llms, 2024b. URL https://openai.com/index/learnin\\ng-to-reason-with-llms/ .\\nOpenAI. Introducing SimpleQA, 2024c. URL https://openai.com/index/introducing\\n-simpleqa/.\\nOpenAI. Introducing SWE-bench verified we’re releasing a human-validated subset of swe-\\nbench that more, 2024d. URL https://openai.com/index/introducing-swe-bench\\n-verified/.\\nQwen. Qwq: Reflect deeply on the boundaries of the unknown, 2024a. URL https://qwenlm\\n.github.io/blog/qwq-32b-preview/.\\nQwen. Qwen2.5: A party of foundation models, 2024b. URL https://qwenlm.github.io/b\\nlog/qwen2.5.\\nD. Rein, B. L. Hou, A. C. Stickland, J. Petty, R. Y. Pang, J. Dirani, J. Michael, and S. R. Bowman.\\nGPQA: A graduate-level google-proof q&a benchmark. arXiv preprint arXiv:2311.12022, 2023.\\nZ. Shao, P . Wang, Q. Zhu, R. Xu, J. Song, M. Zhang, Y. Li, Y. Wu, and D. Guo. Deepseekmath:\\nPushing the limits of mathematical reasoning in open language models. arXiv preprint\\narXiv:2402.03300, 2024.\\nD. Silver, T. Hubert, J. Schrittwieser, I. Antonoglou, M. Lai, A. Guez, M. Lanctot, L. Sifre,\\nD. Kumaran, T. Graepel, T. P . Lillicrap, K. Simonyan, and D. Hassabis. Mastering chess and\\nshogi by self-play with a general reinforcement learning algorithm. CoRR, abs/1712.01815,\\n2017a. URL http://arxiv.org/abs/1712.01815.\\n18'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-23T01:45:31+00:00', 'author': '', 'keywords': '', 'moddate': '2025-01-23T01:45:31+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'papers/2501.12948v1.pdf', 'total_pages': 22, 'page': 18, 'page_label': '19'}, page_content='D. Silver, J. Schrittwieser, K. Simonyan, I. Antonoglou, A. Huang, A. Guez, T. Hubert, L. Baker,\\nM. Lai, A. Bolton, Y. Chen, T. P . Lillicrap, F. Hui, L. Sifre, G. van den Driessche, T. Graepel, and\\nD. Hassabis. Mastering the game of go without human knowledge. Nat., 550(7676):354–359,\\n2017b. doi: 10.1038/NATURE24270. URL https://doi.org/10.1038/nature24270.\\nC. Snell, J. Lee, K. Xu, and A. Kumar. Scaling llm test-time compute optimally can be more\\neffective than scaling model parameters, 2024. URL https://arxiv.org/abs/2408.033\\n14.\\nT. Trinh, Y. Wu, Q. Le, H. He, and T. Luong. Solving olympiad geometry without human\\ndemonstrations. Nature, 2024. doi: 10.1038/s41586-023-06747-5.\\nJ. Uesato, N. Kushman, R. Kumar, F. Song, N. Siegel, L. Wang, A. Creswell, G. Irving, and\\nI. Higgins. Solving math word problems with process-and outcome-based feedback. arXiv\\npreprint arXiv:2211.14275, 2022.\\nP . Wang, L. Li, Z. Shao, R. Xu, D. Dai, Y. Li, D. Chen, Y. Wu, and Z. Sui. Math-shepherd: A label-\\nfree step-by-step verifier for llms in mathematical reasoning. arXiv preprint arXiv:2312.08935,\\n2023.\\nX. Wang, J. Wei, D. Schuurmans, Q. Le, E. Chi, S. Narang, A. Chowdhery, and D. Zhou.\\nSelf-consistency improves chain of thought reasoning in language models. arXiv preprint\\narXiv:2203.11171, 2022.\\nY. Wang, X. Ma, G. Zhang, Y. Ni, A. Chandra, S. Guo, W. Ren, A. Arulraj, X. He, Z. Jiang, T. Li,\\nM. Ku, K. Wang, A. Zhuang, R. Fan, X. Yue, and W. Chen. Mmlu-pro: A more robust and\\nchallenging multi-task language understanding benchmark. CoRR, abs/2406.01574, 2024.\\nURL https://doi.org/10.48550/arXiv.2406.01574.\\nC. S. Xia, Y. Deng, S. Dunn, and L. Zhang. Agentless: Demystifying llm-based software\\nengineering agents. arXiv preprint, 2024.\\nH. Xin, Z. Z. Ren, J. Song, Z. Shao, W. Zhao, H. Wang, B. Liu, L. Zhang, X. Lu, Q. Du, W. Gao,\\nQ. Zhu, D. Yang, Z. Gou, Z. F. Wu, F. Luo, and C. Ruan. Deepseek-prover-v1.5: Harnessing\\nproof assistant feedback for reinforcement learning and monte-carlo tree search, 2024. URL\\nhttps://arxiv.org/abs/2408.08152.\\nJ. Zhou, T. Lu, S. Mishra, S. Brahma, S. Basu, Y. Luan, D. Zhou, and L. Hou. Instruction-following\\nevaluation for large language models. arXiv preprint arXiv:2311.07911, 2023.\\n19'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-23T01:45:31+00:00', 'author': '', 'keywords': '', 'moddate': '2025-01-23T01:45:31+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'papers/2501.12948v1.pdf', 'total_pages': 22, 'page': 19, 'page_label': '20'}, page_content='Appendix\\nA. Contributions and Acknowledgments\\nCore Contributors\\nDaya Guo\\nDejian Yang\\nHaowei Zhang\\nJunxiao Song\\nRuoyu Zhang\\nRunxin Xu\\nQihao Zhu\\nShirong Ma\\nPeiyi Wang\\nXiao Bi\\nXiaokang Zhang\\nXingkai Yu\\nYu Wu\\nZ.F. Wu\\nZhibin Gou\\nZhihong Shao\\nZhuoshu Li\\nZiyi Gao\\nContributors\\nAixin Liu\\nBing Xue\\nBingxuan Wang\\nBochao Wu\\nBei Feng\\nChengda Lu\\nChenggang Zhao\\nChengqi Deng\\nChong Ruan\\nDamai Dai\\nDeli Chen\\nDongjie Ji\\nErhang Li\\nFangyun Lin\\nFucong Dai\\nFuli Luo*\\nGuangbo Hao\\nGuanting Chen\\nGuowei Li\\nH. Zhang\\nHanwei Xu\\nHonghui Ding\\nHuazuo Gao\\nHui Qu\\nHui Li\\nJianzhong Guo\\nJiashi Li\\nJingchang Chen\\nJingyang Yuan\\nJinhao Tu\\nJunjie Qiu\\nJunlong Li\\nJ.L. Cai\\nJiaqi Ni\\nJian Liang\\nJin Chen\\nKai Dong\\nKai Hu*\\nKaichao You\\nKaige Gao\\nKang Guan\\nKexin Huang\\nKuai Yu\\nLean Wang\\nLecong Zhang\\nLiang Zhao\\nLitong Wang\\nLiyue Zhang\\nLei Xu\\nLeyi Xia\\nMingchuan Zhang\\nMinghua Zhang\\nMinghui Tang\\nMingxu Zhou\\nMeng Li\\nMiaojun Wang\\nMingming Li\\nNing Tian\\nPanpan Huang\\nPeng Zhang\\nQiancheng Wang\\nQinyu Chen\\nQiushi Du\\nRuiqi Ge*\\nRuisong Zhang\\nRuizhe Pan\\nRunji Wang\\nR.J. Chen\\nR.L. Jin\\n20'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-23T01:45:31+00:00', 'author': '', 'keywords': '', 'moddate': '2025-01-23T01:45:31+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'papers/2501.12948v1.pdf', 'total_pages': 22, 'page': 20, 'page_label': '21'}, page_content='Ruyi Chen\\nShanghao Lu\\nShangyan Zhou\\nShanhuang Chen\\nShengfeng Ye\\nShiyu Wang\\nShuiping Yu\\nShunfeng Zhou\\nShuting Pan\\nS.S. Li\\nShuang Zhou\\nShaoqing Wu\\nShengfeng Ye\\nTao Yun\\nTian Pei\\nTianyu Sun\\nT. Wang\\nWangding Zeng\\nWen Liu\\nWenfeng Liang\\nWenjun Gao\\nWenqin Yu*\\nWentao Zhang\\nW.L. Xiao\\nWei An\\nXiaodong Liu\\nXiaohan Wang\\nXiaokang Chen\\nXiaotao Nie\\nXin Cheng\\nXin Liu\\nXin Xie\\nXingchao Liu\\nXinyu Yang\\nXinyuan Li\\nXuecheng Su\\nXuheng Lin\\nX.Q. Li\\nXiangyue Jin\\nXiaojin Shen\\nXiaosha Chen\\nXiaowen Sun\\nXiaoxiang Wang\\nXinnan Song\\nXinyi Zhou\\nXianzu Wang\\nXinxia Shan\\nY.K. Li\\nY.Q. Wang\\nY.X. Wei\\nYang Zhang\\nYanhong Xu\\nYao Li\\nYao Zhao\\nYaofeng Sun\\nYaohui Wang\\nYi Yu\\nYichao Zhang\\nYifan Shi\\nYiliang Xiong\\nYing He\\nYishi Piao\\nYisong Wang\\nYixuan Tan\\nYiyang Ma*\\nYiyuan Liu\\nYongqiang Guo\\nYuan Ou\\nYuduan Wang\\nYue Gong\\nYuheng Zou\\nYujia He\\nYunfan Xiong\\nYuxiang Luo\\nYuxiang You\\nYuxuan Liu\\nYuyang Zhou\\nY.X. Zhu\\nYanping Huang\\nYaohui Li\\nYi Zheng\\nYuchen Zhu\\nYunxian Ma\\nYing Tang\\nYukun Zha\\nYuting Yan\\nZ.Z. Ren\\nZehui Ren\\nZhangli Sha\\nZhe Fu\\nZhean Xu\\nZhenda Xie\\nZhengyan Zhang\\nZhewen Hao\\nZhicheng Ma\\nZhigang Yan\\nZhiyu Wu\\nZihui Gu\\n21'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-23T01:45:31+00:00', 'author': '', 'keywords': '', 'moddate': '2025-01-23T01:45:31+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'papers/2501.12948v1.pdf', 'total_pages': 22, 'page': 21, 'page_label': '22'}, page_content='Zijia Zhu\\nZijun Liu*\\nZilin Li\\nZiwei Xie\\nZiyang Song\\nZizheng Pan\\nZhen Huang\\nZhipeng Xu\\nZhongyu Zhang\\nZhen Zhang\\nWithin each role, authors are listed alphabetically by the first name. Names marked with *\\ndenote individuals who have departed from our team.\\n22')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3) Load all PDF file\n",
    "\n",
    "docs = DirectoryLoader(\n",
    "    'papers', '**/*.pdf', loader_cls= PyPDFLoader\n",
    ").load()\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e114be8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded docs: 52\n",
      "Example metadata: {'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-05-23T01:29:23+00:00', 'author': '', 'keywords': '', 'moddate': '2022-05-23T01:29:23+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'papers/2203.14465v2.pdf', 'total_pages': 30, 'page': 0, 'page_label': '1'}\n"
     ]
    }
   ],
   "source": [
    "print('Loaded docs:', len(docs))\n",
    "print('Example metadata:', docs[0].metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "20b0fc1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chunks: 215\n",
      "Example Chunk metadata: {'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-05-23T01:29:23+00:00', 'author': '', 'keywords': '', 'moddate': '2022-05-23T01:29:23+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'papers/2203.14465v2.pdf', 'total_pages': 30, 'page': 0, 'page_label': '1'}\n"
     ]
    }
   ],
   "source": [
    "# 4) Split the text into chunks\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 1000, chunk_overlap = 200\n",
    ")\n",
    "chunks = splitter.split_documents(docs)\n",
    "\n",
    "print('Total chunks:', len(chunks))\n",
    "print('Example Chunk metadata:', chunks[0].metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "07956934",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mohinikathrotiya/Desktop/Langchain/lcenv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading weights: 100%|██████████| 103/103 [00:00<00:00, 2736.81it/s, Materializing param=pooler.dense.weight]                             \n",
      "BertModel LOAD REPORT from: sentence-transformers/all-MiniLM-L6-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
     ]
    }
   ],
   "source": [
    "# 5) Create Embeddings\n",
    "\n",
    "embeddings = HuggingFaceBgeEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b4c08ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6) Store the Embeddings in Vector DB (FAISS)\n",
    "vector_db = FAISS.from_documents(chunks, embeddings)\n",
    "\n",
    "# saving to disk\n",
    "vector_db.save_local('faiss_index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "df4b1fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7) Retrieval: query -> similar chunks\n",
    "\n",
    "query = 'What is the main idea discussed in these papers'\n",
    "retriever = vector_db.as_retriever(search_kwargs = {'k': 4})\n",
    "top_docs = retriever.invoke(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4f308c03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top matches:\n",
      "\n",
      "[1] Source: papers/2501.12948v1.pdf | Page: 1\n",
      "4.2 Unsuccessful Attempts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15\n",
      "5 Conclusion, Limitations, and Future Work 16\n",
      "A Contributions and Acknowledgments 20\n",
      "2 ...\n",
      "\n",
      "[2] Source: papers/2203.14465v2.pdf | Page: 9\n",
      "Acknowledgements\n",
      "We thank Imanol Schlag for his detailed feedback about this work, as well as Rose E Wang, Markus\n",
      "Rabe, Aitor Lewkowycz, Rishi Bommasani, Allen Nie, Alex Tamkin, and Qian Huang. We thank\n",
      "Cem Anil for his very helpful insight that rationale ﬁnetuning performance can be improved if the ...\n",
      "\n",
      "[3] Source: papers/2203.14465v2.pdf | Page: 12\n",
      "Appendix\n",
      "A CommonsenseQA Error Patterns\n",
      "Throughout our experiments, we came across a variety of interesting failure cases for commonsense\n",
      "reasoning. Note that all the ﬁnal answers are correct – however, we take issue with the reasoning\n",
      "used in order to arrive at those answers.\n",
      "A.1 Question Implies A ...\n",
      "\n",
      "[4] Source: papers/2203.14465v2.pdf | Page: 23\n",
      "rather than the STaR-trained model, as the ﬁnal iteration’s rationales are the ones which the model\n",
      "struggled to learn from – that is, if the rationales were good, then we might expect that the model\n",
      "would have already incorporated them into its rationale generation.\n",
      "Q: Why do I eat just enough when ...\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nTop matches:\")\n",
    "for i, d in enumerate(top_docs, 1):\n",
    "    src = d.metadata.get('source', 'unknown')\n",
    "    page = d.metadata.get('page', 'N/A')\n",
    "    print(f'\\n[{i}] Source: {src} | Page: {page}')\n",
    "    print(d.page_content[:300], '...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff3594d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lcenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
