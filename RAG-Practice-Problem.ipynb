{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1987ba8",
   "metadata": {},
   "source": [
    "#### Build a single-turn RAG Pipeline, that answers questions from a documentation-style web-page, using the correct splitter and chain setup."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d9f8d0",
   "metadata": {},
   "source": [
    "#### Setting up the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4431933f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d90f17",
   "metadata": {},
   "source": [
    "#### 1) Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "11a92627",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "import bs4 \n",
    "\n",
    "loader = WebBaseLoader(\n",
    "    'https://docs.langchain.com/oss/python/langchain/rag?utm_source=chatgpt.com',\n",
    "    bs_kwargs=dict(parse_only = bs4.SoupStrainer(\n",
    "        id = 'content-container'\n",
    "    )\n",
    "    ) # This helps scrape only the main content\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4bcd3aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "86cd612d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Docs: 1\n"
     ]
    }
   ],
   "source": [
    "print('Loaded Docs:', len(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "52974627",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample chars: 30691\n",
      "Sample preview:\n",
      " On this pageOverviewConceptsPreviewSetupInstallationLangSmithComponents1. IndexingLoading documentsSplitting documentsStoring documents2. Retrieval and generationRAG agentsRAG chainsNext stepsTutorialsLangChainBuild a RAG agent with LangChainCopy pageCopy page​Overview\n",
      "One of the most powerful applications enabled by LLMs is sophisticated question-answering (Q&A) chatbots. These are applications that can answer questions about specific source information. These applications use a technique known as Retrieval Augmented Generation, or RAG.\n",
      "This tutorial will show how to build a simple Q&A application over an unstructured text data source. We will demonstrate:\n",
      "\n",
      "A RAG agent that executes searches with a simple tool. This is a good general-purpose implementation.\n",
      "A two-step RAG chain that uses just a single LLM call per query. This is a fast and effective method for simple queries.\n",
      "\n",
      "​Concepts\n",
      "We will cover the following concepts:\n",
      "\n",
      "\n",
      "Indexing: a pipeline for ingesting data from a source and indexing it. This usually happens in a separate process.\n",
      "\n",
      "\n",
      "Retrieval and generation: the actual RAG process, which takes the user query at run time and retrieves the relevant data from the index, then passes that to the model.\n",
      "\n",
      "\n",
      "Once we’ve indexed our data, we will use an agent as our orchestration framework to implement the retrieval and generation steps.\n",
      "The indexing portion of this tutorial will largely follow the semantic search tutorial.If your data is already available for search (i.e., you have a function to execute a search), or you’re comfortable with the content from that tutorial, feel free to skip to the section on retrieval and generation\n",
      "​Preview\n",
      "In this guide we’ll build an app that answers questions about the website’s content. The specific website we will use is the LLM Powered Autonomous Agents blog post by Lilian Weng, which allows us to ask questions about the contents of the post.\n",
      "We can create a simple indexing pipeline and RAG chain to do this in ~40 lines of code. See below for the full code snippet:\n",
      "Expand for full code snippetCopyimport bs4\n",
      "from langchain.agents import AgentState, create_agent\n",
      "from langchain_community.document_loaders import WebBaseLoader\n",
      "from langchain.messages import MessageLikeRepresentation\n",
      "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
      "\n",
      "# Load and chunk contents of the blog\n",
      "loader = WebBaseLoader(\n",
      "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
      "    bs_kwargs=dict(\n",
      "        parse_only=b\n"
     ]
    }
   ],
   "source": [
    "print(\"Sample chars:\", len(doc[0].page_content))\n",
    "print(\"Sample preview:\\n\", doc[0].page_content[:2500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a36e04",
   "metadata": {},
   "source": [
    "#### 2) Splitting the text into chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5bbdbe19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Total chunks: 41\n",
      "One Chunk preview: On this pageOverviewConceptsPreviewSetupInstallationLangSmithComponents1. IndexingLoading documentsSplitting documentsStoring documents2. Retrieval and generationRAG agentsRAG chainsNext stepsTutorialsLangChainBuild a RAG agent with LangChainCopy pageCopy page​Overview\n",
      "One of the most powerful applications enabled by LLMs is sophisticated question-answering (Q&A) chatbots. These are applications that can answer questions about specific source information. These applications use a technique known\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 1000, chunk_overlap = 200\n",
    ")\n",
    "chunks = splitter.split_documents(doc)\n",
    "\n",
    "print('\\n Total chunks:', len(chunks))\n",
    "print('One Chunk preview:', chunks[0].page_content[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac335db",
   "metadata": {},
   "source": [
    "#### 3) Creating Embeddings for the chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b4aecb3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0aa6c7f",
   "metadata": {},
   "source": [
    "#### 4) Creating vectore Store (FAISS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c55be600",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding dimension (from FAISS:) 1536\n"
     ]
    }
   ],
   "source": [
    "from langchain_classic.vectorstores import FAISS\n",
    "\n",
    "vector_db = FAISS.from_documents(chunks, embeddings)\n",
    "\n",
    "print('Embedding dimension (from FAISS:)', vector_db.index.d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9748dca",
   "metadata": {},
   "source": [
    "#### 5) Retriever "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c62cddc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vector_db.as_retriever(search_kwargs ={'k': 4})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f4ebeb",
   "metadata": {},
   "source": [
    "#### 6) Document Chain (LLM + Prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "02e7eb7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "llm = ChatOpenAI(model = 'gpt-4o')\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "You are a helpful assistant.\n",
    "Answer the following using ONLY the context below.\n",
    "\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "Question: {input}\n",
    "\n",
    "Rules:\n",
    "- If the answer is not in the context, say: I do not know based on the provided context.\n",
    "-Keep the answer short and direct.\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "c385456a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_classic.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "document_chain = create_stuff_documents_chain(llm, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "04a6ab6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
       "  context: RunnableLambda(format_docs)\n",
       "}), kwargs={}, config={'run_name': 'format_inputs'}, config_factories=[])\n",
       "| ChatPromptTemplate(input_variables=['context', 'input'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'input'], input_types={}, partial_variables={}, template='\\nYou are a helpful assistant.\\nAnswer the following using ONLY the context below.\\n\\n<context>\\n{context}\\n</context>\\n\\nQuestion: {input}\\n\\nRules:\\n- If the answer is not in the context, say: I do not know based on the provided context.\\n-Keep the answer short and direct.\\n'), additional_kwargs={})])\n",
       "| ChatOpenAI(profile={'max_input_tokens': 128000, 'max_output_tokens': 16384, 'image_inputs': True, 'audio_inputs': False, 'video_inputs': False, 'image_outputs': False, 'audio_outputs': False, 'video_outputs': False, 'reasoning_output': False, 'tool_calling': True, 'structured_output': True, 'image_url_inputs': True, 'pdf_inputs': True, 'pdf_tool_message': True, 'image_tool_message': True, 'tool_choice': True}, client=<openai.resources.chat.completions.completions.Completions object at 0x147b7be10>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x149b58180>, root_client=<openai.OpenAI object at 0x147b7afd0>, root_async_client=<openai.AsyncOpenAI object at 0x149b58fc0>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********'), stream_usage=True)\n",
       "| StrOutputParser(), kwargs={}, config={'run_name': 'stuff_documents_chain'}, config_factories=[])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d300566b",
   "metadata": {},
   "source": [
    "#### 7) Retriever Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "b3f60fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_classic.chains import create_retrieval_chain\n",
    "\n",
    "retrieval_chain = create_retrieval_chain(retriever, document_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "44437963",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableBinding(bound=RunnableAssign(mapper={\n",
       "  context: RunnableBinding(bound=RunnableLambda(lambda x: x['input'])\n",
       "           | VectorStoreRetriever(tags=['FAISS', 'OpenAIEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x1484ce900>, search_kwargs={'k': 4}), kwargs={}, config={'run_name': 'retrieve_documents'}, config_factories=[])\n",
       "})\n",
       "| RunnableAssign(mapper={\n",
       "    answer: RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
       "              context: RunnableLambda(format_docs)\n",
       "            }), kwargs={}, config={'run_name': 'format_inputs'}, config_factories=[])\n",
       "            | ChatPromptTemplate(input_variables=['context', 'input'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'input'], input_types={}, partial_variables={}, template='\\nYou are a helpful assistant.\\nAnswer the following using ONLY the context below.\\n\\n<context>\\n{context}\\n</context>\\n\\nQuestion: {input}\\n\\nRules:\\n- If the answer is not in the context, say: I do not know based on the provided context.\\n-Keep the answer short and direct.\\n'), additional_kwargs={})])\n",
       "            | ChatOpenAI(profile={'max_input_tokens': 128000, 'max_output_tokens': 16384, 'image_inputs': True, 'audio_inputs': False, 'video_inputs': False, 'image_outputs': False, 'audio_outputs': False, 'video_outputs': False, 'reasoning_output': False, 'tool_calling': True, 'structured_output': True, 'image_url_inputs': True, 'pdf_inputs': True, 'pdf_tool_message': True, 'image_tool_message': True, 'tool_choice': True}, client=<openai.resources.chat.completions.completions.Completions object at 0x147b7be10>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x149b58180>, root_client=<openai.OpenAI object at 0x147b7afd0>, root_async_client=<openai.AsyncOpenAI object at 0x149b58fc0>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********'), stream_usage=True)\n",
       "            | StrOutputParser(), kwargs={}, config={'run_name': 'stuff_documents_chain'}, config_factories=[])\n",
       "  }), kwargs={}, config={'run_name': 'retrieval_chain'}, config_factories=[])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieval_chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b76c9d",
   "metadata": {},
   "source": [
    "#### 8) Testing with a few queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "69602dae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==========================================================================================\n",
      "Question: What are the main stages of a RAG pipeline described in the tutorial?\n",
      "\n",
      "Answer:\n",
      " The main stages of a RAG pipeline described in the tutorial are Indexing and Retrieval and Generation.\n",
      "\n",
      " Retrieved Contact:\n",
      "\n",
      " Chunk 1 preview: ​Concepts We will cover the following concepts:   Indexing: a pipeline for ingesting data from a source and indexing it. This usually happens in a separate process.   Retrieval and generation: the actual RAG process, which takes the user query at run time and retrieves the relevant data from the index, then passes that to the model. ....\n",
      " Metadata: {'source': 'https://docs.langchain.com/oss/python/langchain/rag?utm_source=chatgpt.com'}\n",
      "\n",
      " Chunk 2 preview: We can see the full sequence of steps, along with latency and other metadata, in the LangSmith trace. You can add a deeper level of control and customization using the LangGraph framework directly— for example, you can add steps to grade document relevance and rewrite search queries. Check out LangGraph’s Agentic RAG tutorial for more advanced formulations. ​RAG chains In the above agentic RAG formulation we allow the LLM to use its discretion in generating a tool call to help answer user querie ....\n",
      " Metadata: {'source': 'https://docs.langchain.com/oss/python/langchain/rag?utm_source=chatgpt.com'}\n",
      "\n",
      " Chunk 3 preview: On this pageOverviewConceptsPreviewSetupInstallationLangSmithComponents1. IndexingLoading documentsSplitting documentsStoring documents2. Retrieval and generationRAG agentsRAG chainsNext stepsTutorialsLangChainBuild a RAG agent with LangChainCopy pageCopy page​Overview One of the most powerful applications enabled by LLMs is sophisticated question-answering (Q&A) chatbots. These are applications that can answer questions about specific source information. These applications use a technique known ....\n",
      " Metadata: {'source': 'https://docs.langchain.com/oss/python/langchain/rag?utm_source=chatgpt.com'}\n",
      "\n",
      " Chunk 4 preview: Now let’s write the actual application logic. We want to create a simple application that takes a user question, searches for documents relevant to that question, passes the retrieved documents and initial question to a model, and returns an answer. We will demonstrate:  A RAG agent that executes searches with a simple tool. This is a good general-purpose implementation. A two-step RAG chain that uses just a single LLM call per query. This is a fast and effective method for simple queries.  ​RAG ....\n",
      " Metadata: {'source': 'https://docs.langchain.com/oss/python/langchain/rag?utm_source=chatgpt.com'}\n",
      "\n",
      "==========================================================================================\n",
      "Question: What is LangChain's revenue in 2025?\n",
      "\n",
      "Answer:\n",
      " I do not know based on the provided context.\n",
      "\n",
      " Retrieved Contact:\n",
      "\n",
      " Chunk 1 preview: We can see the full sequence of steps, along with latency and other metadata, in the LangSmith trace. You can add a deeper level of control and customization using the LangGraph framework directly— for example, you can add steps to grade document relevance and rewrite search queries. Check out LangGraph’s Agentic RAG tutorial for more advanced formulations. ​RAG chains In the above agentic RAG formulation we allow the LLM to use its discretion in generating a tool call to help answer user querie ....\n",
      " Metadata: {'source': 'https://docs.langchain.com/oss/python/langchain/rag?utm_source=chatgpt.com'}\n",
      "\n",
      " Chunk 2 preview: On this pageOverviewConceptsPreviewSetupInstallationLangSmithComponents1. IndexingLoading documentsSplitting documentsStoring documents2. Retrieval and generationRAG agentsRAG chainsNext stepsTutorialsLangChainBuild a RAG agent with LangChainCopy pageCopy page​Overview One of the most powerful applications enabled by LLMs is sophisticated question-answering (Q&A) chatbots. These are applications that can answer questions about specific source information. These applications use a technique known ....\n",
      " Metadata: {'source': 'https://docs.langchain.com/oss/python/langchain/rag?utm_source=chatgpt.com'}\n",
      "\n",
      " Chunk 3 preview: ​Next steps Now that we’ve implemented a simple RAG application via create_agent, we can easily incorporate new features and go deeper:  Stream tokens and other information for responsive user experiences Add conversational memory to support multi-turn interactions Add long-term memory to support memory across conversational threads Add structured responses Deploy your application with LangSmith Deployment   Edit this page on GitHub or file an issue. Connect these docs to Claude, VSCode, and mor ....\n",
      " Metadata: {'source': 'https://docs.langchain.com/oss/python/langchain/rag?utm_source=chatgpt.com'}\n",
      "\n",
      " Chunk 4 preview: For more details, see our Installation guide. ​LangSmith Many of the applications you build with LangChain will contain multiple steps with multiple invocations of LLM calls. As these applications get more complex, it becomes crucial to be able to inspect what exactly is going on inside your chain or agent. The best way to do this is with LangSmith. After you sign up at the link above, make sure to set your environment variables to start logging traces: Copyexport LANGSMITH_TRACING=\"true\" export ....\n",
      " Metadata: {'source': 'https://docs.langchain.com/oss/python/langchain/rag?utm_source=chatgpt.com'}\n"
     ]
    }
   ],
   "source": [
    "questions = [\n",
    "    'What are the main stages of a RAG pipeline described in the tutorial?',\n",
    "    \"What is LangChain's revenue in 2025?\"\n",
    "]\n",
    "\n",
    "\n",
    "for q in questions:\n",
    "    print('\\n' + '='*90)\n",
    "    print('Question:', q)\n",
    "    resu = retrieval_chain.invoke({'input': q})\n",
    "\n",
    "    print('\\nAnswer:\\n', resu['answer'])\n",
    "\n",
    "    print('\\n Retrieved Contact:')\n",
    "    for i, d in enumerate(resu['context'], 1):\n",
    "        preview = d.page_content[:500].replace('\\n', \" \")\n",
    "        print(f'\\n Chunk {i} preview: {preview} ....')\n",
    "        print(' Metadata:', d.metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "bd04580a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==========================================================================================\n",
      "Question: What are the main stages of a RAG pipeline described in the tutorial?\n",
      "\n",
      "Answer:\n",
      " The main stages of a RAG pipeline described in the tutorial are indexing and retrieval and generation.\n",
      "\n",
      "==========================================================================================\n",
      "Question: What is LangChain's revenue in 2025?\n",
      "\n",
      "Answer:\n",
      " I do not know based on the provided context.\n"
     ]
    }
   ],
   "source": [
    "questions = [\n",
    "    'What are the main stages of a RAG pipeline described in the tutorial?',\n",
    "    \"What is LangChain's revenue in 2025?\"\n",
    "]\n",
    "\n",
    "\n",
    "for q in questions:\n",
    "    print('\\n' + '='*90)\n",
    "    print('Question:', q)\n",
    "    resu = retrieval_chain.invoke({'input': q})\n",
    "\n",
    "    print('\\nAnswer:\\n', resu['answer'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83964801",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lcenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
