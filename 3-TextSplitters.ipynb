{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "850d3611",
   "metadata": {},
   "source": [
    "Why we need text splitting?\n",
    "\n",
    "Even if a PDF loads correctly, a single page/chunk can be: \n",
    "- too long for the LLM context window\n",
    "- too large for good retrieval (embeddings work better on focused chunks)\n",
    "\n",
    "So we split into smaller, overlapping chunks that are:\n",
    "- searchable (via embeddings)\n",
    "- small enough to pass into the LLM\n",
    "- still coherent (overlap keeps continuity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01989ba4",
   "metadata": {},
   "source": [
    "#### RecursiveCharacterTextSplitter (Most Used):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "69f6e3e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original docs: 415\n",
      "Split chunks: 800\n",
      "\n",
      "Example chunk metadata: {'producer': 'Prince 15 (www.princexml.com)', 'creator': 'PyPDF', 'creationdate': '2024-03-15T15:25:16-05:00', 'moddate': '2024-03-15T15:25:16-05:00', 'title': 'Introduction to Python Programming', 'source': 'Introduction_to_Python_Programming_-_WEB.pdf', 'total_pages': 415, 'page': 2, 'page_label': '3'}\n",
      "\n",
      "Example chunk preview:\n",
      " Introduction to Python Programming          SENIOR CONTRIBUTING AUTHORS UDAYAN DAS, SAINT MARY'S COLLEGE OF CALIFORNIA AUBREY LAWSON, WILEY CHRIS MAYFIELD, JAMES MADISON UNIVERSITY NARGES NOROUZI, UC BERKELEY\n",
      "\n",
      "Chunk length: 208\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "docs = PyPDFLoader('Introduction_to_Python_Programming_-_WEB.pdf').load()\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 1000, \n",
    "    chunk_overlap = 200\n",
    ")\n",
    "\n",
    "splits = splitter.split_documents(docs)\n",
    "\n",
    "print('Original docs:', len(docs))\n",
    "print('Split chunks:', len(splits))\n",
    "\n",
    "\n",
    "print(\"\\nExample chunk metadata:\", splits[0].metadata)\n",
    "print(\"\\nExample chunk preview:\\n\", splits[0].page_content[:400])\n",
    "print(\"\\nChunk length:\", len(splits[0].page_content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ab501a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk_size=500, overlap=50 -> chunks=1383, avg_len=404\n",
      "chunk_size=1000, overlap=200 -> chunks=800, avg_len=754\n",
      "chunk_size=1500, overlap=200 -> chunks=548, avg_len=1027\n"
     ]
    }
   ],
   "source": [
    "def try_split(chunk_size, chunk_overlap):\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap\n",
    "    )\n",
    "    splits = splitter.split_documents(docs)\n",
    "    avg_len = sum(len(s.page_content) for s in splits) / len(splits)\n",
    "    print(f\"chunk_size={chunk_size}, overlap={chunk_overlap} -> chunks={len(splits)}, avg_len={avg_len:.0f}\")\n",
    "\n",
    "try_split(500, 50)\n",
    "try_split(1000, 200)\n",
    "try_split(1500, 200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aeb638dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty chunks: 0\n"
     ]
    }
   ],
   "source": [
    "# are there empty chunks\n",
    "empty = [s for s in splits if not s.page_content.strip()]\n",
    "print(\"Empty chunks:\", len(empty))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "307a5deb",
   "metadata": {},
   "source": [
    "#### Practice Problem 1)\n",
    "\n",
    "1) Using your papers/ PDFs:\n",
    "    - Split using:\n",
    "        - chunk_size=1000, chunk_overlap=200\n",
    "2) Print:\n",
    "    - number of chunks per PDF (per paper_name)\n",
    "    - show 1 chunk from each PDF (metadata + first 200 chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c9e8e45d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunks per PDF:\n",
      "papers/2203.14465v2.pdf 136\n",
      "papers/2501.12948v1.pdf 79\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader, DirectoryLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from collections import defaultdict\n",
    "\n",
    "docs1 = DirectoryLoader(\n",
    "    \"papers\",\n",
    "    glob = '**/*.pdf',\n",
    "    loader_cls= PyPDFLoader,\n",
    ").load()\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 1000,\n",
    "    chunk_overlap = 200\n",
    ")\n",
    "\n",
    "splits = splitter.split_documents(docs1)\n",
    "\n",
    "chunks_per_pdf = defaultdict(int)\n",
    "for s in splits:\n",
    "    chunks_per_pdf[s.metadata['source']] += 1\n",
    "\n",
    "print('Chunks per PDF:')\n",
    "for k, v in chunks_per_pdf.items():\n",
    "    print(k, v)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f42cb32c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c13ffbd4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lcenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
