{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df25a6bd",
   "metadata": {},
   "source": [
    "### 1) First Langchain call (Chat Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108b3e3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChain is a framework designed for building applications that utilize language models, enabling developers to create complex workflows by integrating various components like data sources, APIs, and user interfaces. It simplifies the process of chaining together different tasks and functionalities to enhance the capabilities of language models in real-world applications.\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model='gpt-4o-mini', temperature=0) # ChatOpenAI is chat model wraper\n",
    "\n",
    "response = llm.invoke('explain LangChain in 2 lines.') # .invole sends a signle input and returns a response in 2 Lines.\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a41692fd",
   "metadata": {},
   "source": [
    "### 2) PromptTemplate  (Dynamic Prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e693fdba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='A vector database is a specialized type of database designed to store and manage high-dimensional vectors, which represent data points in a multi-dimensional space. It enables efficient similarity search and retrieval of data based on vector embeddings, commonly used in applications like machine learning and natural language processing. By leveraging techniques such as approximate nearest neighbor search, vector databases can quickly find relevant data based on proximity in vector space.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 78, 'prompt_tokens': 20, 'total_tokens': 98, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_29330a9688', 'id': 'chatcmpl-D1GbRW2V8hP75NoBq7X8oBHvupkyN', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='lc_run--019bec35-fd81-7f21-b88c-1ab510f7bf9c-0' tool_calls=[] invalid_tool_calls=[] usage_metadata={'input_tokens': 20, 'output_tokens': 78, 'total_tokens': 98, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "llm = ChatOpenAI(model= 'gpt-4o-mini', temperature=0)\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    'You are a helpful teacher. Explain {topic} in {lines} lines'\n",
    ") # lets injection of variables (topic and lines)\n",
    "\n",
    "messages = prompt.format_messages(topic='vector database', lines = 3) # makes a message list the chat model understand\n",
    "response = llm.invoke(messages)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ef1642",
   "metadata": {},
   "source": [
    "### 3) The cleanest \"chain\" pattern (Prompt | Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32252d5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure! Prompt templates are structured formats that help guide the creation of prompts for various tasks. Hereâ€™s a simple example of a prompt template for writing a story:\n",
      "\n",
      "### Story Prompt Template\n",
      "\n",
      "**Title:** [Insert a catchy title here]\n",
      "\n",
      "**Setting:** [Describe the location and time period of the story]\n",
      "\n",
      "**Main Character:** [Introduce the protagonist, including their name, age, and a brief description]\n",
      "\n",
      "**Conflict:** [Outline the main problem or challenge the character faces]\n",
      "\n",
      "**Goal:** [What does the main character want to achieve?]\n",
      "\n",
      "**Resolution:** [How does the story end? What happens to the character?]\n",
      "\n",
      "### Example Using the Template\n",
      "\n",
      "**Title:** The Lost Treasure of Maplewood\n",
      "\n",
      "**Setting:** A small, enchanted forest in the year 1820\n",
      "\n",
      "**Main Character:** Lily, a curious 12-year-old girl with a knack for solving puzzles\n",
      "\n",
      "**Conflict:** Lily discovers an old map that leads to a hidden treasure, but she must outsmart a mischievous fox who wants the treasure for himself.\n",
      "\n",
      "**Goal:** Lily wants to find the treasure to help her village, which is suffering from a drought.\n",
      "\n",
      "**Resolution:** After a series of clever tricks, Lily outsmarts the fox and finds the treasure, which contains magical seeds that bring water back to her village.\n",
      "\n",
      "This template can be adapted for different genres or types of writing, making it a versatile tool for storytelling!\n"
     ]
    }
   ],
   "source": [
    "llm = ChatOpenAI(model='gpt-4o-mini', temperature=0)\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"You are a helpful teacher. Give a simple example of {concept}.\"\n",
    ")\n",
    "\n",
    "chain = prompt | llm # chain is like a pipeline i/p -> prompt -> model -> o/p\n",
    "\n",
    "result = chain.invoke({'concept': 'prompt templates'}) # invoke now takes a dic cause the prompt has variables\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c572137",
   "metadata": {},
   "source": [
    "### 4) Add an o/p Parse (get plain string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ee1f0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChain chains are sequences of interconnected components that enable the construction of complex workflows for natural language processing tasks, allowing for the integration of various models, tools, and data sources.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "llm = ChatOpenAI(model='gpt-4o-mini', temperature=0)\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    'Explain {topic} in 1 sentence.'\n",
    ")\n",
    "\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "text = chain.invoke({'topic': 'LangChain chains'})\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff06357",
   "metadata": {},
   "source": [
    "### Practice 1) Change temperature to 0.7 to see how the response changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f786b373",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChain chains are sequences of linked components that enable the orchestration of various natural language processing tasks, allowing for the creation of complex workflows that leverage language models and other tools.\n"
     ]
    }
   ],
   "source": [
    "llm1 = ChatOpenAI(model='gpt-4o-mini', temperature=0.7)\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    'Explain {topic} in 1 sentence.'\n",
    ")\n",
    "\n",
    "chain = prompt | llm1 | StrOutputParser()\n",
    "\n",
    "text = chain.invoke({'topic': 'LangChain chains'})\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "225cf310",
   "metadata": {},
   "source": [
    "### Practice 2) Modify the prompt to force a format like \"Definition\", \"Example\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "43285e62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Definition:**  \n",
      "LangChain chains are sequences of operations or components that are linked together to process and transform data in a structured manner. They allow for the integration of various functionalities, such as data retrieval, processing, and output generation, enabling complex workflows to be built easily. Each component in a chain can perform a specific task, and the output of one component can serve as the input for the next.\n",
      "\n",
      "**Example:**  \n",
      "Consider a LangChain that processes user queries to provide relevant information from a knowledge base. The chain might consist of the following components:\n",
      "\n",
      "1. **Input Component:** Receives the user's query.\n",
      "2. **Retrieval Component:** Searches a knowledge base for relevant documents or data based on the query.\n",
      "3. **Processing Component:** Analyzes the retrieved data to extract key information or insights.\n",
      "4. **Output Component:** Formats the final response and presents it to the user.\n",
      "\n",
      "In this example, the user inputs a question like \"What are the benefits of using LangChain?\" The chain processes this input, retrieves relevant information from a database, analyzes it, and then outputs a concise answer summarizing the benefits.\n"
     ]
    }
   ],
   "source": [
    "llm = ChatOpenAI(model='gpt-4o-mini', temperature=0)\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    'Define {topic} with example in format of, Definition: , Example: '\n",
    ")\n",
    "\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "text = chain.invoke({'topic': 'LangChain chains'})\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fce105e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Definition: LangChain chains are sequences of interconnected components that facilitate the flow of data and logic in a language model application.  \n",
      "Example: A LangChain chain could take user input, process it through a text summarization model, and then output a concise summary.\n"
     ]
    }
   ],
   "source": [
    "llm = ChatOpenAI(model='gpt-4o-mini', temperature=0)\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    'Return EXACTLY two lines, no extra text.\\n'\n",
    "    'Definition: <one sentence>\\n'\n",
    "    'Example: <one sentence using a simple example>\\n'\n",
    "    'Topic: {topic}'\n",
    ")\n",
    "\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "text = chain.invoke({'topic': 'LangChain chains'})\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c19df4",
   "metadata": {},
   "source": [
    "### Practice 3) Make a chain that takes {role} and {topic} and responds accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aa8dccc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Certainly! Here are three interview questions tailored for a Machine Learning Engineer role, specifically focusing on the topic of Supervised Fine-Tuning (SFT):\n",
      "\n",
      "1. **Question 1:**\n",
      "   \"Can you explain the concept of Supervised Fine-Tuning (SFT) in the context of transfer learning? How does it differ from pre-training, and what are the key considerations when applying SFT to a specific task?\"\n",
      "\n",
      "2. **Question 2:**\n",
      "   \"When performing Supervised Fine-Tuning on a pre-trained model, what strategies would you employ to avoid overfitting, especially when working with a limited amount of labeled data? Can you provide examples of techniques or methods you would use?\"\n",
      "\n",
      "3. **Question 3:**\n",
      "   \"Describe a project where you implemented Supervised Fine-Tuning. What model did you choose, what dataset did you use, and what were the results? How did you evaluate the performance of the fine-tuned model, and what metrics did you consider most important?\"\n",
      "\n",
      "These questions aim to assess the candidate's understanding of SFT, practical experience, and ability to apply theoretical knowledge to real-world scenarios.\n"
     ]
    }
   ],
   "source": [
    "llm = ChatOpenAI(model='gpt-4o-mini', temperature=0)\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    'Generate 3 interview question for {role} role and {topic} topic.'\n",
    ")\n",
    "\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "text = chain.invoke({'role':'Machine Learning Engineer', 'topic': 'SFT'})\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d7dd89b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Can you explain the concept of Supervised Fine-Tuning (SFT) and its importance in the machine learning workflow?  \n",
      "2. What are some common challenges you might face when implementing SFT in a real-world project?  \n",
      "3. How do you determine the optimal dataset and hyperparameters for fine-tuning a pre-trained model using SFT?  \n"
     ]
    }
   ],
   "source": [
    "llm = ChatOpenAI(model='gpt-4o-mini', temperature=0)\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    'You are an interviwer.\\n'\n",
    "    'Generate EXACTLY 3 interview questions.\\n'\n",
    "    'Role: {role}\\n'\n",
    "    'Topic: {topic}\\n'\n",
    "    'Rules:\\n'\n",
    "    '- Output only the questions\\n'\n",
    "    '- Number them 1 to 3\\n'\n",
    "    '- No explanations\\n'\n",
    ")\n",
    "\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "text = chain.invoke({'role':'Machine Learning Engineer', 'topic': 'SFT'})\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e2b6eb6",
   "metadata": {},
   "source": [
    "### Multi-Message Prompts (system + user + optional few-shot example)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a00685",
   "metadata": {},
   "source": [
    "1) System + User Messages (the chat structure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ffcc7814",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChain Chains are sequences of operations that connect multiple components in a language model application. They enable the orchestration of tasks like data retrieval, processing, and response generation.\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "llm = ChatOpenAI(model='gpt-4o-mini', temperature=0)\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    ('system', 'You are a strict teacher. Keep answers short and clear'),\n",
    "    ('user', 'Explain {topic} in exactly {lines} lines.')\n",
    "])\n",
    "\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "text = chain.invoke({'topic': \"LangChain Chains\", 'lines': 2})\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a553343",
   "metadata": {},
   "source": [
    "##### Adding \"few-shot\" examples\n",
    "\n",
    "- showing model examples of what you want\n",
    "\n",
    "\n",
    "- System: \"WHo you are\" + \"rules you must follow\"\n",
    "- User: 'Tasks + inputs'\n",
    "- Assistant examples: 'Show the desired style' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1775d23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1) Can you explain the concept of supervised fine-tuning (SFT) and its importance in machine learning?  \n",
      "2) What are some common techniques used to prevent overfitting during the SFT process?  \n",
      "3) How do you evaluate the performance of a model after applying SFT?  \n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "llm = ChatOpenAI(model='gpt-4o-mini', temperature=0)\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    ('system', 'You are a interviewer. Output only questions. No explanations.'), \n",
    "    ('user', 'Role: {role}\\n Topic: {topic}\\n Give exactly 3 questions, numbered from 1 to 3.'),\n",
    "    ('assistant', '1) What is overfitting, anf how would you detect it?\\n 2) Explain precission vs recall with an example.\\n 3) How do you choose a validation strategy for imbalance data?'),\n",
    "    (\"user\", \"Role: {role}\\n Topic: {topic}\\n Give exactly 3 questions, numbered from 1 to 3.\")\n",
    "])\n",
    "\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "text = chain.invoke({'role':'Machine Learning Engineer', 'topic':'SFT'})\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc55c05",
   "metadata": {},
   "source": [
    "Practice Problem) Return Json only with keys: definition, example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c7e6d5a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"definition\": \"LangChain Templates are pre-defined structures that allow users to create and manage prompts for language models efficiently, enabling dynamic content generation based on variable inputs.\",\n",
      "  \"example\": {\n",
      "    \"template\": \"Hello, {name}! Welcome to {place}.\",\n",
      "    \"variables\": {\n",
      "      \"name\": \"Alice\",\n",
      "      \"place\": \"Wonderland\"\n",
      "    },\n",
      "    \"result\": \"Hello, Alice! Welcome to Wonderland.\"\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "llm = ChatOpenAI(model='gpt-4o-mini', temperature=0)\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    ('system', 'You are a JSON format bot.\\n Output ONLY in JSON with keys: definition and example and no extra text'), \n",
    "    ('user', 'Topic: {topic}\\n Provide a concise definition and a concise example')\n",
    "])\n",
    "\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "text = chain.invoke({'topic':'LangChain Templates'})\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5564b67e",
   "metadata": {},
   "source": [
    "#### For Multiple topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b10f41ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"topic\": \"LangChain Templates\",\n",
      "  \"definition\": \"LangChain Templates are pre-defined structures or formats used to create prompts for language models, allowing for consistent and efficient generation of text based on specific inputs.\",\n",
      "  \"example\": {\n",
      "    \"template\": \"Generate a summary of the following text: {input_text}\",\n",
      "    \"input_text\": \"LangChain is a framework for developing applications powered by language models.\"\n",
      "  }\n",
      "}\n",
      "{\n",
      "  \"topic\": \"LangChain Chains\",\n",
      "  \"definition\": \"LangChain Chains are sequences of operations or components in the LangChain framework that allow for the orchestration of multiple tasks, enabling complex workflows and interactions with language models.\",\n",
      "  \"example\": {\n",
      "    \"chain\": [\n",
      "      {\n",
      "        \"step\": \"Input\",\n",
      "        \"description\": \"Receive user query.\"\n",
      "      },\n",
      "      {\n",
      "        \"step\": \"Processing\",\n",
      "        \"description\": \"Use a language model to generate a response.\"\n",
      "      },\n",
      "      {\n",
      "        \"step\": \"Output\",\n",
      "        \"description\": \"Return the generated response to the user.\"\n",
      "      }\n",
      "    ]\n",
      "  }\n",
      "}\n",
      "{\n",
      "  \"topic\": \"LangChain Memory\",\n",
      "  \"definition\": \"LangChain Memory refers to the capability of a LangChain application to retain information across interactions, allowing it to provide contextually relevant responses based on previous conversations.\",\n",
      "  \"example\": {\n",
      "    \"user\": \"What is my favorite color?\",\n",
      "    \"langChain\": \"You mentioned your favorite color is blue.\",\n",
      "    \"user\": \"Can you remind me of that?\",\n",
      "    \"langChain\": \"Sure! Your favorite color is blue.\"\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "llm = ChatOpenAI(model='gpt-4o-mini', temperature=0)\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    ('system', 'You are a JSON format bot.\\n Output ONLY in JSON with keys: topic, definition, and example and no extra text'), \n",
    "    ('user', 'Topic: {topic}\\n Provide a concise definition and a concise example')\n",
    "])\n",
    "\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "topics = ['LangChaih Templates', 'LangChain Chains', 'LangCgain Memory']\n",
    "for t in topics:\n",
    "    print(chain.invoke({'topic': t}) )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "382e1ae0",
   "metadata": {},
   "source": [
    "#### Using JSON output parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "37e786a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'definition': 'LangChain Templates are pre-defined structures that allow users to create and manage prompts for language models efficiently, enabling dynamic content generation based on variable inputs.', 'example': {'template': 'Hello, {name}! Welcome to {place}.', 'variables': {'name': 'Alice', 'place': 'Wonderland'}, 'result': 'Hello, Alice! Welcome to Wonderland.'}}\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "\n",
    "llm = ChatOpenAI(model='gpt-4o-mini', temperature=0)\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    ('system', 'You are a JSON format bot.\\n Output ONLY in JSON with keys: definition and example and no extra text'), \n",
    "    ('user', 'Topic: {topic}\\n Provide a concise definition and a concise example')\n",
    "])\n",
    "\n",
    "chain = prompt | llm | JsonOutputParser()\n",
    "\n",
    "text = chain.invoke({'topic':'LangChain Templates'})\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ecc2b03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChain Templates are pre-defined structures that allow users to create and manage prompts for language models efficiently, enabling dynamic content generation based on variable inputs.\n",
      "{'template': 'Hello, {name}! Welcome to {place}.', 'variables': {'name': 'Alice', 'place': 'Wonderland'}, 'result': 'Hello, Alice! Welcome to Wonderland.'}\n"
     ]
    }
   ],
   "source": [
    "llm = ChatOpenAI(model='gpt-4o-mini', temperature=0)\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    ('system', 'You are a JSON format bot.\\n Output ONLY in JSON with keys: definition and example and no extra text.'), \n",
    "    ('user', 'Topic: {topic}\\n Provide a concise definition and a concise example')\n",
    "])\n",
    "\n",
    "chain = prompt | llm | JsonOutputParser()\n",
    "\n",
    "data = chain.invoke({'topic':'LangChain Templates'})\n",
    "print(data['definition'])\n",
    "print(data['example'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e4bb4048",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChain Templates are pre-defined structures that facilitate the creation of prompts for language models, allowing for consistent and efficient generation of text based on specific inputs.\n",
      "template = 'Translate the following English text to French: {text}'\n"
     ]
    }
   ],
   "source": [
    "llm = ChatOpenAI(model='gpt-4o-mini', temperature=0)\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    ('system', 'You are a JSON format bot.\\n Output ONLY in JSON with keys: definition and example and no extra text\\n Example must be a string.'), \n",
    "    ('user', 'Topic: {topic}\\n Provide a concise definition and a concise example')\n",
    "])\n",
    "\n",
    "chain = prompt | llm | JsonOutputParser()\n",
    "\n",
    "data = chain.invoke({'topic':'LangChain Templates'})\n",
    "print(data['definition'])\n",
    "print(data['example'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f479832f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lcenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
