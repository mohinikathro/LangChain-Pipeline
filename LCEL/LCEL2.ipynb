{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5095fa12",
   "metadata": {},
   "source": [
    "#### Problem: arXiv RAG Research Assistant\n",
    "\n",
    "Goal:\n",
    "Build a research assistant that answers questions using real arXiv papers, with:\n",
    "- metadata-aware retrieval\n",
    "- citations\n",
    "- LCEL pipeline\n",
    "- zero hallucination"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3bebfa5",
   "metadata": {},
   "source": [
    "#### Tasks\n",
    "\n",
    "1) Load the PDFs with metadata\n",
    "- paper_title\n",
    "- year\n",
    "- arxiv_category\n",
    "- source\n",
    "\n",
    "2) Chunking\n",
    "- chunk_size = 1000\n",
    "- chunk_overlap = 200\n",
    "\n",
    "3) Embedding + Vector DB\n",
    "- Use FAISS or ChromaDB\n",
    "- Store embeddings with metadata\n",
    "\n",
    "4) Metadata-filtered retrieval\n",
    "- Example query like: 'What improvements were proposed after 2021 for RAG?', where filter would be year>=2022\n",
    "\n",
    "5) LCEL RAG Pipeline\n",
    "Pipelne must:\n",
    "Question:\n",
    "\n",
    "  -  -> Retriever \n",
    "  -  -> Context Formatter ([Paper: Self-RAG | Year: 2023 | Page: 4] \\n <chunk_text>)\n",
    "  -  -> Prompt\n",
    "  -  -> LLM\n",
    "  -  -> JSON Output\n",
    "\n",
    "6) Output\n",
    "{\n",
    "  \"answer\": \"...\",\n",
    "  \"sources\": [\n",
    "    {\n",
    "      \"paper\": \"Self-RAG\",\n",
    "      \"year\": 2023,\n",
    "      \"page\": 4\n",
    "    }\n",
    "  ]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "456f11fe",
   "metadata": {},
   "source": [
    "#### Task 1) Load the PDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d200406",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mohinikathrotiya/Desktop/Langchain/lcenv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader, DirectoryLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b633343",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map Filename\n",
    "meta_map = {\n",
    "    'rag_2020.pdf': {'paper_title': 'RAG', 'year': 2020, 'arxiv_category': 'cs.CL'},\n",
    "    'dpr_2020.pdf': {'paper_title': 'DPR', 'year': 2020, 'arxiv_category': 'cs.CL'},\n",
    "    'cot_2022.pdf': {'paper_title': 'CoT', 'year': 2022, 'arxiv_category': 'cs.CL'},\n",
    "    'hyde_2022.pdf': {'paper_title': 'HyDE', 'year': 2022, 'arxiv_category': 'cs.IR'},\n",
    "    'self_rag_2023.pdf': {'paper_title': 'Self-RAG', 'year': 2023, 'arxiv_category': 'cs.CL'},\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ade6b622",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DirectoryLoader(\n",
    "    'data/arxiv_papers', \n",
    "    '**/*.pdf',\n",
    "    loader_cls= PyPDFLoader\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "02bc87d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fcac8a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs= loader.load()\n",
    "\n",
    "for d in docs:\n",
    "    fname = os.path.basename(d.metadata.get('source', ''))\n",
    "    d.metadata['sourcefile'] = fname\n",
    "\n",
    "    meta = meta_map.get(fname, {})\n",
    "    d.metadata['paper_title'] = meta.get('paper_tile', fname.replace('.pdf', ''))\n",
    "    d.metadata[\"year\"] = meta.get(\"year\", None)\n",
    "    d.metadata[\"arxiv_category\"] = meta.get(\"arxiv_category\", None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1c57a323",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded docs: 116\n",
      "\n",
      " Sample metadata: {'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-10-19T00:28:18+00:00', 'author': '', 'keywords': '', 'moddate': '2023-10-19T00:28:18+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'data/arxiv_papers/self_rag_2023.pdf', 'total_pages': 30, 'page': 0, 'page_label': '1', 'sourcefile': 'self_rag_2023.pdf', 'paper_title': 'self_rag_2023', 'year': 2023, 'arxiv_category': 'cs.CL'}\n"
     ]
    }
   ],
   "source": [
    "print('Loaded docs:', len(docs))\n",
    "print('\\n','Sample metadata:', docs[0].metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "778b2f49",
   "metadata": {},
   "source": [
    "#### Task 2) Creating Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "65020e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "42556cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 1000,\n",
    "    chunk_overlap = 200\n",
    ")\n",
    "\n",
    "splits = splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "27fe0b25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunks: 535\n",
      "\n",
      " Chunk sample metadata: {'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-10-19T00:28:18+00:00', 'author': '', 'keywords': '', 'moddate': '2023-10-19T00:28:18+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'data/arxiv_papers/self_rag_2023.pdf', 'total_pages': 30, 'page': 0, 'page_label': '1', 'sourcefile': 'self_rag_2023.pdf', 'paper_title': 'self_rag_2023', 'year': 2023, 'arxiv_category': 'cs.CL'}\n",
      "\n",
      " Chunk Preview: Preprint.\n",
      "SELF -RAG: L EARNING TO RETRIEVE , G ENERATE , AND\n",
      "CRITIQUE THROUGH SELF -R EFLECTION\n",
      "Akari Asai†, Zeqiu Wu†, Yizhong Wang†§, Avirup Sil‡, Hannaneh Hajishirzi†§\n",
      "†University of Washington §Allen Institute for AI ‡IBM Research AI\n",
      "{akari,zeqiuwu,yizhongw,hannaneh}@cs.washington.edu, avi@us.ibm.com\n",
      "ABSTRACT\n",
      "Despite their remarkable capabilities, large language models (LLMs) often produce\n",
      "responses containing factual inaccuracies due to their sole reliance on the paramet-\n",
      "ric knowledge they\n"
     ]
    }
   ],
   "source": [
    "print('Chunks:', len(splits))\n",
    "print('\\n', 'Chunk sample metadata:', splits[0].metadata)\n",
    "print('\\n', 'Chunk Preview:', splits[0].page_content[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeaa7734",
   "metadata": {},
   "source": [
    "#### Task 3) Creating embeddings + Store the embeddings in Chroma DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d6050590",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bfa59a5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/32/4gjfcmjd6sd493d4pqrg896c0000gn/T/ipykernel_80682/3103022082.py:1: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the `langchain-huggingface package and should be used instead. To use it run `pip install -U `langchain-huggingface` and import as `from `langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(\n",
      "Loading weights: 100%|██████████| 103/103 [00:00<00:00, 2981.38it/s, Materializing param=pooler.dense.weight]                             \n",
      "BertModel LOAD REPORT from: sentence-transformers/all-MiniLM-L6-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Chroma Vector DB ready.\n"
     ]
    }
   ],
   "source": [
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name = 'sentence-transformers/all-MiniLM-L6-v2'\n",
    ")\n",
    "\n",
    "persist_dir = 'chroma_arxiv_db'\n",
    "\n",
    "vector_db = Chroma.from_documents(\n",
    "    documents= splits,\n",
    "    embedding= embeddings,\n",
    "    persist_directory= persist_dir,\n",
    "    collection_name= \"arxiv_rag_practice\"\n",
    ")\n",
    "\n",
    "print('\\n','Chroma Vector DB ready.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf19548",
   "metadata": {},
   "source": [
    "#### Task 4) Metadata-filtered retriever (year>=2022)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d78fcba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vector_db.as_retriever(\n",
    "    search_kwargs = {\n",
    "        'k': 4,\n",
    "        'filter': {'year': {'$gte': 2022}} # Chroma-style filter\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7ad8e667",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Rank: 1\n",
      "Paper: self_rag_2023 Year: 2023 Page: 0\n",
      "indicate the need for retrieval and its generation quality respectively (Figure 1 right). In particular,\n",
      "given an input prompt and preceding generations, SELF -RAG first determines if augmenting the\n",
      "continued generation with retrieved passages would be helpful. If so, it outputs a retrieval token th\n",
      "================================================================================\n",
      "Rank: 2\n",
      "Paper: self_rag_2023 Year: 2023 Page: 0\n",
      "indicate the need for retrieval and its generation quality respectively (Figure 1 right). In particular,\n",
      "given an input prompt and preceding generations, SELF -RAG first determines if augmenting the\n",
      "continued generation with retrieved passages would be helpful. If so, it outputs a retrieval token th\n",
      "================================================================================\n",
      "Rank: 3\n",
      "Paper: hyde_2022 Year: 2022 Page: 6\n",
      "Aakanksha Chowdhery, Sharan Narang, Jacob Devlin,\n",
      "Maarten Bosma, Gaurav Mishra, Adam Roberts,\n",
      "Paul Barham, Hyung Won Chung, Charles Sutton,\n",
      "Sebastian Gehrmann, Parker Schuh, Kensen Shi,\n",
      "Sasha Tsvyashchenko, Joshua Maynez, Abhishek\n",
      "Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vin-\n",
      "odkumar Prabhakaran, E\n",
      "================================================================================\n",
      "Rank: 4\n",
      "Paper: self_rag_2023 Year: 2023 Page: 8\n",
      "the ablation studies on three datasets, PopQA, PubHealth, and ASQA. On ASQA, we evaluate models\n",
      "on sampled 150 instances and exclude ablations involving adaptive or no retrieval processes.\n",
      "We show in Table 3a the ablation results. The top part of the table shows results for training ablations,\n",
      "and t\n"
     ]
    }
   ],
   "source": [
    "query = 'What improvements were proposed after 2021 for RAG?'\n",
    "\n",
    "top_docs = retriever.invoke(query)\n",
    "\n",
    "for i, d in enumerate(top_docs, 1):\n",
    "    print('='*80)\n",
    "    print('Rank:', i)\n",
    "    print('Paper:', d.metadata.get('paper_title'), 'Year:', d.metadata.get('year'),\n",
    "          'Page:', d.metadata.get('page'))\n",
    "    print(d.page_content[:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e86d0cd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " What improved retrieval-augmented generation after 2021?\n",
      "================================================================================\n",
      "Rank: 1\n",
      "Paper: self_rag_2023 Year: 2023 Page: 11\n",
      "Transactions on Machine Learning Research , 2022a. URL https://openreview.net/\n",
      "forum?id=jKN1pXi7b0.\n",
      "Gautier Izacard, Patrick Lewis, Maria Lomeli, Lucas Hosseini, Fabio Petroni, Timo Schick, Jane\n",
      "Dwivedi-Yu, Armand Joulin, Sebastian Riedel, and Edouard Grave. Few-shot learning with\n",
      "retrieval augmente\n",
      "================================================================================\n",
      "Rank: 2\n",
      "Paper: self_rag_2023 Year: 2023 Page: 11\n",
      "Transactions on Machine Learning Research , 2022a. URL https://openreview.net/\n",
      "forum?id=jKN1pXi7b0.\n",
      "Gautier Izacard, Patrick Lewis, Maria Lomeli, Lucas Hosseini, Fabio Petroni, Timo Schick, Jane\n",
      "Dwivedi-Yu, Armand Joulin, Sebastian Riedel, and Edouard Grave. Few-shot learning with\n",
      "retrieval augmente\n",
      "================================================================================\n",
      "Rank: 3\n",
      "Paper: self_rag_2023 Year: 2023 Page: 1\n",
      "retrieval-augmented ChatGPT on four tasks, Llama2-chat (Touvron et al., 2023) and Alpaca (Dubois\n",
      "et al., 2023) on all tasks. Our analysis demonstrates the effectiveness of training and inference with\n",
      "reflection tokens for overall performance improvements as well as test-time model customizations\n",
      "(e.\n",
      "================================================================================\n",
      "Rank: 4\n",
      "Paper: self_rag_2023 Year: 2023 Page: 1\n",
      "retrieval-augmented ChatGPT on four tasks, Llama2-chat (Touvron et al., 2023) and Alpaca (Dubois\n",
      "et al., 2023) on all tasks. Our analysis demonstrates the effectiveness of training and inference with\n",
      "reflection tokens for overall performance improvements as well as test-time model customizations\n",
      "(e.\n"
     ]
    }
   ],
   "source": [
    "query = 'What improved retrieval-augmented generation after 2021?'\n",
    "\n",
    "top_docs = retriever.invoke(query)\n",
    "\n",
    "print('\\n', query)\n",
    "for i, d in enumerate(top_docs, 1):\n",
    "    print('='*80)\n",
    "    print('Rank:', i)\n",
    "    print('Paper:', d.metadata.get('paper_title'), 'Year:', d.metadata.get('year'),\n",
    "          'Page:', d.metadata.get('page'))\n",
    "    print(d.page_content[:300])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf5b74c5",
   "metadata": {},
   "source": [
    "#### Task 5) Building LCEL RAG Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a36ce7",
   "metadata": {},
   "source": [
    "A) Choosing an LLM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5b6a1652",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ab27a70b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ['LANGCHAIN_TRACKING_V2'] = 'true'\n",
    "os.environ['LANGCHAIN_PROJECT'] = os.getenv('LANGCHAIN_PROJECT')\n",
    "os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b44924c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model='gpt-4o-mini', temperature=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e59c07",
   "metadata": {},
   "source": [
    "B) Building LCEL Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5a8a79ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:35: SyntaxWarning: invalid escape sequence '\\I'\n",
      "<>:35: SyntaxWarning: invalid escape sequence '\\I'\n",
      "/var/folders/32/4gjfcmjd6sd493d4pqrg896c0000gn/T/ipykernel_80682/595191749.py:35: SyntaxWarning: invalid escape sequence '\\I'\n",
      "  'If the answer is not in the context, say: \\Insufficientcontext\\.\\n'\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from typing import List, Dict, Any\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough, RunnableParallel\n",
    "\n",
    "def format_context(docs):\n",
    "    # Context formatting with citations\n",
    "    lines = []\n",
    "    for d in docs:\n",
    "        paper = d.metadata.get('paper_title', 'Unknown')\n",
    "        year = d.metadata.get('year', 'Unknown')\n",
    "        page = d.metadata.get('page', 'Unknown')\n",
    "        lines.append(f\"[Paper : {paper} | Year: {year} | Page: {page} \\n{d.page_content}\")\n",
    "\n",
    "    return '\\n\\n'.join(lines)\n",
    "\n",
    "def build_sources(docs) -> List[Dict[str, Any]]:\n",
    "    # Deduplication citations\n",
    "    seen = set()\n",
    "    sources = []\n",
    "    for d in docs:\n",
    "        paper = d.metadata.get('paper_title', 'Unknown')\n",
    "        year = d.metadata.get('year', None)\n",
    "        page = d.metadata.get('page', None)\n",
    "        key = (paper, page)\n",
    "        if key in seen:\n",
    "            continue\n",
    "        seen.add(key)\n",
    "        sources.append({'paper':paper, 'year':year, 'page':page})\n",
    "    return sources\n",
    "\n",
    "prompt = ChatPromptTemplate([\n",
    "    ('system', \n",
    "     'You are a research assistant. Use ONLY provided contextto answer.\\n'\n",
    "     'If the answer is not in the context, say: \\Insufficientcontext\\.\\n'\n",
    "     'Return only valid JSON with keys: answer, sources.\\n'\n",
    "     'No extra text.'),\n",
    "     ('human', \n",
    "      'Question:\\n{question}\\n\\n'\n",
    "      'Context:\\n{context}\\n\\n'\n",
    "      'Return JSON only.')\n",
    "\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ab166ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 1) fetch relevant docs + keep answers\n",
    "\n",
    "fetch_docs = RunnableParallel(\n",
    "    question = RunnablePassthrough(), \n",
    "    docs = retriever\n",
    ")\n",
    "\n",
    "## Step 2) format context\n",
    "add_context = RunnableLambda(lambda x: {\n",
    "    'question': x['question'],\n",
    "    'docs': x['docs'],\n",
    "    'context': format_context(x['docs'])\n",
    "})\n",
    "\n",
    "## Step 3) Call LLM\n",
    "call_llm = RunnableLambda(lambda x: {\n",
    "    \"raw_llm\": (prompt | llm).invoke({\"question\": x[\"question\"], \"context\": x[\"context\"]}),\n",
    "    \"docs\": x[\"docs\"]\n",
    "})\n",
    "\n",
    "# Step 4: enforce strict JSON + attach sources from docs (grounded)\n",
    "def finalize(x):\n",
    "    # LLM output (string-like)\n",
    "    content = x[\"raw_llm\"].content if hasattr(x[\"raw_llm\"], \"content\") else str(x[\"raw_llm\"])\n",
    "    # Parse JSON safely\n",
    "    try:\n",
    "        parsed = json.loads(content)\n",
    "    except Exception:\n",
    "        # Hard fallback: still return strict structure\n",
    "        parsed = {\"answer\": \"Insufficient context\", \"sources\": []}\n",
    "\n",
    "    # If model answered insufficient context, keep sources empty\n",
    "    answer = parsed.get(\"answer\", \"\")\n",
    "    if isinstance(answer, str) and answer.strip().lower() == \"insufficient context\":\n",
    "        return {\"answer\": \"Insufficient context\", \"sources\": []}\n",
    "\n",
    "    # Always ground sources from retrieved docs (not hallucinated)\n",
    "    return {\n",
    "        \"answer\": parsed.get(\"answer\", \"\"),\n",
    "        \"sources\": build_sources(x[\"docs\"])\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8df1d3ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain = fetch_docs | add_context | call_llm | RunnableLambda(finalize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "14724180",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"answer\": \"HyDE is a retrieval model that shows better performance than other models like ANCE and DPR, even though those models are fine-tuned on specific datasets. It provides sizable improvements in retrieval tasks, particularly in low-resource scenarios, and demonstrates strong performance compared to fine-tuned models. HyDE's architecture allows it to operate without the need for relevance labels, making it practical for early stages of search systems. As the search log grows, it can be gradually replaced by a supervised dense retriever.\",\n",
      "  \"sources\": [\n",
      "    {\n",
      "      \"paper\": \"hyde_2022\",\n",
      "      \"year\": 2022,\n",
      "      \"page\": 4\n",
      "    },\n",
      "    {\n",
      "      \"paper\": \"hyde_2022\",\n",
      "      \"year\": 2022,\n",
      "      \"page\": 5\n",
      "    },\n",
      "    {\n",
      "      \"paper\": \"hyde_2022\",\n",
      "      \"year\": 2022,\n",
      "      \"page\": 6\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "## Testing 1\n",
    "\n",
    "result1 = rag_chain.invoke(\"What is HyDE and why does it help retrieval?\")\n",
    "print(json.dumps(result1, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e81572fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"HyDE is a retrieval model that shows better performance than other models like ANCE and DPR, even though those models are fine-tuned on specific datasets. It provides sizable improvements in retrieval tasks, particularly in low-resource scenarios, and demonstrates strong performance compared to fine-tuned models. HyDE's architecture allows it to operate without the need for relevance labels, making it practical for early stages of search systems. As the search log grows, it can be gradually replaced by a supervised dense retriever.\""
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result1['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "13d6b448",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"answer\": \"Improvements proposed after 2021 for retrieval-augmented generation include the effectiveness of training and inference with reflection tokens for overall performance improvements and test-time model customizations, such as balancing the trade-off between citation precision and completeness.\",\n",
      "  \"sources\": [\n",
      "    {\n",
      "      \"paper\": \"self_rag_2023\",\n",
      "      \"year\": 2023,\n",
      "      \"page\": 11\n",
      "    },\n",
      "    {\n",
      "      \"paper\": \"self_rag_2023\",\n",
      "      \"year\": 2023,\n",
      "      \"page\": 1\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "result = rag_chain.invoke(\"What improvements were proposed after 2021 for retrieval-augmented generation?\")\n",
    "print(json.dumps(result, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f7e1ce9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"answer\": \"Insufficient context\",\n",
      "  \"sources\": []\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "result = rag_chain.invoke(\"What is LangChain's revenue in 2025?\")\n",
    "print(json.dumps(result, indent=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "097edf28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c80505c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lcenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
